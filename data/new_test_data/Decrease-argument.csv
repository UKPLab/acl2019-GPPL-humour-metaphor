#id	rank	argument	turkID
0	0.0	We mean mostly treating the model weights as implicit factorizations of tensor representations of some formal grammars / interesting clustering of words or phrases, similarly to Goldberg's and Levy's interpretation of word2vec.	dummyTurk
1	0.0	Not having any solid results in this, we did not want mention that explicitly in the paper.	dummyTurk
2	0.0	ESIM chosen because it is one of the state-of-the-art models and had a codebase available that was easiest to work with.	dummyTurk
3	0.0	We have some new results based on an LSTM sentence-embedding encoder.	dummyTurk
4	0.0	The idea was to compare the performance between a model that does not use a sentence-level representation, but rather tries to model interactions between hidden states of some kind of RNN (ESIM and DIIN fit into this category) and a model that uses a fixed-length sentence embedding or representation vector.	dummyTurk
5	0.0	We believe that the sentence embedding models, due to their structure, may be more prone to learning these superficial correlations.	dummyTurk
6	0.0	The experiment attempts to test existing models for their reliance on the hypothesis for classification.	dummyTurk
7	0.0	During testing, we shuffle the premises so that they do not correspond to the right hypotheses.	dummyTurk
8	0.0	The model still achieved a 40.5% accuracy in this setting.	dummyTurk
9	0.0	This suggests that while the model it still uses some of the correlations found in the hypothesis, otherwise this experiment should result in an ~33% accuracy.	dummyTurk
10	0.0	The sentence-embedding models that we trained achieved 70% accuracy when trained on the full dataset, and 68% on the pruned dataset.	dummyTurk
11	0.0	In the shuffled premise test, they achieved accuracies of 50% and 47% respectively.	dummyTurk
12	0.0	While the results are not yet as conclusive as they could be but the results hint that how much the model is affected by the dataset biases may be related to model structure.	dummyTurk
13	0.0	Thank you for the reference!	dummyTurk
14	0.0	The goal of this paper was mainly to point out biases in the SNLI dataset, but these are relevant points which seem to be related to our findings.	dummyTurk
15	0.0	It is true, that there are other superficial features that can be leveraged when we include both the premises and the hypothesis (like leveraging antonyms), but it is harder to measure and remove these biases.	dummyTurk
16	0.0	We agree.	dummyTurk
17	0.0	Part of the motivation of collecting the SNLI dataset was for the purpose of training sentence embedding models.	dummyTurk
18	0.0	In order to properly evaluate the sentence embeddings produced by training on this dataset, we need to evaluate them on other tasks.	dummyTurk
19	0.0	Yes.	dummyTurk
20	0.0	"""I would like to know if these biases exist in all textual entailment datasets. """	dummyTurk
21	0.0	Our general hypothesis is: Given a task where we need to predict y given x_1 and x_2, and we know that given only one of these inputs, y should be conditionally independent, the dataset should reflect this.	dummyTurk
22	0.0	We believe that this property is hard to come by unless somehow explicitly controlled for.	dummyTurk
23	0.0	We will include further explanation about MutliNLI.	dummyTurk
24	0.0	We have actually conducted an experiment using SVM for our Indonesian NER.	dummyTurk
25	0.0	SVM is the most frequently used machine learning algorithm in many Indonesian NER researches.	dummyTurk
26	0.0	Many experiments showed that a deep-learning-based Indonesian NER could outperform an SVM-based one.	dummyTurk
27	0.0	Should the manuscript be accepted, we will add our experimental result on SVM-based Indonesian NER.	dummyTurk
28	0.0	We regret that we have not further analyzed the effect of PoS feature and output layer for each NE type.	dummyTurk
29	0.0	Should the manuscript be accepted, we will add further explanation about the effect of PoS feature and output layer for each NE type.	dummyTurk
30	0.0	We appreciate the valuable feedback the reviewer give.	dummyTurk
31	0.0	We could not give too much explanation about the corpus because of the page limitation.	dummyTurk
32	0.0	Should the manuscript be accepted and need revision, we are pleased to provide further explanation about the training and testing corpus.	dummyTurk
33	0.0	As long as the writers know, our research is the first that empirically compares the softmax and conditional random field (CRF) layer.	dummyTurk
34	0.0	We are also the first that use part-of-speech (PoS) as one of the input features for recognizing named entities (NEs).	dummyTurk
35	0.0	We have added some sentences to the Abstract (page 1 line 27-30) to clarify those.	dummyTurk
36	0.0	While Lample (2016) only mentioned that CRF is better than softmax for NE tagging, we empirically prove that CRF outperform softmax albeit only a little, supporting their explanation.	dummyTurk
37	0.0	However, adding PoS as the input feature can even make the softmax slightly outperform the CRF.	dummyTurk
38	0.0	We have performed the experiment using around 3000, 6000, and exactly 8400 sentences.	dummyTurk
39	0.0	While the performance increased significantly when the training data was increased from 3000 to 6000 sentences, the performance increase was not significant when the data was increased from 6000 to 8400 sentences.	dummyTurk
40	0.0	Therefore we assumed that 8400 sentences were sufficient for our experiment.	dummyTurk
41	0.0	We regret that some of our data and code are intellectual property that belongs to a company.	dummyTurk
42	0.0	Therefore, we cannot release them as an open-source software.	dummyTurk
43	0.0	Thank you for your review and for your very good questions.	dummyTurk
44	0.0	 the components of the system are independent and there is no joint optimization.	dummyTurk
45	0.0	The components communicate only in a top-down manner.	dummyTurk
46	0.0	each component can be independently changed/removed form the architecture and there is no cost associated with linking these components, other than  API interface.	dummyTurk
47	0.0	 The worst performing component is the linguistic model, followed by SQL generation for simple sentences.	dummyTurk
48	0.0	That is natural, as the burden of NLIDB task is precisely on the extraction of logic form from the query.	dummyTurk
49	0.0	There is an error analysis paragraph in evaluation section.	dummyTurk
50	0.0	However, we have more analysis on the components.	dummyTurk
51	0.0	We analyzed the errors carried out for each of the CLP types that occur in the queries.	dummyTurk
52	0.0	We describe it in the final form of the paper.	dummyTurk
53	0.0	We used human annotators for output evaluation.	dummyTurk
54	0.0	Their IAA is perfect because the logical form is unique up to a translation, so the figures reported in paper are precise.	dummyTurk
55	0.0	"The seq2Tree model we used is similar to the one described in Li Dong and Mirella Lapata 2016, ""Language to logical form with neural attention"", ACL 2016, that we cited in the paper and listed in References."	dummyTurk
56	0.0	Unfortunately, there was a LaTex compiling error, and in article, when we mentioned the first time the seq2Tree model, the citation was compiled to an empty brackets.	dummyTurk
57	0.0	"""()""."	dummyTurk
58	0.0	Thank you so much for pointing out this editing error.	dummyTurk
59	0.0	 The CLP we cited in the article are indeed not exhaustive for language in general, where much more phenomena occur - like metonymy, methaphors, hiperonymy etc.	dummyTurk
60	0.0	However, the queries to databases hardly exhibit any other CLP other than the one we considered in the paper.	dummyTurk
61	0.0	There was no query we analyzed ,  so none in thousands, that exhibits other CLPs.	dummyTurk
62	0.0	But, the system can learn other CLP, if they are present in queries.	dummyTurk
63	0.0	This is an excellent point, thank you for raising it , we will include in the paper.	dummyTurk
64	0.0	 Thank you for pointing out the editing errors.	dummyTurk
65	0.0	We have taken care of them.	dummyTurk
66	0.0	We appreciate your effort.	dummyTurk
67	0.0	 The complex queries containing CLP are decomposed in simple queries which are mapped automatically to SQL.	dummyTurk
68	0.0	This process is logically equivalent to an autodecoder, I think.	dummyTurk
69	0.0	We would like to pursue this suggestion further.	dummyTurk
70	0.0	Thank you.	dummyTurk
71	0.0	 The role of NLG in the current system , is to translate  back to the user, in English, the obtained SQL formula.In this way, the user can verify the correctness of the result.	dummyTurk
72	0.0	Indeed, in future, NLG  will  be fully developed to an interactive system which will handle through dialog the inconsistencies determined by Decomposition module in order to clarify the structural and lexical ambiguities, scope of the operators etc.	dummyTurk
73	0.0	The motivation for our evaluation metric (final paragraph of Section 4) was rather brief indeed.	dummyTurk
74	0.0	The choice of the metric was triggered by the following question we tried to answer ???does this segment contribute higher to a particular tag than other segments????, leading to the comparison with the average.	dummyTurk
75	0.0	An alternative question could indeed have been ???does this character lead to the strongest contribution???, which we expect will correlate quite well with our metric, although probably with lower absolute values.	dummyTurk
76	0.0	We agree with the reviewer that it is possible that another segment has a higher contribution, but that does not mean that the current position does not contribute (as it still contributes more than the average over the sequence), only that the other contributes even stronger.	dummyTurk
77	0.0	The highest-contribution-or-not objective would treat contributions slightly below the highest the same as completely irrelevant ones.	dummyTurk
78	0.0	Indeed, we have observed that the effects of several segments are often correlated.	dummyTurk
79	0.0	For example, when a character is the final letter of a word, the <eow> char is also important: the 2 characters together provide the evidence, but when looking at the impact of single characters, the joint contribution is split between them.	dummyTurk
80	0.0	In the case of ???leikkasin???, this means two word-feature pairs.	dummyTurk
81	0.0	However, we believe that there is still enough variety in the dataset and we found that correctly predicting/attributing one word-feature pair does not mean that the other word-feature pair is correctly predicted/attributed.	dummyTurk
82	0.0	We also would like to mention that the script for generating the word pairs and evaluating the attributions is available as supplementary material, making our dataset perfectly reproducible.	dummyTurk
83	0.0	It???s indeed an interesting phenomena.	dummyTurk
84	0.0	We believe this is due to the fact that plural words typically end with the character ???s??? but that the character ???s??? is not unique.	dummyTurk
85	0.0	Hence, this means that the model should also take positional information into account while for the synthetic character this is not the case.	dummyTurk
86	0.0	When the synthetic character is available, there is no doubt about the word being plural.	dummyTurk
87	0.0	This is also means that during training, the synthetic character immediately gains ???decision??? weight and in the end will have such as large contribution that the contribution of the natural character becomes irrelevant.	dummyTurk
88	0.0	We are aware of these papers but we decided to not include them due to space constraints.	dummyTurk
89	0.0	Both papers fall under the same category of papers that analyze hidden states as a way to understand what the network is paying attention to (Li 2016, Radford 2017, Strobelt 2018).	dummyTurk
90	0.0	Upon acceptance, we are more than happy to add them, given the additional allowed page in the camera-ready version.	dummyTurk
91	0.0	These are answered in detail below.	dummyTurk
92	0.0	As noted in the strength and contribution sections, we believe that our paper contains a number of novel contributions which in total make this a significant research contribution.	dummyTurk
93	0.0	Not only do we extend the CD framework, we complete it to contain the most common architectural components.	dummyTurk
94	0.0	Moreover, we also execute an extensive study and compare CNNs and LSTM architectures and show that these models indeed pick up on linguistic rules.	dummyTurk
95	0.0	The difference in parameter counts is due to both the number of parameters in the input layer and the number of parameters in the classification layer.	dummyTurk
96	0.0	A big portion of the difference is caused by the highly different number of classes in the final classification layer.	dummyTurk
97	0.0	Given that we introduce the BiLSTM in that particular paragraph, our reasoning was that we should also justify why we choose a certain number of hidden states.	dummyTurk
98	0.0	We understand that the position of this sentence can cause confusion and will add additional clarification.	dummyTurk
99	0.0	We also checked why Spanish has a higher parameter count than Swedish and found out that the Spanish Universal Dependency dataset contains a portion of noisy sentences containing Russian and Chinese characters (a/o).	dummyTurk
100	0.0	The number of distinct characters is 309 and 91 for Spanish and Swedish, respectively.	dummyTurk
101	0.0	Taking the difference of those values and multiplying it with the character embedding size 50 results in a difference of 10k parameters.	dummyTurk
102	0.0	Consequently, subtracting this value from the total number of parameters results in a lower number of parameters for Spanish compared to Swedish.	dummyTurk
103	0.0	That is indeed correct.	dummyTurk
104	0.0	Per type of feature class (e.g., Person), we select the most occuring class as baseline (e.g., ???1???) (which we will clarify in the final version).	dummyTurk
105	0.0	By ???full test set??? we refer to the UD test set, while the ???test pairs??? are the annotated segments.	dummyTurk
106	0.0	We recognize that we should define this more clearly.	dummyTurk
107	0.0	We will also add an additional line to Table 1, providing statistics about the UD test set.	dummyTurk
108	0.0	The results in Table 2 are based on the full (UD) test set, while the results in Table 3 only concern the word-feature pair test set, and where the latter is a subset of the full (UD) test set.	dummyTurk
109	0.0	All contributions within one model/figure are divided by the largest magnitude of all the contributions within one model/figure.	dummyTurk
110	0.0	Hence, the relative proportions are still the same.	dummyTurk
111	0.0	This makes the figures aesthetically more pleasing and allows to visually compare the relative proportions of individual contributions across models.	dummyTurk
112	0.0	While there is no single section which covers all aspects of the question, a number of them are addressed in different sections.	dummyTurk
113	0.0	All visualized contributions are calculated as mentioned in line 326.	dummyTurk
114	0.0	That is, we calculate the relevant contribution of a certain character (sequence) with respect to a certain class j.	dummyTurk
115	0.0	To evaluate whether the model uses the same characters as linguistically segmented and annotated by experts, we calculate the contribution of each character sequence of the same length as the ground truth character sequence and evaluate whether the ground truth character sequence has a higher than average contribution for predicting the correct class j compared to the other character sequences of the same length.	dummyTurk
116	0.0	This definition can be found in paragraph ???Evaluation??? at line 433.	dummyTurk
117	0.0	Consequently, if the ground truth character sequence is of length 1, we only compare with other character sequences of length 1.	dummyTurk
118	0.0	Similarly for length 2, 3, 4, ... As pointed out by the reviewer, comparing variable-length character sequences within a single word-feature pair evaluation becomes harder to interpret, due to the different expected contributions of longer segments.	dummyTurk
119	0.0	Therefore, we chose for a straightforward, less complicated evaluation and deem finding a good way for comparing the contributions of variable-length sequences within a single word, as future work.	dummyTurk
120	0.0	While it is indeed true that we did not give an exact definition, we double checked the numbers and did not find a calculation mistake.	dummyTurk
121	0.0	Hence, the confusion seems to originate from the definitions we used.	dummyTurk
122	0.0	We defined precision as the ratio of correctly predicted attributions given that the actual label was assigned with or without the correct attribute: (cor pred cor attr) / (cor pred incor attr + cor pred cor attr).	dummyTurk
123	0.0	The definition of recall is correctly deduced by the reviewer: (cor pred cor attr) / (incor pred cor attr + cor pred cor attr), i.e., from all items with the correct attribute, which fraction of them was actually detected by resulting in the correct label.	dummyTurk
124	0.0	The F1-scores are the geometric mean of P and R. Our situation does not lead to the traditional ground truth vs. prediction confusion matrix.	dummyTurk
125	0.0	Instead, we applied the traditional precision/recall definitions to a confusion matrix with on the one axis correctly/incorrectly predicted and on the other axis correctly/incorrectly attributed, where we are interested in the attribution rather than predictions, such that ???correctly attributed??? replaces the ???correctly predicted??? in the traditional sense.	dummyTurk
126	0.0	Stating it like this, we clearly see how this leads to confusion, and thank the reviewer for this very valid remark.	dummyTurk
127	0.0	We will ensure that this matter is unambiguously clear in the final version.	dummyTurk
128	0.0	Table 4 shows that CNN models never perform significantly better on sequences of character length one, and we did not state the opposite in the text.	dummyTurk
129	0.0	However, we do agree that the paragraph starting from line 677 can be phrased less strong, given that our explanation can be seen as a hypothesis based on the results of Table 4 and the properties of the architectures.	dummyTurk
130	0.0	We appreciate your careful review.	dummyTurk
131	0.0	The following is our response to the comments.	dummyTurk
132	0.0	Thank you for your comment.	dummyTurk
133	0.0	We???d like to add more description for the other MRT and reinforcement learning-based ones in our future revision.	dummyTurk
134	0.0	We appreciate your careful review.	dummyTurk
135	0.0	The following is our response to the comments.	dummyTurk
136	0.0	As you mentioned, our novelty is a penalty for overlength summaries.	dummyTurk
137	0.0	As a result, a model trained with the proposed method tends to generate summaries that has high ROUGE scores within a length constraint.	dummyTurk
138	0.0	To the best of our knowledge, this is the first work that applies minimum risk training to a summarization task with a length constraint.	dummyTurk
139	0.0	We used a modified version of the Gigaword test set for comparing the original LenEmb with the proposed method in terms of ROUGE scores and the controlling ability of summary lengths.	dummyTurk
140	0.0	This is because we focus on generation of summaries with a desired length.	dummyTurk
141	0.0	ROUGE scores of the proposed method may be lower than those of the other state-of-the-art models, however, their models do not consider lengths of summaries.	dummyTurk
142	0.0	We compared the proposed method with the other state-of-the-art systems on DUC 2004.	dummyTurk
143	0.0	The model of Ayana et al.	dummyTurk
144	0.0	showed better ROUGE scores than the proposed one, however, their model does not control summary lengths.	dummyTurk
145	0.0	We only tried fixed values of the number of samples (K) and the maximum length of samples (T).	dummyTurk
146	0.0	We assume that the larger K is used, the better summarization performance is obtained.	dummyTurk
147	0.0	This is because samples with high ROUGE scores can easily be found compared to those of smaller K.	dummyTurk
148	0.0	When T is too small, the summarization performance may decrease.	dummyTurk
149	0.0	When the number of sampled words reaches T, a model is forced to generate end-of-sentence (EOS) regardless of probability distribution of words.	dummyTurk
150	0.0	Use of reference summaries as samples by a model improves the readability.	dummyTurk
151	0.0	Since MRT optimizes a model for ROUGE scores and ROUGE scores between two same texts are 1, the probability of generation of a reference summary largely increases than those of the other samples.	dummyTurk
152	0.0	We can see that reference summaries are more readable than the other samples by a model.	dummyTurk
153	0.0	Thus, readability of generated summaries increases by using reference summaries as samples by a model.	dummyTurk
154	0.0	In our method, use of overlength summaries contribute to decrease the probabilities of generation of overlength summaries.	dummyTurk
155	0.0	We did not check the generated samples by Gumbel-max.	dummyTurk
156	0.0	We assume readabilities of generated samples are slightly worse than those of the proposed method in Table 3.	dummyTurk
157	0.0	This is because our sampling process in training time probabilistically chooses words based on LenEmb while LenEmb chooses words that has the maximum probability among the probability distribution of words in test time.	dummyTurk
158	0.0	Thank you for your comment.	dummyTurk
159	0.0	We will provide examples.	dummyTurk
160	0.0	The early detection experiment tested in certain intervals has been implemented and the result is shown in Figure 4.	dummyTurk
161	0.0	In this figure, using different percentages of repost information means using information within certain time interval, because repost information is sorted by time.	dummyTurk
162	0.0	The improvement obtained by RNN-based rumor detection is significant from this figure.	dummyTurk
163	0.0	Our early rate result shown in Table 2 is the average of the early rate for each sample in the test set, because our CED method can detect an Credible Detection Point for each test sample.	dummyTurk
164	0.0	That is also the reason why ablation experiment cannot be implemented.	dummyTurk
165	0.0	Baseline methods cannot employ ???Early Detection Point??? to decide how much repost information every microblog needs to use.	dummyTurk
166	0.0	Thanks for your suggestion.	dummyTurk
167	0.0	Actually, only using O_{pred} and O_{time} violates our original intention to keep the prediction probabilities after Credible Detection Point stable.	dummyTurk
168	0.0	But we will conduct ablation test to show the gain of each component and various loss functions.	dummyTurk
169	0.0	Thanks for pointing it out.	dummyTurk
170	0.0	Eq.	dummyTurk
171	0.0	7 is the decision function for the testing set, because we need to make a decision before the last repost.	dummyTurk
172	0.0	For the real-world application, we will take your suggestion and state it in revision.	dummyTurk
173	0.0	Sorry for the mistake.	dummyTurk
174	0.0	Actually, \beta is not trainable.	dummyTurk
175	0.0	It is determined by \alpha in our model.	dummyTurk
176	0.0	We will correct this mistake in revision.	dummyTurk
177	0.0	Thanks for this instructive idea.	dummyTurk
178	0.0	We will follow it to test the gain of RNN.	dummyTurk
179	0.0	Results will be shown in the next version.	dummyTurk
180	0.0	In addition to false positive cases, we also considered false negative cases, which are detected as non-rumors but finally judged as rumors.	dummyTurk
181	0.0	The number of such inversion cases in Weibo testing set is 121(11.52%).The case study shown in section 4.7 is one of inversion cases.	dummyTurk
182	0.0	The fact that these examples are finally detected correctly reflects the necessity and rationality of the Critical Detection Point.	dummyTurk
183	0.0	As you suggested, we will give an example in revision.	dummyTurk
184	0.0	Thanks for pointing out the line numbers with mistakes, they have been corrected.	dummyTurk
185	0.0	Dear reviewer #1	dummyTurk
186	0.0	Thanks for your kind review.	dummyTurk
187	0.0	 In the final submission, the readability issue will be thoroughly examined.	dummyTurk
188	0.0	Best regards,	dummyTurk
189	0.0	 We fully agree that redundancy helps readers to understand texts.	dummyTurk
190	0.0	In this paper, however, we are not focusing on how the summary should be writing such that a reader can understand the content properly.	dummyTurk
191	0.0	We only discuss what content should be included in the summary and assume that the content can be understood by the reader.	dummyTurk
192	0.0	 According to the introduction in the paragraph at 205-213, new information cannot be created.	dummyTurk
193	0.0	We assume that all information is already available and is contained in set I.	dummyTurk
194	0.0	Again, we definitely agree that the order of presentation matters to the quality of the summary.	dummyTurk
195	0.0	How to present information is, however, out of scope of this paper.	dummyTurk
196	0.0	- small t is an element in set T as defined in line 276.	dummyTurk
197	0.0	Since T contains texts, small t is a text.	dummyTurk
198	0.0	- v_p is defined in Definition 2 (line 290).	dummyTurk
199	0.0	u_p models utility scores of information nuggets (elements in I) and v_p models utility scores of texts (elements in T).	dummyTurk
200	0.0	- we fixed line 306	dummyTurk
201	0.0	"With respect to ""Echo chambers"", we would like to say that the selection of important content (u_p) is not necessarily determined by personal preference."	dummyTurk
202	0.0	"A piece of information can be important according to u_p even if the reader does not ""like"" the information."	dummyTurk
203	0.0	We do not aim at achieving high click-through-rates but providing users with the most important information, not matter if they like to read it or not.	dummyTurk
204	0.0	We also agree that u_p depends on the time.	dummyTurk
205	0.0	We missed to state that we only consider one fixed point in time.	dummyTurk
206	0.0	We will clarify this in a revised version.	dummyTurk
207	0.0	 Yes, it can scale to large number of entities.	dummyTurk
208	0.0	Actually, the size of entities mainly influences the vocabulary size, since the number of entities that appear simultaneously in a sentence is usually small.	dummyTurk
209	0.0	For example, in the dstc2 task, 4 entities appear in a single sentence at most.	dummyTurk
210	0.0	 Agree.	dummyTurk
211	0.0	We have been running more experiments.	dummyTurk
212	0.0	Due to page limit, it might be hard to be included in the main paper, but we will put more result in the appendix.	dummyTurk
213	0.0	 Agree.	dummyTurk
214	0.0	We should write it more clearly.	dummyTurk
215	0.0	What in our mind is to impose other structures in U and V, which might lead to other regularizations.	dummyTurk
216	0.0	E.g., if U and V can be reasonably assumed to be sparse, then L_1 regularization might be another reasonable choice (or possibly L_1 + L_2 like elastic net in statistics).	dummyTurk
217	0.0	 In our opinion, using SGD to solve an ill-posed problem is not a good ideas for several reasons.	dummyTurk
218	0.0	Firstly, optimization algorithms, more efficient than SGD, are hard to be applied in those problems.	dummyTurk
219	0.0	Moreover, for an ill-posed problem to be solved, even with SGD, we need to design much more carefully with the setting of the optimization scheme, like initialization, step size and batch size.	dummyTurk
220	0.0	Furthermore, as shown in the paper, to rectify the ill-posed problem is very easy, just adding quadratic regularizations.	dummyTurk
221	0.0	 Adding a variety of regulation, like l_1 norm, l_2 norm or even nuclear norm, to improve the numerical result, may not be that hard.	dummyTurk
222	0.0	But the critical question is why we need to do that.	dummyTurk
223	0.0	More important than the empirical improvement over one or two datasets, we think a model should justify itself.	dummyTurk
224	0.0	In statistics, all regularizations come up with a good reason and solid theory, e.g.	dummyTurk
225	0.0	ridge regression, LASSO and elastic net regularization.	dummyTurk
226	0.0	That is the missing piece in many neural network models.	dummyTurk
227	0.0	Our logic here is more or less a mathematic/axiomatic approach	dummyTurk
228	0.0	Thanks for the feedback, and we will improve the paper readability by clarifying the contributions and release the code for better reproducibility.	dummyTurk
229	0.0	The motivation is to enable nonparametric learning for MUSE, but the proposed method is different from Bartunov et al (2016) in that their method cannot be applied to deep learning structure.	dummyTurk
230	0.0	Thanks for the suggestions, and we will add the the qualitative study using the additional page if accepted.	dummyTurk
231	0.0	We will sample the examples and show the captured senses for further discussion.	dummyTurk
232	0.0	We will report the variance and mean of the number of learned senses and compares the learned senses with the senses defined in the dictionary using the additional page if accepted.	dummyTurk
233	0.0	Thanks for the suggestion, and we will revise Table 1 and mention that \epsilon-greedy performs better for all cases in the paragraph.	dummyTurk
234	0.0	Thank the reviewer for this great suggestion.	dummyTurk
235	0.0	We will add the experimental results of CNN+CRF and CNN+Softmax to our paper to illustrate the contribution of BiLSTM to our approach.	dummyTurk
236	0.0	Thank the reviewer for this comment.	dummyTurk
237	0.0	In Fig.	dummyTurk
238	0.0	3 we illustrate the performance of our approach on test data with different lambda values.	dummyTurk
239	0.0	We agree with the reviewer that when lambda is 0.5 our approach achieves the highest Fscore on the test data.	dummyTurk
240	0.0	In our experiments, the hyper-parameter lambda was tuned on the training data using cross-validation, and 0.4 seems to be the most appropriate value for lambda according to the cross-validation results.	dummyTurk
241	0.0	Thus, we set lambda to 0.4 in our experiments, although it is not the optimal value on the test data.	dummyTurk
242	0.0	Thank the reviewer very much for recommending this related work.	dummyTurk
243	0.0	We will cite this paper and add some discussions on it to our manuscript.	dummyTurk
244	0.0	In addition, we will compare the results of our approach with the method proposed in that paper.	dummyTurk
245	0.0	Thank the reviewer very much for pointing out these typos and spelling errors.	dummyTurk
246	0.0	We will carefully refine the writing of our paper and correct these typos and spelling errors.	dummyTurk
247	0.0	Thank the reviewer very much for this great suggestion.	dummyTurk
248	0.0	We will explore to apply the active learning techniques to select the most informative instances for Chinese NER.	dummyTurk
249	0.0	Thank you for supporting the paper.	dummyTurk
250	0.0	The raised issues will be included.	dummyTurk
251	0.0	We would very grateful if you were willing to 'fight' for the paper after reading the author response	dummyTurk
252	0.0	Thanks for your support!	dummyTurk
253	0.0	Existing techniques for knowledge alignment suffer significantly from the inconsistency of multiple knowledge graphs.	dummyTurk
254	0.0	This is because the multiple knowledge graphs are constructed from different sources, so the knowledge inside the KGs could be different.	dummyTurk
255	0.0	So they may have different inside structures.	dummyTurk
256	0.0	The inconsistency of multiple knowledge graphs make the performance worse.	dummyTurk
257	0.0	In our model, we leverage the relation paths to reduce the inconsisitency of multiple knowledge graphs, so that we could get better alignment performance.	dummyTurk
258	0.0	We will revise the paper accordingly.	dummyTurk
259	0.0	 We will add a discussion of the results of Kiela et al in the camera-ready submission.	dummyTurk
260	0.0	We cannot directly compare our results with theirs, because we do not know what subset of each evaluation dataset (e.g., SimLex) is used in the experiment of Kiela et al.	dummyTurk
261	0.0	We did not add these results to the submission because we did not have room for this discussion (due to the length constraint).	dummyTurk
262	0.0	 We will fix the mentioned issues in the camera-ready submission and add a discussion on how our objective function relates to cross-entropy.	dummyTurk
263	0.0	 Indeed, every layer treats its previous layer as input, so this means that each LSTM is fed into the next BiLSTM layer.	dummyTurk
264	0.0	In Figure 1, we omit other arrows to make the model more concise.	dummyTurk
265	0.0	Thanks for the encouraging comments.	dummyTurk
266	0.0	True.	dummyTurk
267	0.0	We could probably have improved our results by incorporation more context, here.	dummyTurk
268	0.0	One way to add further context is to use attention mechanism, which we have not.	dummyTurk
269	0.0	This is certainly an avenue that we can explore.	dummyTurk
270	0.0	However, LSTM architecture learns the long-term dependencies, and incorporates some amount of context.	dummyTurk
271	0.0	In fact, the improved performance of LSTM models in sequential tasks, such as processing words of sentences, is due to its ability to store contextual information from previously seen words.	dummyTurk
272	0.0	We construct the entity-embedding for D (Document-Creation-Time) by concatenating word embedding for the TIMEX3-value of D (which is in this format: YYYYMMDD) and the vector of its entity tag D. This is similar to how entity embeddings of TIME entities are generated.	dummyTurk
273	0.0	After entity-embedding for D is generated, it is fed to relation-classification network, along with the entity-emebdding of E, for classification of this E-D pair into a temporal relationship.	dummyTurk
274	0.0	By manual engineering, we meant manually constructed syntactic and semantic rules specific to data being used and task at hand.	dummyTurk
275	0.0	An example of such manual engineering would be to label entities e1 and e2 to be connected by after relation if e2 is the logical subject of e1, as in ???the chain reaction touched(e1) off by the collapse(e2) of Lehman Brothers???.	dummyTurk
276	0.0	This example is taken from Mirza and Tonelli (2016), who use a lot of manual feature engineering in their parser.	dummyTurk
277	0.0	We do not use any manually constructed attributes such as these, and only use features generated by publicly available parsers.	dummyTurk
278	0.0	Point taken.	dummyTurk
279	0.0	We should have added a sentence about how dependency embeddings, character embeddings, and POS embeddings are used in Figure 1.	dummyTurk
280	0.0	The dependency embeddings are concatenated to word embeddings, in a manner similar to the concatenation of the vector of entity tag T to word embedding of TIMEX3-value ???19980277??? or the concatenation of the vector of entity tag E and word embedding of lemma ???block??? in Figure 1.	dummyTurk
281	0.0	Yes, the global numbers do not beat the state of the art in performance.	dummyTurk
282	0.0	We managed to get best in performance in a couple of the fine-grained relationship classifications, as is highlighted in Table 1.	dummyTurk
283	0.0	But, we need to improve results.	dummyTurk
284	0.0	Answered above, in Reply 4.	dummyTurk
285	0.0	Thanks for your review.	dummyTurk
286	0.0	Yes.	dummyTurk
287	0.0	However, our tasks are both quite challenging especially for the reading comprehension tasks, existing model improvements have been already very marginal which can be seen from the leaderboards.	dummyTurk
288	0.0	However, we find a new solution in a new direction instead of stacking sophisticated attention mechanisms.	dummyTurk
289	0.0	The introduced subword embedding could promisingly give further advances due to its meaningful linguistic augments, which has not thoroughly studied yet for the concerned tasks.	dummyTurk
290	0.0	Yes, this is insightful and indeed what we are pushing on.	dummyTurk
291	0.0	Besides, as you suggested, Chinese character itself can be represented by strokes, the writing components of character, with embedding enhancement, it is hopeful to furthermore extend our current subword work.	dummyTurk
292	0.0	The sizes were found empirically on the dev set.	dummyTurk
293	0.0	We will clarify this in the later version.	dummyTurk
294	0.0	Yes, as in W3	dummyTurk
295	0.0	Yes, as in W1, the latest existing model improvements have been already very marginal which can be seen from the leaderboards.	dummyTurk
296	0.0	We think the research community is facing a bottleneck.	dummyTurk
297	0.0	Compared with the recent advances, our improvements should be substantial.	dummyTurk
298	0.0	Following your suggestion, we will add necessary statistical significant test results later.	dummyTurk
299	0.0	 We intend to compare our results with traditional MT metrics and not the top performing models from WMT data.	dummyTurk
300	0.0	So we did not write about those systems.	dummyTurk
301	0.0	However, we can include the results from top-performing models in WMT14 evaluation task in the camera-ready version, if accepted.	dummyTurk
302	0.0	 The main contribution of our approach is a novel approach to MT evaluation, which is not data dependent.	dummyTurk
303	0.0	We did not include these results due to lack of space.	dummyTurk
304	0.0	On acceptance, we will include results of wmt15-17 in the camera-ready version, if accepted.	dummyTurk
305	0.0	 Yes, we are aware that NLI data is not available in non-English languages.	dummyTurk
306	0.0	But there are techniques in the literature which creates NLI data for non-English languages using Neural Networks.	dummyTurk
307	0.0	We intend to explore such techniques in our future work.	dummyTurk
308	0.0	 Thanks for suggesting this.	dummyTurk
309	0.0	We will try this in our future work.	dummyTurk
310	0.0	 Yes, due to lack of space we kept the description minimal but we will include more information about those papers in the camera-ready version if accepted.	dummyTurk
311	0.0	 Same answer as Weakness argument 2.	dummyTurk
312	0.0	We thank the reviewer for appreciating the major contributions of our work.	dummyTurk
313	0.0	 We agree that in some cases there is some amount of noise in the generated speech.	dummyTurk
314	0.0	Adding a post-processing step using standard signal processing techniques could eliminate much of this noise.	dummyTurk
315	0.0	We thank the reviewer for the constructive comments.	dummyTurk
316	0.0	 The purpose of this metric was to serve as additional evidence (beyond human evaluation) that natural speech and generated speech remain indistinguishable to classifiers.	dummyTurk
317	0.0	But, we agree that the accent recognition system is noisy and as such provides only weak evidence.	dummyTurk
318	0.0	So, we will omit this in the revised version as suggested by the reviewer.	dummyTurk
319	0.0	 We will add a figure as suggested by the reviewer.	dummyTurk
320	0.0	As shown in Figure 3, we did do a preference comparison of SEGAN vs. ground-truth and AccentGAN vs. SEGAN.	dummyTurk
321	0.0	We observe that the humans considered AccentGAN to be more natural as compared to SEGAN and we can incorporate a histogram for the same in a revised version.	dummyTurk
322	0.0	Q1: AccentGAN can generate speech over relatively long time scales, and lets us control the accent.	dummyTurk
323	0.0	These features are unique to AccentGAN among GAN-based speech generating systems.	dummyTurk
324	0.0	Q2: Preference score is used to evaluate if the generated speech has the correct accent -- not just whether it sounds natural.	dummyTurk
325	0.0	As explained in Section 6.2, this is done by presenting the listeners with 3 speech segments -- a reference segment of natural speech in the target accent, and two candidates, of which one is again natural speech in the target accent and the other is synthesized speech -- and asking them to give their preference as to which of the two candidates sounds like the reference in terms of accent.	dummyTurk
326	0.0	If the generated speech does as well as the natural speech (neutral preference) this indicates that the model is generating speech in the correct accent.	dummyTurk
327	0.0	We would like to thank you for your helpful review.	dummyTurk
328	0.0	We will add a more analysis of the prompts.	dummyTurk
329	0.0	20 prompts were randomly chosen from a StarTrek test set.	dummyTurk
330	0.0	Regard evaluators, we will add inter-annotator agreement.	dummyTurk
331	0.0	This may not have been sufficiently clear, Twitter was used only for training the classifier.	dummyTurk
332	0.0	"The backoff solution may initially sound problematic; however,  it is be better to return a topical response and add words like ""sir"" or ""Jim""."	dummyTurk
333	0.0	A rule-based system would not be able to be on topic.	dummyTurk
334	0.0	Furthermore, the perplexity filter will remove responses which are too far from the StarTrek language model.	dummyTurk
335	0.0	Interactive system comparison is quite difficult to assess, and one loses the statistical power of A vs B comparisons.	dummyTurk
336	0.0	Nonetheless, it is our goal to publish our chatbot to Alexa and FB messenger to collect more data.	dummyTurk
337	0.0	Regarding the Cornell Movie Dialog Corpus, we chose science fiction.	dummyTurk
338	0.0	Regarding Twitter, we used a random subset.	dummyTurk
339	0.0	Thank you for pointing out the unclear parts of our paper.	dummyTurk
340	0.0	We omitted some details because of the lack of space, but we will definitely reorganize the paper a bit, and take advantage of the extra page, to add all the necessary information.	dummyTurk
341	0.0	 In sequence alignment, we align a word in a modern language with its proto-word.	dummyTurk
342	0.0	From the alignment we extract the labels for each character of the modern word, to be used in the sequence labeling task.	dummyTurk
343	0.0	This is done only in the training phase, where we have access to proto-words.	dummyTurk
344	0.0	For the testing phase, our goal is to obtain these labels automatically, to reconstruct the proto-word.	dummyTurk
345	0.0	Training example	dummyTurk
346	0.0	 The data looks as below (extract from Ciobanu and Dinu 2014).	dummyTurk
347	0.0	Each line contains a cognate set.	dummyTurk
348	0.0	The first line represents the word 'absence' and the second one 'eagle'.	dummyTurk
349	0.0	The words are not aligned at character level here, this is why we employ an algorithm for this in Section 2.1.	dummyTurk
350	0.0	ROMANIAN | FRENCH | ITALIAN | SPANISH | PORTUGUESE | LATIN	dummyTurk
351	0.0	absen???? | absence | assenza | ausencia | aus??ncia | absentia	dummyTurk
352	0.0	acvil?? | aigle | aquila | ??guila | ??guia | aquila	dummyTurk
353	0.0	 The edit distance is the number of different characters between the proto-word our system produces (only the first word from the output n-best list) and the real proto-word (normalized in [0,1] by dividing to the length of the longest string).	dummyTurk
354	0.0	The average edit distance is the sum of all edit distances computed for the test dataset, divided by the size of the test dataset.	dummyTurk
355	0.0	As an example from Table 2, for the Italian word vicino, our system has vicinum as the first production, while the correct proto-word is vicinus (in bold).	dummyTurk
356	0.0	So the edit distance here, computed between vicinum and vicinus is 1/7.	dummyTurk
357	0.0	 We mention at line 239 that the second ensemble performs best.	dummyTurk
358	0.0	So we reported results for the second fusion method.	dummyTurk
359	0.0	We will make the information more clear.	dummyTurk
360	0.0	 We did not have access to the dataset used in their more recent work.	dummyTurk
361	0.0	But we are definitely interested in extending to other language families as well.	dummyTurk
362	0.0	 The datasets are available from their respective authors, by request.	dummyTurk
363	0.0	We will sort out the license problem.	dummyTurk
364	0.0	Our idea is to release the code with the most permissive license that the third party libraries we used allow us.	dummyTurk
365	0.0	 We will do this with the additional page afforded to addressing reviewer comments.	dummyTurk
366	0.0	 The submitted manuscript is 8 pages long.	dummyTurk
367	0.0	There is an additional page and a half that is part of the permitted supplementary material (submitted in another document).	dummyTurk
368	0.0	We thank the reviewer for the time spent in preparing the review	dummyTurk
369	0.0	and for the positive score.	dummyTurk
370	0.0	If the reviewer finds the paper	dummyTurk
371	0.0	interesting, we ask the reviewer to hold the ground and argue	dummyTurk
372	0.0	for acceptance.	dummyTurk
373	0.0	"""Only accuracy is reported, no precision or recall."""	dummyTurk
374	0.0	Many-to-one accuracy is a standard and often the only metric	dummyTurk
375	0.0	used in unsupervised POS tagging (Berg-Kirkpatrick et al., 2010;	dummyTurk
376	0.0	Christodoulopoulos et al., 2010; Stratos et al., 2016) as explained	dummyTurk
377	0.0	in lines 514-521.	dummyTurk
378	0.0	We choose to fix the evaluation metric so that	dummyTurk
379	0.0	we can conform to the existing literature and focus on model	dummyTurk
380	0.0	comparison.	dummyTurk
381	0.0	"""No error analysis in the results section."""	dummyTurk
382	0.0	There is a section titled Error Analysis for Fine-Grained POS	dummyTurk
383	0.0	Tagging in the supplementary material.	dummyTurk
384	0.0	We moved this out due to	dummyTurk
385	0.0	space constraints.	dummyTurk
386	0.0	We will strengthen it and incorporate it back	dummyTurk
387	0.0	in the final version.	dummyTurk
388	0.0	The recent copy-and-generate-based seq2seq model did suffer this problem, but our MAT system has a 5% relative improvement of novel n-grams in comparison to the model without the multitask part, so this is a significant step to be more like human abstraction.	dummyTurk
389	0.0	We manually checked the training data.	dummyTurk
390	0.0	The human summaries did have a large proportion of copy from the source texts, with some modification, so it is hard to produce more abstractive summaries with this training data.	dummyTurk
391	0.0	To generally agree, it is obvious that we should try to produce a more abstractive model; this is a key issue of our future research.	dummyTurk
392	0.0	Please refer to answers below.	dummyTurk
393	0.0	We will release all the necessary code, checkpoint of trained model and the generated summaries on test set once accepted.	dummyTurk
394	0.0	In this paper, we use the same settings as the recent baseline papers including (See et al., 2017; Paulus et al., 2017; Liu et al., 2017).	dummyTurk
395	0.0	"We used pyrouge, a Python package, to compute all ROUGE scores with parameters ""-c 95 -2 -1 -U -r 1000 -n 2 -w 1.2 -a""."	dummyTurk
396	0.0	All our ROUGE scores have a 95% confidence interval of at most ??0.25 as reported by the official ROUGE script:	dummyTurk
397	0.0	rouge_1_f_score: 40.74 with confidence interval (40.53, 40.96)	dummyTurk
398	0.0	rouge_2_f_score: 18.14 with confidence interval (17.91, 18.36)	dummyTurk
399	0.0	rouge_L_f_score: 37.15 with confidence interval (36.93, 37.35)	dummyTurk
400	0.0	We have to point out that our model out-performs even sophisticated designed extractive models with reinforcement learning (Narayan et al., 2018).	dummyTurk
401	0.0	Our model improves 1.2 absolute point in comparison to (See et al., 2017).	dummyTurk
402	0.0	This is an acceptable improvement for recent papers as the CNN/Dailymail dataset is one of the most challenging datasets in summarization area.	dummyTurk
403	0.0	It is difficult to boost 1 absolute point ROUGE score on abstractive summarization on CNN/Dailymail (Paulus et al., 2017).	dummyTurk
404	0.0	Extractive summarization typically has higher readability than abstractive summarization, since it produces summaries by choosing a subset of the sentences in the original text.	dummyTurk
405	0.0	In the revision, we will add the human evaluation of the extractive methods.	dummyTurk
406	0.0	2 graduate students and 1 undergraduate student were invited to perform human evaluation.	dummyTurk
407	0.0	The readability and quality are given in the form of the ranking of summaries generated by various methods.	dummyTurk
408	0.0	For the quality part, the human evaluator will read through the summaries to try to get a sense of what the story is talking about.	dummyTurk
409	0.0	If the human evaluator does not get an idea about what the original story is talking about, then this summary will be ranked lower.	dummyTurk
410	0.0	Also, for the readability part, if the summary is not well-written (e.g., grammatical mistakes), it will have a lower ranking score.	dummyTurk
411	0.0	The human evaluators are required to perform ranking of summaries taken the above 2 components into consideration.	dummyTurk
412	0.0	To address the comments of reviewers, we will give 2 separate scores regarding the quality and readability in the camera-ready version.	dummyTurk
413	0.0	We mainly compare with the neural methods because recent deep learning based methods achieve state-of-the-art in keyphrase extraction without carefully designed features.	dummyTurk
414	0.0	Rule-based methods will be compared in the revised version to make our experiment more convincing.	dummyTurk
415	0.0	z is the topic distribution of the document with k topics represented by k probabilities.	dummyTurk
416	0.0	It is obtained using pre-trained LDA model as stated in Section 5.2.	dummyTurk
417	0.0	This reviewer totally accepted our contribution without comment.	dummyTurk
418	0.0	At the time of writing, it wasn???t possible to make the dataset available, mainly because the data was owned by the company.	dummyTurk
419	0.0	We are still in the process of negotiating with the company for a release of the dataset entirely or partially.	dummyTurk
420	0.0	Yes, there are different avenues to explore for different algorithms and datasets.	dummyTurk
421	0.0	Nonetheless, we thought that the current result would be a good contribution as a short ACL paper because it has a novel idea that has been proven with a reasonable test collection.	dummyTurk
422	0.0	As we have explained in the paper, there are rarely parallel data for sentiment modification.	dummyTurk
423	0.0	The lack of gold standard data forces all existing work to evaluate the transformation accuracy with the help of a pretrained classifier.	dummyTurk
424	0.0	The evaluation is worthy of study.	dummyTurk
425	0.0	We will study this issue further in the future work.	dummyTurk
426	0.0	The task is the sentence-level sentiment modification.	dummyTurk
427	0.0	However, the original review of the dataset includes many sentences.	dummyTurk
428	0.0	In most cases, the first sentence of a paragraph is a topic sentence, so we only keep the first sentence as the input text.	dummyTurk
429	0.0	We first train a classifier to classify the sentiments of the sentences.	dummyTurk
430	0.0	The sentiments of some sentences are ambiguous.	dummyTurk
431	0.0	We then filtered the sentences to which the classifier cannot assign a high probability (>0.8) of the sentiment polarity.	dummyTurk
432	0.0	Our proposed system will be released to facilitate other researchers.	dummyTurk
433	0.0	Thanks for your suggestion.	dummyTurk
434	0.0	The over claimed representations will be revised or removed in the next version.	dummyTurk
435	0.0	We randomly sampled 500 items to conduct the human evaluation.	dummyTurk
436	0.0	Each item contains an input and three outputs generated by different systems.	dummyTurk
437	0.0	The items are distributed to 3 annotators.	dummyTurk
438	0.0	They are all English native speakers.	dummyTurk
439	0.0	In addition, before human evaluation, we randomly sampled 10 items.	dummyTurk
440	0.0	The annotators are asked to evaluate the outputs of these 10 items together.	dummyTurk
441	0.0	The evaluation results are required to be consistent to achieve the inter-annotator agreement.	dummyTurk
442	0.0	We thank the reviewer for the valuable suggestions.	dummyTurk
443	0.0	"We relied on ""Nap"" dataset to develop intuition for designing the algorithm and tuning of MLN rules."	dummyTurk
444	0.0	"All other datasets (""ACE"", ""BoH"", ""Fas"", and ""Mao"") are unseen, independent test datasets."	dummyTurk
445	0.0	Details of the dataset are given in the supplementary material (Table A2 in Appendix).	dummyTurk
446	0.0	 Many state-of-the-art, especially neural-network-based approaches are not applicable in production due to their complexity.	dummyTurk
447	0.0	Instead, what is used here is SVD-based dimensionality reduction, so the aim of our work was to present the new dimensionality reduction technique that utilizes additional information which beats standard SVD.	dummyTurk
448	0.0	 Not quite understand about what restrictions it is said in the argument.	dummyTurk
449	0.0	We will correct the writing carefully in final submission.	dummyTurk
450	0.0	Since this is a short paper and the focus of this paper is a neural network based approach, we didn???t give a entire background discussion about different dependency parsing models in our first draft.	dummyTurk
451	0.0	Graph-based dependency parsing is definitely a point worth notice, we will add it in in our final submission.	dummyTurk
452	0.0	Chen and Manning???s work is still a dependency parser work, which may not be able to be transferred to NER task.	dummyTurk
453	0.0	The binary tree dependency is from CFG parser???s requirement as used for the NER task in the paper of Finkel and Manning, 2009.	dummyTurk
454	0.0	The three layer???s decision is data-driven.	dummyTurk
455	0.0	The RNN model will stop if a word reached its last level (which is its POS tag following the format given by Finkel and Manning, 2009)	dummyTurk
456	0.0	Thanks for your valuable reviews and hope the clarification helps.	dummyTurk
457	0.0	 We did not manage to come up with  the intrinsic evaluation for a accented transcriptions generation problem.	dummyTurk
458	0.0	Possible extrinsic evaluation may be an accent classification.	dummyTurk
459	0.0	We can split an original GMU corpus for train/dev/test sets and evaluate the classification quality.	dummyTurk
460	0.0	Then enhance the training set with the generated samples and measure quality again.	dummyTurk
461	0.0	The problem for this technique is the size of GMU corpus (about 2000 samples), so we can possibly try to use another corpus for evaluation.	dummyTurk
462	0.0	Thank you for your comments.	dummyTurk
463	0.0	In fact, easy-first parsing can be regarded as a non-directional version of classical Arc-Standard stack-based parsing.	dummyTurk
464	0.0	We think that easy-first parsing is relatively intuitive and due to space limitation, we have not included too many details of it.	dummyTurk
465	0.0	We will add more descriptions for it.	dummyTurk
466	0.0	About the neural model, we adopted a similar architecture to the one of (Kiperwasser and Goldberg, 2016), that is, collecting word representations from LSTM and feeding them to MLP for scoring.	dummyTurk
467	0.0	Since modeling is not our focus and space is limited, we did not provide too many descriptions for it.	dummyTurk
468	0.0	Please notice that we will open our source code and all the details can be found there.	dummyTurk
469	0.0	About the formula at line 505, according to the current form, $\alpha$ should be even.	dummyTurk
470	0.0	But we can address this by taking the absolute value of $\Delta$, and then $\alpha$ will not be restricted.	dummyTurk
471	0.0	Thank you for denoting this and we will modify it correspondingly.	dummyTurk
472	0.0	Thanks for the detailed and thoughtful review.	dummyTurk
473	0.0	We are especially happy that you thought the work is interesting, the explanations are clear, and the results are convincing.	dummyTurk
474	0.0	We thank the reviewer for their thorough work and supporting review.	dummyTurk
475	0.0	As noted in our response to reviewer 3 - our results on the Spanish dataset of CoNLL2002 are on par with the Carreras et al 2002 model stated in the NAACL 2016 paper provided by reviewer 3 (which is a SOTA non-neural NER model) - although we used a very basic set of features and apply very limited task-specific tuning to our models.	dummyTurk
476	0.0	More generally, the comparison to SOTA in our case is not trivial.	dummyTurk
477	0.0	For example this paper: https://arxiv.org/abs/1801.09851 presents SOTA numbers for the JNLPBA and BC2GM datasets.	dummyTurk
478	0.0	While these results are substantially better than those we report, they build on an ensemble of three different models, each capturing a different set of features.	dummyTurk
479	0.0	In contrast, our goal in this paper is to demonstrate the applicability of our ideas for a variety of tasks and datasets, we did not aim to select a task specific set of features.	dummyTurk
480	0.0	Our strategy was hence to select the feature set that we consider standard for sequence labeling tasks in NLP - which indeed provides SOTA and near-SOTA results for the CoNLL2002 NER and the CoNLL2000 chunking datasets.	dummyTurk
481	0.0	Having said all that, we realize that a discussion of how our results relate to the SOTA is important for the completeness of our presentation.	dummyTurk
482	0.0	Given the opportunity we will provide this discussion in the camera ready version.	dummyTurk
483	0.0	" We did implement a CRF model (see  lines 284-296) but, as the reviewer says, trained it with ""argmax algorithms"" rather than with a probabilistic log-likelihood approach."	dummyTurk
484	0.0	We will add a comparison to that approach to the camera ready version, given the opportunity.	dummyTurk
485	0.0	 We agree with the reviewer that additional theoretical discussion will improve the paper.	dummyTurk
486	0.0	One of the reasons we aimed for a short paper is that we do not have theoretical justification for our method (as stated in the paper) - but a discussion can put our work in a more concrete context and facilitate a future fruitful work.	dummyTurk
487	0.0	One thing we do want to note is that the various algorithms we work with do optimize an objective - in our implementation this is the CRF objective described in lines 284-296.	dummyTurk
488	0.0	It is true that we do not optimize this objective using a probabilistic log-likelihood approach.	dummyTurk
489	0.0	"However, perceptron and MIRA (as well as their SWV extensions) do that using an ""argmax-based"" approach."	dummyTurk
490	0.0	Actually, it can be shown that the update rule of the argmax approaches approximates the gradient computation of the log-likelihood model (and hence the argmax approaches are in fact gradient methods over the log-likelihood objective, but with approximated gradients).	dummyTurk
491	0.0	We will discuss this in more details in the camera ready version, given the opportunity (some of this discussion is already made in the Collins EMNLP 2002 paper on the structured perceptron) .	dummyTurk
492	0.0	Response to additional comments: Thanks for these good comments.	dummyTurk
493	0.0	We will devote the extra page given in the camera ready to discuss the efficiency of each weighting scheme and to consider the idea of softmin with a temperature parameter.	dummyTurk
494	0.0	We will also improve the writing, particularly with respect to JJ-x and the mentioned theorem.	dummyTurk
495	0.0	---------------	dummyTurk
496	0.0	It is important to note that 2-duplicate-decoders model does not share parameters across the decoders, and hence the entire model has more parameters (35,316,688) than our final closedbook-2-decoders model (34,398,160).	dummyTurk
497	0.0	Moreover, our initial experiments with more layers (on both encoder and decoder) in the baseline model does not lead to any significant improvements.	dummyTurk
498	0.0	This shows that just more parameters is not the main motivation of our improvements, but this is rather to force the model to have better memory for summarization (leading to better saliency, out-of-domain generalization, as well as higher abstractiveness scores-- see our Reviewer1 replies).	dummyTurk
499	0.0	Increased sizes of hidden layer and word embeddings is an orthogonal idea and should improve both the baseline as well as the closedbook-2-decoders model.	dummyTurk
500	0.0	---------------	dummyTurk
501	0.0	Our closed-book decoder is exactly the same as the pointer decoder except without attention layer and pointer mechanism.	dummyTurk
502	0.0	They share a single vocabulary of 50k words and OOV are simply represented as [UNK] with no other tricks.	dummyTurk
503	0.0	Dimensions of memory states and hidden states are also the same (256) for both decoders.	dummyTurk
504	0.0	---------------	dummyTurk
505	0.0	Pg(See) is the result reported in SeeEtAl2017???s paper, while pg(baseline) is obtained by running SeeEtAl2017???s github code.	dummyTurk
506	0.0	Our model achieved statistically-significant improvements (p<0.001) even on the higher-reported SeeEtAl2017???s results.	dummyTurk
507	0.0	---------------	dummyTurk
508	0.0	We use saliency metric to show our closed-book-decoder model can enhance the encoder memory states so as to remember/recover more salient information from the source text into output summary, which is a very important aspect of optimal summary generation.	dummyTurk
509	0.0	The saliency measure is based on the original CNNDM cloze-blank-QA-HermannEtAl2015 keywords--these keywords are important entities like countries, names, etc.	dummyTurk
510	0.0	that were extracted from the reference summaries (ground truth).	dummyTurk
511	0.0	We will add more details in the final version???s extra page.	dummyTurk
512	0.0	---------------	dummyTurk
513	0.0	Our main argument is that training the closed-book decoder along with the pointer decoder can enhance the representation power of encoder, because encoder???s final memory state is the only information about the source text that is available to the closed-book decoder (no shortcut via attention).	dummyTurk
514	0.0	We prove some of this effect via amplified (1.26 times) gradient flows through the encoder???s memory state.	dummyTurk
515	0.0	We next show that this also leads to higher saliency scores, without simply generating longer summaries, as well as with higher abstractiveness.	dummyTurk
516	0.0	We also proved the stronger representation power of our 2-decoder???s encoder (please see all 4 details in Reviewer1 replies).	dummyTurk
517	0.0	We???ll make the intuitions more concise and clearer in the final version???s extra page.	dummyTurk
518	0.0	Thank you for the valuable feedback.	dummyTurk
519	0.0	== Comparison with contemporary techniques on the same dataset ==	dummyTurk
520	0.0	Our original idea was to conduct fair and direct comparison such that the effectiveness of proposed algorithms is clearly evaluated.	dummyTurk
521	0.0	To avoid repeating the same information, please refer to our response to Reviewer 1 to see the comparison with contemporary techniques.	dummyTurk
522	0.0	For machine translation, our implementation is clearly more efficient than previous methods (better MLE results with smaller/same architecture).	dummyTurk
523	0.0	More importantly, the proposed ERAC consistently outperforms AC with or without the input feeding technique.	dummyTurk
524	0.0	For image captioning, our proposed method achieves comparable performance with self-critic training with a smaller architecture.	dummyTurk
525	0.0	== Human evaluation ==	dummyTurk
526	0.0	We do wish we had the resource and time to conduct a reliable human evaluation.	dummyTurk
527	0.0	However, due to the lack of resources, we follow previous works in developing algorithms for sequence prediction in NLP and rely on the automatic evaluation.	dummyTurk
528	0.0	== Metric for image captioning ==	dummyTurk
529	0.0	We present BLEU in the paper because it is the metric used for training.	dummyTurk
530	0.0	Actually, we have the results for other metrics, which are summarized below.	dummyTurk
531	0.0	<table>	dummyTurk
532	0.0	<tbody>	dummyTurk
533	0.0	<tr><td>Methods</td><td>| BLEU4</td><td>| ROUGE-L</td><td>| Meteor</td><td>| CIDEr</td></tr>	dummyTurk
534	0.0	<tr><td></td></tr>	dummyTurk
535	0.0	<tr><td>RAML</td><td>| 29.83 +/- 0.21</td><td>| 51.66 +/- 0.11</td><td>| 23.84 +/- 0.11</td><td>| 88.88 +/- 0.62</td></tr>	dummyTurk
536	0.0	<tr><td>VAML</td><td>| 29.93 +/- 0.22</td><td>| 51.72 +/- 0.14</td><td>| 23.84 +/- 0.11</td><td>| 88.84 +/- 0.58</td></tr>	dummyTurk
537	0.0	<tr><td>AC</td><td>| 30.90 +/- 0.20</td><td>| 52.34 +/- 0.13</td><td>| 23.59 +/- 0.10</td><td>| 89.01 +/- 0.61</td></tr>	dummyTurk
538	0.0	<tr><td>ERAC</td><td>| 31.44 +/- 0.22</td><td>| 52.66 +/- 0.07</td><td>| 23.78 +/- 0.05</td><td>| 90.52 +/- 0.53</td></tr>	dummyTurk
539	0.0	</tbody>	dummyTurk
540	0.0	</table>	dummyTurk
541	0.0	We appreciate very much for your positive comments and your steady support.	dummyTurk
542	0.0	We really really appreciate if you read all our rebuttals and continue to support us during the discussion phase.	dummyTurk
543	0.0	Thank you very much for your kind help!	dummyTurk
544	0.0	 Lead-3 is particularly suitable for news documents of the CNN/DailyMail dataset, with important sentences at the beginning.	dummyTurk
545	0.0	We in particular have some discussions in Section 4.5, and we will elaborate more in the revised version.	dummyTurk
546	0.0	 As to unsupervised learning, we set an objective function to learn patterns automatically from the data.	dummyTurk
547	0.0	For traditional unsupervised summarization methods, they are basically built on human intuitions and/or features by human expertise.	dummyTurk
548	0.0	"For instance, the well-known graph-based summarization methods are unsupervised, based on the intuition that good summary sentences will be similar to many other sentences within the document (analogously to ""majority voting"" to some extent)."	dummyTurk
549	0.0	 Since document have variance in lengths and contain different numbers of words.	dummyTurk
550	0.0	We choose 50 words and 30 sentences because the setting is applicable to almost all documents in the dataset.	dummyTurk
551	0.0	We will make it clearer in the revised version.	dummyTurk
552	0.0	 For Line 686, we indicate that learning-based methods (including our proposed method) are likely to have superiority than intuition-based methods such as Lead-3.	dummyTurk
553	0.0	Lead-3 will be quite effective for news with important sentences occurring at the beginning of the document.	dummyTurk
554	0.0	For documents with important sentences at the end, the intuition-based methods are not really so adaptive.	dummyTurk
555	0.0	Thank you very much, again, for your support!	dummyTurk
556	0.0	Thank you for your helpful comments.	dummyTurk
557	0.0	We would like to share that this is a part of our ongoing work.	dummyTurk
558	0.0	Our proposed solution is simple but allows specialized handling for entities and can be used in parallel with other proposed approaches.	dummyTurk
559	0.0	We are currently working on 2 areas ???	dummyTurk
560	0.0	a) additional experiments on other datasets	dummyTurk
561	0.0	We are performing experiments on (i) Multi-turn multi-domain dataset (https://arxiv.org/abs/1705.05414) and (ii) Persona-Chat dataset (https://arxiv.org/abs/1801.07243).	dummyTurk
562	0.0	Both datasets are human-human dialogs where entities play a crucial role and we are evaluating the performance of our proposed solution on top of the baselines proposed for each dataset.	dummyTurk
563	0.0	b) combining our solution with existing approaches	dummyTurk
564	0.0	We are also exploring our proposed solution in a generative setting, where the system utterance is generated word-by-word.	dummyTurk
565	0.0	We are currently exploring a pointer network (https://arxiv.org/abs/1506.03134) architecture, where the separate handling for entities would help as instead of performing attention over all the words in the context, we would need to perform attention only over the set of entities encountered so far.	dummyTurk
566	0.0	Through these additional experiments, we are working on strengthening our idea and demonstrating its use on more complex human-human conversations as well as improving natural language generation.	dummyTurk
567	0.0	This work is a first step in the direction towards incorporating prior knowledge for goal-oriented dialog.	dummyTurk
568	0.0	In addition to the ongoing work mentioned above, we are also exploring other areas such as using man pages and other external sources for improving accuracy on the Ubuntu dataset.	dummyTurk
569	0.0	Reply to weakness argument 1	dummyTurk
570	0.0	We appreciate this comment.	dummyTurk
571	0.0	Based on our observations, the implicit patterns are highly language and data/annotation dependent.	dummyTurk
572	0.0	The patterns reside in the rich set of features and we believe it is non-trivial to summarize such patterns with explicit linguistic patterns or grammar rules.	dummyTurk
573	0.0	Reply to weakness argument 2	dummyTurk
574	0.0	Thanks for your question on the B-type evaluation.	dummyTurk
575	0.0	We note that none of the prior works reported results under this evaluation metric.	dummyTurk
576	0.0	We will make this clear in the revised version.	dummyTurk
577	0.0	Reply to Question 1	dummyTurk
578	0.0	Please see our reply to argument 1 above.	dummyTurk
579	0.0	Reply to Question 2	dummyTurk
580	0.0	Please see our reply to argument 2 above.	dummyTurk
581	0.0	Reply to Additional Comments	dummyTurk
582	0.0	Thanks for the helpful suggestions.	dummyTurk
583	0.0	We will move some content from the supplementary material to the main body of the paper when more space is available, and will improve the writing of this paper.	dummyTurk
584	0.0	We used a support vector machine for classification experiments.	dummyTurk
585	0.0	We used the vector as the features for the classification, and used kfold-validation technique to obtain the best classifier.	dummyTurk
586	0.0	This classifier was then used for training and testing.	dummyTurk
587	0.0	The goal of this paper is to provide an alternative way of computing text embedding from the pre-trained word embedding without having to perform deep learning on the dataset.	dummyTurk
588	0.0	Our model works well on datasets that are deemed small for deep architectures to work and belong to special domains for which transfer learning is not possible.	dummyTurk
589	0.0	We are providing a way to obtain task-specific optimal weight computation of the words that is easier, faster, and cheaper than deep learning-based approaches and comparable-to-better performance than simpler text embedding approaches	dummyTurk
590	0.0	We used sentence embedding vector as the features for the classification.	dummyTurk
591	0.0	No other features were used in the model	dummyTurk
592	0.0	As our goal was to provide an optimal text embedding without using complex architectures, like neural networks, and learning embeddings from datasets, we didn't compare our results against such embeddings.	dummyTurk
593	0.0	As you can see in our results table, majority of our datasets were too small for application of deep embedding models, which is usually the case in many domains.	dummyTurk
594	0.0	We are using existing datasets for the paper.	dummyTurk
595	0.0	References to these datasets can be seen in the datasets table.	dummyTurk
596	0.0	We generated train and test sets from these datasets.	dummyTurk
597	0.0	We can add more description for these datasets, if needed.	dummyTurk
598	0.0	We did try simple feed forward networks (2-3 hidden layers), however, although they beat the KL-divergence and frequency measures, their performance was quite poor even on the NYT dataset.	dummyTurk
599	0.0	Sequence models seem to work much better for this task.	dummyTurk
600	0.0	The fact that our model generalizes across genres and tasks if trained on news text does not mean that it can be trained on any type of input text.	dummyTurk
601	0.0	In our opinion, a news corpus was the best genre to train on because of the topic diversity in any news corpus.	dummyTurk
602	0.0	We would expect many concepts from the real world to be captured in the news corpus, thereby making it easy for the models to perform in a different genre like fiction.	dummyTurk
603	0.0	We were also surprised with the performance drop for the GRU with increasing context size.	dummyTurk
604	0.0	We are currently investigating this.	dummyTurk
605	0.0	The significance test performed in the Front Page headline detection task was the t-test for comparing means.	dummyTurk
606	0.0	In our opinion, it doesn't make sense to train on Wattpad because the notion of importance in these summaries of fiction is more specific, and the worlds in which they take place are also often fictional (e.g., a fantasy world, or a world with werewolves and vampires).	dummyTurk
607	0.0	So, one would expect that importance derived from the real world to transfer to fictional worlds, because writers are all aware of the real world when writing their fiction, but not the other way around, as each fictional world is specific to its own piece of writing.	dummyTurk
608	0.0	If we trained on Wattpad, we would get better performance on Wattpad dataset surely, but we would expect our performance to be a lot worse on CNN and NYT datasets.	dummyTurk
609	0.0	Thank you for your review.	dummyTurk
610	0.0	"We trained all the three model with pseudo data as in section 5.2: ""We reimplement LSTM-Char-CNN and scRNN and train them with the same pseudo data."""	dummyTurk
611	0.0	We are sorry for the lack of comparision experiments without pseudo data.	dummyTurk
612	0.0	We will improve this next.	dummyTurk
613	0.0	Thank you for your careful evaluations and detailed comments about our work.	dummyTurk
614	0.0	We will try our best to improve the quality of our manuscript.	dummyTurk
615	0.0	 We agree with you that the title with ???Dynamic Skipping??? is a little confusing.	dummyTurk
616	0.0	We are willing to consider the change of title and relevant description.	dummyTurk
617	0.0	It's worth noting that when lambda = 1, the proposed model becomes a pure dynamic LSTM.	dummyTurk
618	0.0	In addition, Zhang et al.	dummyTurk
619	0.0	(2016) defined Skip Coefficient as a function of the ???shortest path??? from time i to time i + n, regardless of whether the model exploits all states of a LSTM, just like our model setting.	dummyTurk
620	0.0	It seems that the unclear title may have confused you, we apologize for that.	dummyTurk
621	0.0	We will proofread our paper and give more explicit explanations.	dummyTurk
622	0.0	 The main difference is that the Jump LSTM cannot be used for sequence labeling tasks, because the Jump LSTM does not produce outputs for the skipped tokens.	dummyTurk
623	0.0	We have made a detailed comparison between these two models in Sections 3.3 and 4.	dummyTurk
624	0.0	Regarding the skipping technique, 1) the state of the reinforcement learning agent is different.	dummyTurk
625	0.0	The Jump LSTM uses h_t as state of the agent, while the proposed model uses h_{t-1} and x_t instead; 2) the action of the agent is also different.	dummyTurk
626	0.0	The Jump LSTM decides to jump a few steps forward, while the proposed model decides which previous state should be used to compute recurrent transition functions.	dummyTurk
627	0.0	1) Some tasks like language modeling cannot see the whole sentence ahead of time.	dummyTurk
628	0.0	2) If we used the full information of a sentence to decide the skipping steps, then the proposed model may need the LSTM to model the sentence twice ??? once to obtain sentence information, and again for deciding the skip actions.	dummyTurk
629	0.0	Thus, the time complexity may increase significantly.	dummyTurk
630	0.0	Therefore, the proposed method uses h_{t-1} and x_t to decide skipping steps.	dummyTurk
631	0.0	 We agree with you to report the result of vanilla LSTM.	dummyTurk
632	0.0	The F1-score of BLSTM-CNN is 89.36.	dummyTurk
633	0.0	 The CRFs can model the dependency between labels, not the input tokens.	dummyTurk
634	0.0	On the contrary, the proposed method can model the dependency between tokens.	dummyTurk
635	0.0	In addition, the proposed method can better tackle the gradient vanishing problem, as shown in Figure 4.	dummyTurk
636	0.0	Therefore, the proposed method is better.	dummyTurk
637	0.0	 The training time of proposed method is 1.25 times that of vanilla LSTM.	dummyTurk
638	0.0	The Jump LSTM makes 1.6x speedup compared to LSTM.	dummyTurk
639	0.0	Campos et al.	dummyTurk
640	0.0	(2017) did not report the training time of skip LSTM.	dummyTurk
641	0.0	We will report the time complexity in detail.	dummyTurk
642	0.0	Reference: Zhang, Saizheng, et al.	dummyTurk
643	0.0	"""Architectural complexity measures of recurrent neural networks."""	dummyTurk
644	0.0	Advances in Neural Information Processing Systems.	dummyTurk
645	0.0	2016.	dummyTurk
646	0.0	 The same/similar dataset has been exploited by a number of studies in the last year, including Li et al.	dummyTurk
647	0.0	(2017), Chen et al.	dummyTurk
648	0.0	(2017a) , Chen et al.	dummyTurk
649	0.0	(2017b), Wu et al.	dummyTurk
650	0.0	(2017a), Wu et al.	dummyTurk
651	0.0	(2017b), Yang et al.	dummyTurk
652	0.0	(2017) and Zhou et al.	dummyTurk
653	0.0	(2017) as mentioned in our work.	dummyTurk
654	0.0	Thus we believe this dataset is reasonable.	dummyTurk
655	0.0	In addition, we have also conducted experiments on the En-De dataset from WMT16, which show a similar tendency as Ch-En.	dummyTurk
656	0.0	Due to the limited space, we are unable to include them in this version.	dummyTurk
657	0.0	  We agree that a stronger baseline is better, and actually we do make the baseline much stronger by ensemble.	dummyTurk
658	0.0	We agree you can list a number of other techniques to make the baseline stronger (e.g.	dummyTurk
659	0.0	more layers, UNK replacement, subword units and coverage).	dummyTurk
660	0.0	Is it necessary to include all these techniques?	dummyTurk
661	0.0	Several of these techniques can bring extra issues which are not closely-related to our claims.	dummyTurk
662	0.0	For example, if we use more layers, several questions will come	dummyTurk
663	0.0	 Our proposed approaches are new to the NMT community.	dummyTurk
664	0.0	We focus on NMT and are the first to apply the three methods on NMT, comparing and analyzing them in detail.	dummyTurk
665	0.0	 It aims to explain the benefit of Tree-Linearization from the linguistic perspective, which lies in that it handles function words better.	dummyTurk
666	0.0	We explain your opposite issue of Tree-Linearization theoretically and empirically.	dummyTurk
667	0.0	As known, the attention mechanism is used to solve word alignments together with translation jointly.	dummyTurk
668	0.0	Theoretically, most words are still aligned to the same original words (e.g., modern ??? ??????) in Tree-Linearization, and only several high-frequency function words are aligned to high-frequency reduce actions with less ambiguities, while their suitable source words are difficult to be determined as discussed in the paper.	dummyTurk
669	0.0	Thus the increasing difficulties are relatively small (maybe even easier as we reduce the alignment ambiguities).	dummyTurk
670	0.0	Empirically, the resulting word alignments are good enough with the same scale of parallel corpus (the word alignments of other words in the example is similar among our proposed models), bringing better performances.	dummyTurk
671	0.0	We will describe this point clearly in this Section.	dummyTurk
672	0.0	Thanks for your comments, we agree it is possible to further improve the performance of the pruned model.	dummyTurk
673	0.0	Still, LD-Net demonstrates the capability to effectively prune PTLMs without retraining.	dummyTurk
674	0.0	To make our model more applicable, we try to conduct the compression without retraining.	dummyTurk
675	0.0	This setting makes our task even more challenging.	dummyTurk
676	0.0	For example, we???ve tried a very recent sparse regularization technique (Louizos et al., 2017), however, its performance is even worse.	dummyTurk
677	0.0	We will add this variant as a baseline in the revised version, if it helps demonstrate the effectiveness of LD-Net.	dummyTurk
678	0.0	In our future work, we will explore other principles for better effectiveness and efficiency.	dummyTurk
679	0.0	Louizos, Christos, Max Welling, and Diederik P. Kingma.	dummyTurk
680	0.0	"""Learning Sparse Neural Networks through L_0 Regularization."""	dummyTurk
681	0.0	International Conference on Learning Representations Accepted as poster.	dummyTurk
682	0.0	https://openreview.net/forum?id=H1Y8hhg0b.	dummyTurk
683	0.0	We will make the source code available upon the acceptance of the paper.	dummyTurk
684	0.0	We simply forgot to mention this.	dummyTurk
685	0.0	We are strongly committed to open source and sharing software to more effectively advance research.	dummyTurk
686	0.0	We believe that, to make the replication of the results easier, especially for neural models, it is important to make the code available.	dummyTurk
687	0.0	We will also provide the split of the data that we used.	dummyTurk
688	0.0	We completely agree with the reviewer: variation of the language and the target NE type to be recognized is very important.	dummyTurk
689	0.0	Thus, we also carried out experiments on the Italian dataset (I-CAB).	dummyTurk
690	0.0	Please note that, we also vary the target NE types (LOC & GPE).	dummyTurk
691	0.0	The reviewer is right.	dummyTurk
692	0.0	there is no particular motivation to the architecture choice.	dummyTurk
693	0.0	We simply selected a state-of-the-art model, because we want to test if our approach can be applied to the state-of-the-art model (i.e., CRF+BLSTM) for NER task.	dummyTurk
694	0.0	We agree on the fact that error analysis is important, we could not elaborate on it as we had limited space to explain the entire new architecture.	dummyTurk
695	0.0	However, if the paper is accepted, we will have an extra page to add error analysis.	dummyTurk
696	0.0	The standard conference paper space constrained us to try our approach on only one task.	dummyTurk
697	0.0	It is our intention in the future to apply the proposed transfer learning method on other related IE and NLP tasks.	dummyTurk
698	0.0	Our research was inspired by a real industrial need, and we know well that named entities have been one of the first type of information companies have targeted.	dummyTurk
699	0.0	However, this is not a limitation of our approach.	dummyTurk
700	0.0	We can surely target other interesting sequence labeling tasks, e.g., concept segmentation and labeling in semantic parsing, semantic role labeling, and we some deeper study relation extraction.	dummyTurk
701	0.0	Again, the major motivation  to the chocie of our architecture was to start from a state-of-the-art model.	dummyTurk
702	0.0	We have created a hand-crafted nonverbal model in our previous study using the same dataset (we will refer this paper in the final version).	dummyTurk
703	0.0	The performance of the hand-crafted nonverbal model was 0.724 in F-measure.	dummyTurk
704	0.0	As we did not use linguistic features in this model, we cannot directly compare this model with V-NV fusion model, but we can compare this with our NV fusion model.	dummyTurk
705	0.0	As the F-measure of NV fusion model was 0.809, which is much better than the hand-crafted nonverbal model, we can conclude that the DNN approach outperformed hand-crafted feature engineering.	dummyTurk
706	0.0	According to the reviewer's comment, we created a length baseline model that judges an utterance as important if the utterance contains more than three words (this threshold performed best).	dummyTurk
707	0.0	The performance of this length baseline model was 0.7 in F-measure.	dummyTurk
708	0.0	As this model just looked at the number of words, the model performance was worse than the hand-crafted nonverbal model (0.724) that was described above in response to Weakness argument 1.	dummyTurk
709	0.0	More importantly, our V-NV fusion model (0.827) was much better than this baseline model.	dummyTurk
710	0.0	"As responded above in ""argument1,"" we created a hand-crafted model on this dataset and task only using nonverbal information."	dummyTurk
711	0.0	The model performance was 0.724 in F-measure, which was much lower than the NV fusion model (0.809 in F-measure).	dummyTurk
712	0.0	Thus, we will use this hand-crafted model as the nonverbal baseline model in the final version.	dummyTurk
713	0.0	"As responded above in ""argument2,"" we created the sentence length baseline model."	dummyTurk
714	0.0	The model performance was 0.7 in F-measure, which is much lower than V model (0.785) and V-NV fusion model (0.827).	dummyTurk
715	0.0	We will use this sentence length model as the baseline of verbal model.	dummyTurk
716	0.0	As Section 5.1 already showed that V-NV model performed best, in Section 5.2 we focused on examining whether the best performance model is useful as a summarization method.	dummyTurk
717	0.0	That's why we did not think that reporting ROUGE results for other models was necessary.	dummyTurk
718	0.0	The second reason is just because of the space limitation.	dummyTurk
719	0.0	We confirmed that the V-NV fusion model outperformed other models in ROUGE, and we can report the results if it is necessary.	dummyTurk
720	0.0	 In the literature, the Paragraph Vector has shown very impressive performance on tasks such as sentence classification, thus it was used instead of a summation or averaging of word vectors.	dummyTurk
721	0.0	The algorithm has a benefit of obtaining the general unique context of each sentence (using the ???Paragraph Id???), thus being a more state-of-the-art approach.	dummyTurk
722	0.0	 It would be difficult to directly compare results between sentence and word-level identification, since the nature of the task is very different, with the Paragraph Vector being intended for sentence-level analysis.	dummyTurk
723	0.0	This paper argues for the need of taking into account wider contextual elements that would signal the presence/absence of a metaphor.	dummyTurk
724	0.0	The VUAMC is considered more challenging for classification due to the amount of conventional metaphors, with other available datasets generally being much simpler (eg.	dummyTurk
725	0.0	adjective-noun pairs, annotation of only verbs).	dummyTurk
726	0.0	 This paper compares the use of NNs with other models, showing that they improve results from more basic methods, but do not need to be overly complex (eg.	dummyTurk
727	0.0	LSTM models).	dummyTurk
728	0.0	Although SVM was also very effective due to high recall, NN models were still an improvement on more basic methods such as logistic regression and outperforming other methods like SVM with the best input features (PV-DBOW).	dummyTurk
729	0.0	 Significance tests would definitely be an advantage to compare different models.	dummyTurk
730	0.0	Relative to other similar studies it would arguably not be a distinct weakness of the work, since they have often not been employed in the literature on metaphor detection.	dummyTurk
731	0.0	 The results obtained from the test set were overall unmodified with respect to hyper-parameter tuning, with the exception of dropout values.	dummyTurk
732	0.0	While other configurations of these hyper-parameters were tested, the original arrangement was found to be very effective, which can in part be explained by the inherent benefit of these hyper-parameters over others (eg.	dummyTurk
733	0.0	Adam, ReLU algorithms).	dummyTurk
734	0.0	 Paragraph vectors were frozen after making the vector representations of the training set, with the classifier separately trained on these vectors.	dummyTurk
735	0.0	Thank you so much for your kind and helpful comments.	dummyTurk
736	0.0	We appreciate it a lot.	dummyTurk
737	0.0	1) NMT models have achieved some state-of-art results in the historical German normalization task.	dummyTurk
738	0.0	One of the motivations is that we want to explore different NMT models on different languages to shed light on the research of historical spelling normalization.	dummyTurk
739	0.0	The other motivation is to summarize some possible solutions that could be beneficial to the development of NMT models in the real MT task.	dummyTurk
740	0.0	2) We thought that the simple models (vanilla-RNN without attention) would be enough for such a simple task.	dummyTurk
741	0.0	However, our result shows that the simple models are clearly inferior to the models with heavy architectures.	dummyTurk
742	0.0	Thank you so much for your kind and helpful comments.	dummyTurk
743	0.0	We appreciate it a lot.	dummyTurk
744	0.0	1) The original settings are designed for the spelling normalization task.	dummyTurk
745	0.0	We do not focus on the parameter tuning, thus we use the default settings to ensure that the models perform well.	dummyTurk
746	0.0	2) We mentioned that the comparison between Transformer model and other models is not convincing.	dummyTurk
747	0.0	It is not easy to keep the parameters consistent for two different architectures.	dummyTurk
748	0.0	We found a toolkit Marian which implements both soft-attention-based models and Transformer.	dummyTurk
749	0.0	We will use Marian to train new models in order to do the meaningful comparison.	dummyTurk
750	0.0	1) We had a discussion on the filter step.	dummyTurk
751	0.0	The accuracy improvements of the filter step in the five languages are 1.18%, 0.28%, 0.74%, 4.93%, and 0.36%.	dummyTurk
752	0.0	We also have a table including the word accuracy on both the Changed and Unchanged spellings.	dummyTurk
753	0.0	Because of the page limit, we removed it before the submission.	dummyTurk
754	0.0	2) From the accuracy improvements of the filter step, we can see that the improvement is language dependent.	dummyTurk
755	0.0	Based on the unchanged rate and word accuracy on the unchanged spellings, I would say that the drop would not be big even without the filter step.	dummyTurk
756	0.0	 The models trained on a specific time period are usually used to normalize the spellings on this time period.	dummyTurk
757	0.0	The performance on the other time periods will drop.	dummyTurk
758	0.0	 Yes, it will be very interesting to check the performance in more detail.	dummyTurk
759	0.0	We will have a look at the normalizations of both the root and the inflection parts.	dummyTurk
760	0.0	 The vocabulary sizes of characters vary from 152 to 207 (given in Table 3).	dummyTurk
761	0.0	In addition, the BPE vocabulary does not include any characters.	dummyTurk
762	0.0	That is to say, the vocabulary of BPE-models is the sum of character vocabulary and BPE vocabulary.	dummyTurk
763	0.0	 The document decoder can capture the sequence information of document, which is different from the graph-based model.	dummyTurk
764	0.0	A main distinction is that two similar sentences may be distant in the sentence sequence but close in document graph.	dummyTurk
765	0.0	As for the	dummyTurk
766	0.0	 Our model is an graph-based abstactive summarization model, so we just compare our model with other graph-based summarization model.	dummyTurk
767	0.0	 For our model is an abstractive summarization model, so we use ROUGE F1 metrics rather than ROUGE Recall metrics in our experiments.	dummyTurk
768	0.0	So the lengths of summaries are not important when evaluating the performance of generated summary.	dummyTurk
769	0.0	It is known that slot filling task can be treated as a sequence labeling task.	dummyTurk
770	0.0	The authors should refer bit old approaches used for sequence labeling task such SVM, CRF, and so on.	dummyTurk
771	0.0	There is a good work done by Zhang.	dummyTurk
772	0.0	(A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding, Xiaodong Zhang and Houfeng Wang, IJCAI-16, 2016.)	dummyTurk
773	0.0	Thank you for your suggestions.	dummyTurk
774	0.0	We plan to include them in our discussion in the updated version of our paper.	dummyTurk
775	0.0	We also plan to discuss our work has an advantage when compared to - ???A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding???  by Xiaodong Zhang and Houfeng Wang, IJCAI-16, 2016 - as the attention-only mechanism is step towards moving away from complicated models while achieving very similar slot-labeling accuracy.	dummyTurk
776	0.0	Thanks for your advice .	dummyTurk
777	0.0	We will correct the grammatical errors in the revised version.	dummyTurk
778	0.0	We will append experiment setting, hyper-parameter choice and other details.	dummyTurk
779	0.0	All the implementations are based on PyTorch http://pytorch.org/.	dummyTurk
780	0.0	We will open source the implementation.	dummyTurk
781	0.0	To Review#2	dummyTurk
782	0.0	Thanks for your insightful consideration and comments.	dummyTurk
783	0.0	We use self-collected NE dictionaries for both monolingual and cross-lingual NE detection task.	dummyTurk
784	0.0	Result from the deficiency of NE dictionaries in other languages, we only complete our experiment in and between English and Chinese.	dummyTurk
785	0.0	As claimed in line 294, we state that there are undeniable differences between different language and NE types due to the fact that the quality of NE dictionaries has a major impact on the performances as well as the quality of pre-trained word embeddings.	dummyTurk
786	0.0	Furthermore, as we do not have enough human resource to verify, we randomly sampled embeddings inside the hypersphere but outside NE dictionary, finding quite a lot of embeddings were actually true NEs just due to the insufficiency of NE dictionary.	dummyTurk
787	0.0	Referring to line 171, we claim that we choose hypersphere due to it has the least definition parameters and it presents a closure surface that includes gathering NEs.	dummyTurk
788	0.0	We will consider more mature techniques later accordingly.	dummyTurk
789	0.0	When we use the term Visualization, we mean that the distribution property is actually obtained from human observation by various angles of figure drawing, which is not an image processing issue actually.	dummyTurk
790	0.0	To Review#1	dummyTurk
791	0.0	Thanks for your insightful consideration and comments.	dummyTurk
792	0.0	We manually sampled and checked embedding inside the supposed NE hypersphere but outside our NE dictionaries.	dummyTurk
793	0.0	As we give the corresponding NE visualization which has been shown far from the proper hypersphere shape, we found nearly all checked embeddings are truly NE but not in our dictionaries which means that our NE dictionaries over these languages are extremely insufficient.	dummyTurk
794	0.0	Considering the concerned NE dictionaries are not sufficient enough to support a meaningful evaluation, we dropped out the respective experiments based on them.	dummyTurk
795	0.0	Yes, what you understand is exact, the evaluation is just be about hypersphere exactly includes all the NEs in the dictionary.	dummyTurk
796	0.0	The performances are evaluated jointly by two factors, calculating the ratio of NE from the dictionary which is included in the hypersphere (recall), and counting those NE inside the hypersphere but outside the dictionary (this is for precision).	dummyTurk
797	0.0	F-score is then computed from the harmonic average of recall and precision.	dummyTurk
798	0.0	All the evaluated data are NE dictionaries, which is supposed to be sufficient and accurate, though not really so.	dummyTurk
799	0.0	The purpose of the evaluation in monolingual case is just to show how well a hypershpere model can accurately depict all NE distribution given by a good enough dictionary.	dummyTurk
800	0.0	Once we have more space later, we will add back the required analysis and discussion.	dummyTurk
801	0.0	For the reason why cross-lingual NE mapping is better than monolingual, it is due to that the high quality of Chinese location NE dictionary and transform matrix can additionally help the bilingual processing which right demonstrates the effectiveness of our approach and model.	dummyTurk
802	0.0	For the monolingual case, we have too poor English location NE dictionary in the meantime.	dummyTurk
803	0.0	We use a simple strategy to represent multi-word NEs with average vector of all member word vectors inside the corresponding NE.	dummyTurk
804	0.0	The evaluation method is described in response to W1 for review#1.	dummyTurk
805	0.0	This is probably because of relatively more poor quality of German NE dictionary and there is more serious mismatch between the NE dictionary and the corresponding pre-trained embedding.	dummyTurk
806	0.0	Even though German NE dictionary is much larger, the above two factors also play an important role in NE hypersphere modeling.	dummyTurk
807	0.0	 The worst-case results for the proposed method are that it cannot predict a boundary within the maximum length of sentences.	dummyTurk
808	0.0	In that case, following the paper of Wang et al 2016, the method will predict a boundary after the position that has the highest confidence score.	dummyTurk
809	0.0	 The results on the table 6 are obtained from the 'online' version of system.	dummyTurk
810	0.0	The words were input into the system one by one.	dummyTurk
811	0.0	 This work regards domains as genres and explores 10 different genres in Simple Wikipedia.	dummyTurk
812	0.0	If one regards domains as different types of text (e.g., informal texts and dialogue), the situation will be different and we leave it as future work.	dummyTurk
813	0.0	We will clarify this distinction in Section 1.	dummyTurk
814	0.0	 Our definition of events differs from (O'Gorman et al., 2016) in that ours is explicitly based on two theoretical bases of eventualities and event nuggets, and it explicitly covers how to deal with adverbs and discontinuous phrases.	dummyTurk
815	0.0	These differences make our definition more suitable for the goal of open-domain event detection with wider and more consistent coverage.	dummyTurk
816	0.0	 We annotate text spans that match with our definition from the both perspectives, not just either one.	dummyTurk
817	0.0	It is a great idea to report the performance both on sentence-level nuclearity and above sentence-level nuclearity and the experimental results prove your conjecture.	dummyTurk
818	0.0	We  find out that the performance of the sentence-level nuclearity recognition is better than that of the above sentence-level as follows.	dummyTurk
819	0.0	sentence-level(F1): NA 56.3, NB 46.0, MN 84.1, Macro-F1 63.1, Micro-F1 74.7;	dummyTurk
820	0.0	above sentence-level(F1): NA 54.2, NB 42.2, MN 75.8, Macro-F1 58.5, Micro-F1 65.0.	dummyTurk
821	0.0	We will add a table to the revised paper and give the analysis.	dummyTurk
822	0.0	Yes.	dummyTurk
823	0.0	This method also can be used in English.	dummyTurk
824	0.0	To compare our method with the state-of-the-art neural networks methods used in English nuclearity recognization, we re-implement the neural network model (Li et al.	dummyTurk
825	0.0	2016), i.e., Bi-LSTM(A)+T, as a baseline.	dummyTurk
826	0.0	Besides, We are preparing the experiments on the RST-DT corpus and will report the results in our revised paper if this paper is accepted.	dummyTurk
827	0.0	Convolutional-pooling LSTM has been applied to many NLP tasks.	dummyTurk
828	0.0	In our paper, we first applied it to the nuclearity recognition task in Chinese discourses.	dummyTurk
829	0.0	We will modify this claim in our revised paper.	dummyTurk
830	0.0	Nuclearity recognition task should be performed after the construction of discourse tree.	dummyTurk
831	0.0	In our future work, we will focus on the other tasks of the discourse parsing in CDTB.	dummyTurk
832	0.0	The ME model, one of the baseline, is the system provided by Li et al.	dummyTurk
833	0.0	(2015), maybe we don not have a clear explanation about this in the Section Experiments.	dummyTurk
834	0.0	And we also run the system provided by Chu et al.	dummyTurk
835	0.0	(2015) and find out that its performance is worse than Li et al.	dummyTurk
836	0.0	(2015), so we didn't choose their model as a baseline.	dummyTurk
837	0.0	We are preparing the experiments on the RST-DT corpus and will report the results in our revised paper if this paper is accepted.	dummyTurk
838	0.0	Besides, to compare with the methods on RST DT, we re-implement a state-of-the-art neural network model (Li et al.	dummyTurk
839	0.0	2016), i.e., Bi-LSTM(A)+T, as a baseline.	dummyTurk
840	0.0	The Me model of Li et al.	dummyTurk
841	0.0	(2015) is the state-of-the-art in CDTB.	dummyTurk
842	0.0	This statement is related to the characteristics of Chinese language.	dummyTurk
843	0.0	You are right, the syntactic information is useful in many English NLP tasks.	dummyTurk
844	0.0	However, it is not so effective in Chinese due to the low performance of syntactic parser and the flexible structure of Chinese sentences.	dummyTurk
845	0.0	So we think more semantic information is needed in Chinese discourse parsing tasks.	dummyTurk
846	0.0	The choice of these hyperparameters is based more on experience or on the values used in other papers.	dummyTurk
847	0.0	We simply the hyperparameter tuning and list several different sets of the hyperparameters.	dummyTurk
848	0.0	We choose the one with the best performance.	dummyTurk
849	0.0	All inputs of baselines do not include paragraph information, and it is an innovative point of our work to consider the semantic links between discourse units and paragraph in nuclearity recognition task.	dummyTurk
850	0.0	Due to the space limitation, we didn???t give these features in ME.	dummyTurk
851	0.0	The ME model contains the context features, lexicon features and dependency tree features, which is similar with Lin et al.	dummyTurk
852	0.0	(2009) (Recognizing implict discourse relations in the penn discourse treebank, in EMNLP 2009).	dummyTurk
853	0.0	You are right, NNs are not better than systems based on hand-crafted features.	dummyTurk
854	0.0	We will modify the corresponding description in our paper.	dummyTurk
855	0.0	We will simplify example 1 and give an example with 4 or 5 EDUs.	dummyTurk
856	0.0	The discourse relation between DUs a and b is Nucleus Ahead, because DU b gives the detail information to further explain DU a.	dummyTurk
857	0.0	We have conducted a systematic investigation for this task.	dummyTurk
858	0.0	In particular, we have studied textual diagnosis report examples written by human doctors.	dummyTurk
859	0.0	It seems that there are no general, unified, and standard metadata definitions for diagnosis report.	dummyTurk
860	0.0	An example of a diagnosis provided by a human doctor can look like this:	dummyTurk
861	0.0	???	dummyTurk
862	0.0	???Prominent mediastinum.	dummyTurk
863	0.0	???May be due to mediastinal fat.	dummyTurk
864	0.0	???Comparison films, if available, would be useful to determine if this is a chronic appearance.	dummyTurk
865	0.0	???Clear lungs.	dummyTurk
866	0.0	???	dummyTurk
867	0.0	The above example shows that a diagnosis report is mainly unstructured free texts containing phrases and sentences.	dummyTurk
868	0.0	Thus, this task cannot be simplified via metadata extraction.	dummyTurk
869	0.0	Jing's approach and ours share the same dataset.	dummyTurk
870	0.0	In Jing's method, they generate both findings and diagnosis by taking X-ray image as input.	dummyTurk
871	0.0	In comparison to their work, we generate only diagnosis by using 1) findings, 2)indication, and also 3) X-ray image.	dummyTurk
872	0.0	A typical diagnosis includes both conclusions about the radiographic study and recommendations for further evaluation and patient management.	dummyTurk
873	0.0	Recommendations are based on the results of the radiologic study and the experience of the individual radiologist.	dummyTurk
874	0.0	Our method is more comparable to how the human doctors work, since the human doctors also take more information types into account for their diagnosis report.	dummyTurk
875	0.0	In Jing???s method, only partial data in the dataset was utilized for generating diagnosis.	dummyTurk
876	0.0	By leveraging more data and data types, we can obtain better result for the diagnosis generation task.	dummyTurk
877	0.0	Thanks for pointing this out.	dummyTurk
878	0.0	More solid error analysis will be available in the final version.	dummyTurk
879	0.0	Thanks for raising this insightful question.	dummyTurk
880	0.0	In the final version, we will add a comparison of our approach and some rule-based systems.	dummyTurk
881	0.0	Rule-based systems can only work well if relevant information can be encoded into rules, namely, in an explicit way.	dummyTurk
882	0.0	However, medical images are complex and the relevant information encoded in the images may not be all explicitly encoded into rules.	dummyTurk
883	0.0	At the same time, textual descriptions report facts encoded in an X-ray for a doctor, which are also mostly difficult to be explicitly represented by rules.	dummyTurk
884	0.0	Our model is an end-to-end neural network with better scalability.	dummyTurk
885	0.0	It avoids managing a complex sequential pipeline.	dummyTurk
886	0.0	Deep learning based method is more suitable to identify implicit correlations between images and textual data.	dummyTurk
887	0.0	Thank you very much for this suggestion.	dummyTurk
888	0.0	We will ask a native speaker for the proofreading of our final version.	dummyTurk
889	0.0	Let us explain the two evaluation metrics for our approach, BLEU and ROUGE.	dummyTurk
890	0.0	1) BLEU score is a metric to compare n-gram words of generated sentences against reference ones.	dummyTurk
891	0.0	2) ROUGE stands for Recall Oriented Understudy of Gisting Evaluation.	dummyTurk
892	0.0	It computes n-gram based recall for the candidate sentence with respect to the reference one.	dummyTurk
893	0.0	B and R in Table 1 are initials, and B# represents #-gram BLEU score, while R for ROUGE.	dummyTurk
894	0.0	We will explain them in our final version more explicitly.	dummyTurk
895	0.0	Thank you for reminding us not to overclaim the approach.	dummyTurk
896	0.0	However, we believe that the application scenario will have a big potential to support a radiologist in their everyday work in the near future.	dummyTurk
897	0.0	 As most of the previous studies did not report this result, we followed them and did not report either in our first version.	dummyTurk
898	0.0	But this is a nice suggestion, thanks.	dummyTurk
899	0.0	 For repetition, we presented the percentage of duplicates at sentence level in Figure 1, including 1-gram to 4-gram duplicates, and the results show that our model has clear advantage on reducing repetition compared with the baseline.	dummyTurk
900	0.0	Human evaluation may be a more reasonable method, and we will add it in the final version.	dummyTurk
901	0.0	 The average length of the summaries of Giga is 8.8 words and that of LCSTS is 18.8 Chinese characters.	dummyTurk
902	0.0	 We evaluate this baseline by selecting the first 10 words for Giga and 20 characters for LCSTS in our experiments (the average lengths of the summaries of the corpora are 8.8 and 18.8), and the results ROUGE-1, 2 and L are	dummyTurk
903	0.0	Many thanks for your helpful reviews.	dummyTurk
904	0.0	1.Reply to weakness argument-???I am not total ??????:	dummyTurk
905	0.0	We conducted manual evaluation of response self-consistency on 30 message pairs built on test set.	dummyTurk
906	0.0	Each pair consists of two messages sharing similar meaning, e.g.	dummyTurk
907	0.0	???I am waiting to get off work.???/???It's really boring to work.???.	dummyTurk
908	0.0	Then we run one system on the two messages in each pair and then checked if the two responses express similar personal preference or attitude.	dummyTurk
909	0.0	We see that OGM can show self-consistency with 50% message pairs, S2S_{1m, fo} with a ratio of 33%,  S2S_{4m, fo} with a ratio of 40%, and S2S_{4m, mr} with a ratio of 30%.	dummyTurk
910	0.0	We will update the draft with these results.	dummyTurk
911	0.0	This system is applicable to generation of opinions with various targets, e.g.	dummyTurk
912	0.0	, products, concepts, not limited to individuals.	dummyTurk
913	0.0	Moreover, this system can also generate responses to non-fact utterances since our model does not require the input to be factual utterances.	dummyTurk
914	0.0	"""e"" refers to an opinion target."	dummyTurk
915	0.0	The vector representations v(r) and v(m) are trained using ScoreNet.	dummyTurk
916	0.0	W1: ???relevance of metrics like syntactic complexity and use of non-frequent words depends on use case???	dummyTurk
917	0.0	These measures were introduced to show limitations of current language-model-based systems, answering the question ???if you want more complex and diverse outputs, which model should you use????	dummyTurk
918	0.0	These metrics reflect use cases where system output should not be repetitive and might need to adapt to different personality styles (e.g.	dummyTurk
919	0.0	PERSONAGE system).	dummyTurk
920	0.0	On the other hand, it is indeed questionable whether more complex and diverse outputs are desirable for other use cases, which e.g.	dummyTurk
921	0.0	depend on cognitive load.	dummyTurk
922	0.0	However, this is not a question we can address within the limitations of this paper.	dummyTurk
923	0.0	We will add this point to our discussion.	dummyTurk
924	0.0	1) We would like to mention that the scope of this work was to introduce the idea of non-adversarial evaluation and show that existing models do not perform well on such examples.	dummyTurk
925	0.0	In future, we would like to develop models which are robust to such evaluation.	dummyTurk
926	0.0	2) It is desirable to do so.	dummyTurk
927	0.0	The RACE dataset was chosen since it is MCQ style, allowing a wider range of non-adversarial examples involving passage, question and option modifications and performance of existing models is far below human performance, giving scope for improvement on non-adversarial datasets.	dummyTurk
928	0.0	There are nuances related to doing the same on, say, SQuAD dataset and the performance of models is near human (~83%), leaving little room for improvement on non-adversarial examples.	dummyTurk
929	0.0	Yes, we will release the dataset and the scripts used for generating it.	dummyTurk
930	0.0	We weren???t aware of FusionNet since it's a very recent paper published after we chose our models.	dummyTurk
931	0.0	We will carefully examine and try our best to include it in our analysis.	dummyTurk
932	0.0	We are working on further investigation and extensions of the proposed method for a follow-up paper.	dummyTurk
933	0.0	We observed the impact of different types of rules generally depends on the domain; s-rules are more important for domains with more system-driven dialogs while u-rules domains with more user-driven dialogs.	dummyTurk
934	0.0	The performance is roughly proportional to the number of rules but the gains vary according to the relationship between rules.	dummyTurk
935	0.0	We are working on some extensions where more meta info can be transferred and other knowledge representation can be used, e.g.	dummyTurk
936	0.0	flow chart.	dummyTurk
937	0.0	It would be great if we could get some suggestions from reviewers.	dummyTurk
938	0.0	A longer version will include such analysis.	dummyTurk
939	0.0	We observed there are two big error sources: sometimes sentence similarity is not good enough to perform correct inferences.	dummyTurk
940	0.0	Sometime none of the rules are relevant in the context.	dummyTurk
941	0.0	For real systems, domain experts are already likely to have such rules in mind.	dummyTurk
942	0.0	But we took a look at 5-10 training dialogs for each domain to figure out what the domain is about because we used data that someone else made publicly available.	dummyTurk
943	0.0	We are working on further investigation and extensions of the proposed method for a follow-up paper.	dummyTurk
944	0.0	We observed the impact of different types of rules generally depends on the domain; s-rules are more important for domains with more system-driven dialogs while u-rules domains with more user-driven dialogs.	dummyTurk
945	0.0	The performance is roughly proportional to the number of rules but the gains vary according to the relationship between rules.	dummyTurk
946	0.0	We expect that more rules than what is necessary would cause more noise.	dummyTurk
947	0.0	For clarification, we didn't mention that complex rules are needed but we think they are interesting opportunities.	dummyTurk
948	0.0	The current work looks at only previous user utterance and system utterance but it is common that multiple features from dialog history are used to control the dialog in rule-based dialog systems, e.g., flow-charts.	dummyTurk
949	0.0	It would be interesting if we could represent such rules in natural language.	dummyTurk
950	0.0	Thanks for the insightful comments.	dummyTurk
951	0.0	 The qualitative analysis should directly measure how humans produce errors.]	dummyTurk
952	0.0	Visually and phonologically similar characters are two types of characters in spelling errors according to the analysis (Liu et al.	dummyTurk
953	0.0	2010), based on which we generate the annotated corpus.	dummyTurk
954	0.0	As for what percentage of our produced spelling errors that humans would actually produce in their writing, it is difficult to evaluate and what we can do is to generate as many spelling errors as possible in our generated corpus in order to improve the detection performance.	dummyTurk
955	0.0	Chao-Lin Liu, Min-Hua Lai, Yi-Hsuan Chuang, Chia-Ying Lee: Visually and Phonologically Similar Characters in Incorrect Simplified Chinese Words.	dummyTurk
956	0.0	COLING (Posters) 2010: 739-747	dummyTurk
957	0.0	 How large the dataset should be for outperforming the manually annotated corpora?]	dummyTurk
958	0.0	We conduct an additional experiment for studying the performance of models trained on different scale of A-corpus.	dummyTurk
959	0.0	The results show that an A-corpus about four times larger than the manually annotated corpora is needed to achieve the expected performance.	dummyTurk
960	0.0	Whether the tests address error detection (= spell checking), correction, or both?]	dummyTurk
961	0.0	Error detection serves as the key focus of this paper.	dummyTurk
962	0.0	F1, precision, and recall reported in the experiment results are all concerned with the detection-specific performance.	dummyTurk
963	0.0	We will clarify this in the revised paper.	dummyTurk
964	0.0	  The current state-of-the-art results on these four datasets?]	dummyTurk
965	0.0	In the revised paper, we will include the four state-of-the-art results reported in the original reports.	dummyTurk
966	0.0	The experimental results show that the model trained on our generated corpus outperform the state-of-the-art results.	dummyTurk
967	0.0	" Why is the approach called ""a compound approach""?]"	dummyTurk
968	0.0	In generating the annotated corpus, we use two different methods.	dummyTurk
969	0.0	So the overall approach is a compound or hybrid approach.	dummyTurk
970	0.0	[Additional Comments]	dummyTurk
971	0.0	Thanks for your detailed advice in further polishing our paper.	dummyTurk
972	0.0	We will revise our paper accordingly.	dummyTurk
973	0.0	In deep learning based works, datasets are always divided by the same manner to ensure credibility.	dummyTurk
974	0.0	According to existing papers, such as (Lample et al.,2016) and (Ma et al.,2016), none of them conducted repeated experiments.	dummyTurk
975	0.0	Therefore, we followed the setting of previous works for fair comparison.	dummyTurk
976	0.0	We used the early stopping strategy for model selection.	dummyTurk
977	0.0	At the end of each epoch, accuracy on validation dataset is calculated.	dummyTurk
978	0.0	And the training process ends when the accuracy stops increasing.	dummyTurk
979	0.0	"Here, we also adopted the ""No-improvement-in-10"" strategy to improve the credibility of model selection."	dummyTurk
980	0.0	On all three datasets, hyper-parameters were tuned with the same grid-search, and we chose models that achieved the best performances.	dummyTurk
981	0.0	And we have controlled possible variations to ensure the reliability and reproducibility of experiments.	dummyTurk
982	0.0	"We justify our use of UrbanDictionary by noting that is the largest compilation of online slang spanning more than 15 years which we believe reflects slang usage ""in the wild, as opposed to studying pre-compiled dictionaries in print which may not be up to date and representative of slang as used in recent years."	dummyTurk
983	0.0	We acknowledge in the paper that the analysis of offline slang (in speech etc) is a separate research question (see Footnote 2).	dummyTurk
984	0.0	Our goal in this work is to characterize online slang in English.	dummyTurk
985	0.0	We are quite aware that our analysis likely reflects the usage of usage of slang of likely young people (again we explicitly note this in Lines 782-786) but we believe our study sets the stage for future work and analysis of slang across cultures and demographics.	dummyTurk
986	0.0	Our argument is that slang manifests at-least as much gender stereotyping as found in News.	dummyTurk
987	0.0	Note that we even analyze this and show prevailing stereotypes in Figure 5.	dummyTurk
988	0.0	Analysis in Section 3.2.2 was done using the data released by Mattiello.	dummyTurk
989	0.0	"Identifying a ""Front"" or ""Back"" clipping is just a matter of checking whether a prefix or a suffix of the original word is retained (as per definition)."	dummyTurk
990	0.0	We would like to clarify that we do contextualize our results with the work of Nguen.2017 et.al and quite some other prior work.	dummyTurk
991	0.0	In particular, see Lines 725-740	dummyTurk
992	0.0	It is precisely for that very reason, we do two things (a) We establish the generic quality of these slang embeddings by evaluating them on standard word embedding benchmarks (see Lines 571-573) and show that they are comparable to other standards like GloVE at-least in a holistic sense.	dummyTurk
993	0.0	(b) Second, we ensure we test for differences via statistical tests to weed out false positives (see Figure 7 with 95% confidence intervals) and Lines 687-690 for justification of our claims.	dummyTurk
994	0.0	 (Synthetic experiments) This is a good point, and we are well aware of this being a rather crude approximation.	dummyTurk
995	0.0	However, available annotated Twitter data is sparse, and would introduce many confounds.	dummyTurk
996	0.0	Synthetic data enabled us to isolate the effect of punctuation.	dummyTurk
997	0.0	 (Segmentation and POS tagging) We use gold sentence segmentation, but our POS taggers are trained the same way as the corresponding parser.	dummyTurk
998	0.0	In fact, we think of parser-tagger combinations as our parsers.	dummyTurk
999	0.0	Sorry if we were not clear on this.	dummyTurk
1000	0.0	 (Explanation) We DO explain the relative drops of neural parsers by feature swamping.	dummyTurk
1001	0.0	This is more likely to happen with neural parsers that can condition on longer stretches of context, and that include more redundancies by over-specifying parser configurations.	dummyTurk
1002	0.0	We sample 5 SQL queries for each table in WikiSQL training data, resulting in 43.5K SQL queries.	dummyTurk
1003	0.0	Applying our QG model on these SQL queries, we get 92.8K SQL-question pairs.	dummyTurk
1004	0.0	As a reference, the original WikiSQL training data consists of 61.3K SQL-question pairs.	dummyTurk
1005	0.0	In our preliminary experiment, we set the number of generated question for each table as 3, 5, to 7.	dummyTurk
1006	0.0	In the setting with 30% supervised data, the execution accuracy of our semantic parser is 73.2%, 74.0% and 72.9% respectively.	dummyTurk
1007	0.0	Based on this result we use 5 in the following experiment.	dummyTurk
1008	0.0	We believe that a small number of generated questions lacks diversity while a large number of generated questions would induce noise.	dummyTurk
1009	0.0	We will give more discussion about these details.	dummyTurk
1010	0.0	Thanks.	dummyTurk
1011	0.0	We will polish the paper via discussing on missing related work wang-berant-liang-acl2015, and properly mention Iyer-2017 in the question generation part of related work.	dummyTurk
1012	0.0	Thanks for the suggestion on the structure of this paper.	dummyTurk
1013	0.0	We totally agree with you and would emphasize the strong empirical results than the logarithmic relationship.	dummyTurk
1014	0.0	We would surely polish the article to give more clarifications on section 5.1, modifying confusing symbols in equations, and fix grammar and word choice issues.	dummyTurk
1015	0.0	The main take-home message of the quantitative evaluation is that our method indeed captures meaningful similarity relations, and that it performs comparably to regular first-order word embedding methods.	dummyTurk
1016	0.0	Skip-gram performs best (though in two out of five cases the performance is roughly on par with SOCO), but for GloVe, CBOW and FOCO, our method performs best in three out of five benchmarks.	dummyTurk
1017	0.0	Considering the simplicity of our method, this is in our view a significant result.	dummyTurk
1018	0.0	Compared to the other methods we also avoid pairwise similarity calculations, which may be crucial in online settings with strict latency requirements.	dummyTurk
1019	0.0	Examples of potential applications are online word clustering/concept induction, document classification and term expansion.	dummyTurk
1020	0.0	The mention that the algorithm can run on a laptop was primarily to underline that the method has a small memory footprint, not that it will typically be run on a laptop in real-world applications.	dummyTurk
1021	0.0	We will make these points more clear in the paper.	dummyTurk
1022	0.0	The intention behind this was not to tune the method towards the benchmarks.	dummyTurk
1023	0.0	We agree, however, that the influence of the parameters is highly relevant and will complement the experiments section.	dummyTurk
1024	0.0	We will describe the method more extensively with further examples and figures.	dummyTurk
1025	0.0	We will also give a thorough description of the count-min approach for estimating second-order co-occurrence counts in order to make the paper self-contained.	dummyTurk
1026	0.0	We acknowledge that LSA/I is related to our work and will refer to it in the paper.	dummyTurk
1027	0.0	Note though that LSA/I relates to our approach in the same way as word2vec, GloVe and other word embedding methods in that LSA/I is based on first-order contexts (with respect to documents through dimensionality reduction).	dummyTurk
1028	0.0	That is, second-order co-occurrences are not explicitly used as in our approach.	dummyTurk
1029	0.0	Instead, vector representations are first derived and then used in (expensive) pairwise similarity calculations.	dummyTurk
1030	0.0	Moreover, LSA/I is batch-based and not applicable in a streaming setting.	dummyTurk
1031	0.0	"In our view ""relatedness"" is too a general term that may encompass relations such as for ""car - wheel"" and ""ocean - ship""."	dummyTurk
1032	0.0	"Although ""similarity"" is not an ideal term either (including antonym relations, for example), we believe that it more closely describes the types of relations captured by our method."	dummyTurk
1033	0.0	"We therefore propose to keep ""similarity"" while clarifying what we include in this relation (e.g."	dummyTurk
1034	0.0	"that ""hot"" and ""cold"" are similar in this sense)."	dummyTurk
1035	0.0	We will clarify the similarity-relatedness distinction in the benchmarks.	dummyTurk
1036	0.0	Thank you for the comments!	dummyTurk
1037	0.0	 Multi-task learning is almost always done with (at least loosely) related tasks	dummyTurk
1038	0.0	Thank you for the suggestion.	dummyTurk
1039	0.0	The dataset we use in this paper is a popular machine translation dataset.	dummyTurk
1040	0.0	There are many previous work performing the experiments on it, so that we can compare our model with these state-of-the-art models.	dummyTurk
1041	0.0	In the future work, we will explore the effect of our model on more datasets, including the high-resource languages and the low-resource languages.	dummyTurk
1042	0.0	Yes, the bag of word is exactly the set of words that appear in the reference sentence.	dummyTurk
1043	0.0	We will explain it more clearly in the revision.	dummyTurk
1044	0.0	Thanks for your approval of our work.	dummyTurk
1045	0.0	The suggestions are very helpful and instructive.	dummyTurk
1046	0.0	For the weaknesses you raised, we reply as follows.	dummyTurk
1047	0.0	Thanks for your suggestion.	dummyTurk
1048	0.0	We will add the essential details of our MISD dataset.	dummyTurk
1049	0.0	Our MISD dataset has 14 kinds of intents in total.	dummyTurk
1050	0.0	The average turn number of our MISD dataset is about 4 turns.	dummyTurk
1051	0.0	In fact, there are about 3 different intents in a single dialogue session on average.	dummyTurk
1052	0.0	Each dialogue session tend to switch intent for 2 or 3 times.	dummyTurk
1053	0.0	We think the dataset comes from real-world scenario and is pretty complex.	dummyTurk
1054	0.0	Actually, Q_t represents the hidden vector of current turn in the utterance-level RNN.	dummyTurk
1055	0.0	Since the dialogue process is a hierarchical structure, the representation of a single intent also considers information from former turns.	dummyTurk
1056	0.0	We???ll carefully check the technical part and make it more readable.	dummyTurk
1057	0.0	That's correct -- it's a mistake that we will fix in the final version.	dummyTurk
1058	0.0	Thanks.	dummyTurk
1059	0.0	At the beginning of section 3.2, we mentioned that we used Illinois NER [1], which describes the features used in detail.	dummyTurk
1060	0.0	Due to the space limit, we didn???t reiterate the features, but we can clarify in the camera-ready version.	dummyTurk
1061	0.0	[1] Design Challenges and Misconceptions in Named Entity Recognition, Ratinov and Roth, 2009	dummyTurk
1062	0.0	At the beginning of section 3.2, we mentioned that we used Illinois NER [1], which describes the features used in detail.	dummyTurk
1063	0.0	Due to the space limit, we didn???t reiterate the features, but we can clarify in the camera-ready version.	dummyTurk
1064	0.0	[1] Design Challenges and Misconceptions in Named Entity Recognition, Ratinov and Roth, 2009	dummyTurk
1065	0.0	The standard bootstrapping method we chose does not operate with weights, but only inclusion/exclusion of tokens in the training set.	dummyTurk
1066	0.0	The window heuristic fits this framework.	dummyTurk
1067	0.0	The puncnum heuristic does also, but we found that in this case, having a lot of punctuation and numbers as negative examples harmed performance.	dummyTurk
1068	0.0	Thank you very much for your review.	dummyTurk
1069	0.0	 Due to limited space, We abandoned a lot of details about the baselines Seq2Seq and IMP to make room for MAST.	dummyTurk
1070	0.0	We will clarify the details in the final version and try our best to make better writing.	dummyTurk
1071	0.0	We really appreciate the helpful writing suggestions you proposed.	dummyTurk
1072	0.0	 Here we present the BLEU results (on the same test set) of Seq2Seq with all training pairs (except for dev and test set, roughly 28k pairs) and with 10k sampled pairs.	dummyTurk
1073	0.0	There are only 5 versions as the space and time for rebuttal is limited.	dummyTurk
1074	0.0	BTW, although these results seems to be much better, it's not a general case with so many paralleled data, and there is no need to compare the models under different data settings.	dummyTurk
1075	0.0	Seq2Seq(10k): ASV-BBE(27.81) ASV-DARBY(48.38) ASV-DRA(25.08) ASV-WEB(63.76)  ASV-YLT(27.75)	dummyTurk
1076	0.0	Seq2Seq(28k): ASV-BBE(36.98) ASV-DARBY(57.30) ASV-DRA(32.13) ASV-WEB(72.22)  ASV-YLT(40.09)	dummyTurk
1077	0.0	 Indeed, we are not very familiar with the Bibles.	dummyTurk
1078	0.0	That's the reason why we asked the subjects in human evaluations to have read Bibles.	dummyTurk
1079	0.0	If you think it's necessary, we will investigate the differences and supplement these.	dummyTurk
1080	0.0	 The training set for a particular style are all the same in Seq2Seq, IMP and MAST.	dummyTurk
1081	0.0	 Because there are 15 different versions in the experiments of Table 4, but only 5 versions in Table 2, which means the neighbors are not the same according to SOS.	dummyTurk
1082	0.0	 That's a really good question.	dummyTurk
1083	0.0	Generally, in multi-agent system, the agents are independent from each other, but somehow(following some self-organization policies) work together to make the whole system effective.	dummyTurk
1084	0.0	If you regard the self-organization part as an attention mechanism, it's OK(maybe better) to make attentions of the weights of other agents.	dummyTurk
1085	0.0	But in this paper, we only want to see whether the simplest multi-agent system works fine with style transfer tasks.	dummyTurk
1086	0.0	So that we try to make every part of our system easy and common.	dummyTurk
1087	0.0	It's easy for us to fine-tune all kinds of sub-modules and hyper-parameters to get more significant results, but it does not make any sense to the exploration.	dummyTurk
1088	0.0	 Of course the ACC are originally in [0,1].	dummyTurk
1089	0.0	But, for example, if there are three styles, and their ACCs is (0.8, 0.85, 0.9), and these values are not good option for further measures.	dummyTurk
1090	0.0	We think rescaling them to (0, 0.5, 1.0) can be much better to make a difference.	dummyTurk
1091	0.0	 ACC is the classification accuracy of the binary-classification model, the higher ACC is, the more different the two styles are (easier to distinguish, line 320~324 in our paper).	dummyTurk
1092	0.0	But we want the value to measure the similarity of two styles.	dummyTurk
1093	0.0	So we use 1-ACC' to contribute to SIMI.	dummyTurk
1094	0.0	 Yes, a vanilla SGD.	dummyTurk
1095	0.0	In this paper, Our work is mainly the use of deep learning methods to do the task of joint model in Chinese word segmentation, POS tagging and dependency parsing.	dummyTurk
1096	0.0	However, so far, the joint model based on deep learning has been few.	dummyTurk
1097	0.0	Kurita et al(2017) did this task, and showed that the experimental results are lower than those based on feature engineering.	dummyTurk
1098	0.0	To better explore the joint model based on deep learning, we used the Kurita et al(2017) model as our baseline.	dummyTurk
1099	0.0	In this paper, our main work focuses on how to use deep learning to improve the performance of the joint model.	dummyTurk
1100	0.0	We propose a stack tree lstm to encode multiple dependent subtrees.	dummyTurk
1101	0.0	In the experimental part we make comparsion with Dyer et al (2015) stack lstm and Experiments show that our method is superior to stack lstm in joint model.In the future work, we will conduct a series of experiments to sufficient verify the performance of stack tree lstm.	dummyTurk
1102	0.0	In section 1??? we did not give a detailed explanation of the figure, causing your misunderstanding.	dummyTurk
1103	0.0	In this paper, we use the transition based method to do a joint model of Chinese word segmentation, POS tagging, and dependency parsing.	dummyTurk
1104	0.0	In the process of parsing, we incrementally build a dependency tree according to the order in which words appear in sentences, so I think the order of x3 and x4 is correct.	dummyTurk
1105	0.0	Thank you for your review!	dummyTurk
1106	0.0	Please allow us to give explanation and clarification on your misunderstanding.	dummyTurk
1107	0.0	We validate our idea on two annotations due to the tight experiment and writing schedule before the submission deadline.	dummyTurk
1108	0.0	Since most of the recent work related to learning on different annotations experimented on two annotations, we think that the experiments can still demonstrate the point.	dummyTurk
1109	0.0	As you suggested, it will be better to conduct experiments on more annotations.	dummyTurk
1110	0.0	The unified learning is indeed a new way to do parameter sharing, as well as previous methodologies including multitask learning and cascaded classification.	dummyTurk
1111	0.0	In this work we argue that our method is more efficient for learning on different annotations.	dummyTurk
1112	0.0	The major difference between our method and multitask learning lies in that, the unified learning shares information across all of the embedding, encoding and predicting layers, and the topmost module in unified learning is also the same for different annotations.	dummyTurk
1113	0.0	Thank you very much for your detailed comments and insightful suggestions for further comparison.	dummyTurk
1114	0.0	We have decided to focus on our curated ClinVar dataset because we can directly get high-quality annotations from genetic variant curators that we are working with.	dummyTurk
1115	0.0	For future work we definitely want to gather more annotations for ClinVar dataset and evaluate our work on standard sentiment-based datasets.	dummyTurk
1116	0.0	We agree and hope to use the extra page in the final version to highlight interesting examples from our error analysis.	dummyTurk
1117	0.0	We agree that this is a valid point; still we believe we make several important contributions beyond this earlier work.	dummyTurk
1118	0.0	Please see our response to the first argument made by the first reviewer, on the same issue.	dummyTurk
1119	0.0	The following will be added to the final version.	dummyTurk
1120	0.0	"Error analysis examples for the topic ""Private education brings more good than harm"":"	dummyTurk
1121	0.0	Different Topic (contains a claim but not exactly to the topic): Changes in private school enrollment is not a likely contributor to any changes in schools segregation patterns during that time.	dummyTurk
1122	0.0	"Different Topic (contains a claim but not exactly to the topic): In 2014 Hunt proposed that private schools should be required to form ""partnerships"" with local state schools if they wanted to keep their charitable status."	dummyTurk
1123	0.0	Factual (related to the topic but factual): The IRS announced in 1970 that private schools with racially discriminatory admissions policies would no longer receive tax exemptions.	dummyTurk
1124	0.0	The related work is 1.5 pages, the model section 1 page.	dummyTurk
1125	0.0	We feel that the length is adequate for the content in these sections.	dummyTurk
1126	0.0	You are the first reviewer who wants to have less related work, instead of having references added.	dummyTurk
1127	0.0	:-) Yes, this is an extension of a previous model (as we make clear in the abstract and throughout the article).	dummyTurk
1128	0.0	Yes, an RNN is more suitable to model sequences.	dummyTurk
1129	0.0	In our approach, we are explaining away the sequence information from the semantic representation.	dummyTurk
1130	0.0	We therefore called our approach sequence-aware, as opposed to sequence-modeling.	dummyTurk
1131	0.0	We expect TopicRNN to beat document models in sentiment classification, as explained in the last paragraph of Section 4.6.	dummyTurk
1132	0.0	We gladly take any hints for how to better structure the references.	dummyTurk
1133	0.0	Please bear in mind that we're using the official template of ACL 2018 and are therefore limited.	dummyTurk
1134	0.0	We will add the citation for scikit-learn.	dummyTurk
1135	0.0	We will gladly take all hints regarding typos that we overlooked when carefully editing this paper.	dummyTurk
1136	0.0	We thank the reviewer for the helpful comments and will gladly accept any advice on better structure.	dummyTurk
1137	0.0	We feel really delightful for your comments expressing many strength points of our paper.	dummyTurk
1138	0.0	Unfortunately, we cannot directly release the dataset, since the articles are not ours.	dummyTurk
1139	0.0	However, the urls of them can be publicly available, and we will release the urls to enable comparison with our results for testing.	dummyTurk
1140	0.0	We agree this weakness, but we want to emphasize the results of the human evaluation rather than those of the ROUGE evaluation, since the human evaluation is what we want.	dummyTurk
1141	0.0	As shown in Table 2, GateFusion has at least a strong advantage for usefulness.	dummyTurk
1142	0.0	In addition, we confirmed that GateFusion (3.357) performed better than OpenNMT (3.345) in terms of the average of readability and usefulness.	dummyTurk
1143	0.0	For your information, GateFusion also performed better than MultiModal (3.316) and QueryBased (3.318).	dummyTurk
1144	0.0	These differences are statistically significant (p<0.01) based on a one-tailed, paired t-test.	dummyTurk
1145	0.0	We are sorry for the lack of explanations.	dummyTurk
1146	0.0	We have an implicit motivation to construct a short-title-generation model in the same way as professional editors who read both headline and lead.	dummyTurk
1147	0.0	Therefore, we chose two models, GateFusion and QueryBased, on the basis of the ROUGE results of the simple and complex subsets.	dummyTurk
1148	0.0	Dear Reviewer,	dummyTurk
1149	0.0	Although our approaches are similar to the work cited, there are two key differences: first, we use fewer parameters in the state propagation method; second, we consider the whole document structure with the document attention and the attention is performed at sentence level (represented with skipt-thoughts), while Jean et al.	dummyTurk
1150	0.0	is at word level across sentences.	dummyTurk
1151	0.0	Sorry if this was not clear, we will use the extra page to make this part clear after revision.	dummyTurk
1152	0.0	"Comment ""Section 3."	dummyTurk
1153	0.0	Information(...).	dummyTurk
1154	0.0	""":"	dummyTurk
1155	0.0	We apologize for accidentally omitting the datasets sizes.	dummyTurk
1156	0.0	Sizes are: KFTT ~0.4M pairs, TED: 0.16M and 0.18M for EN-DE and EN-FR, respectively.	dummyTurk
1157	0.0	As for test sizes: ~ 4k sentences for both EN-DE and EN-FR, while ~1k sentences for EN-JA.	dummyTurk
1158	0.0	We measured the statistical significance of our methods across the multiple runs with multeval (https://github.com/jhclark/multeval).	dummyTurk
1159	0.0	With statistically significance and a p-value < 0.05: EN-JA both source side propagation and document attention; JA-EN all methods; EN-De source side and target side propagation, and EN-FR source side propagation.	dummyTurk
1160	0.0	Thanks for your kind advice.	dummyTurk
1161	0.0	We have corrected the typos, grammatical mistakes and inaccuracies as you suggested.	dummyTurk
1162	0.0	We have been carefully editing the paper since the submission on Feb. 22th.	dummyTurk
1163	0.0	And we are ready to contact ELSEVIER WebShop for further proofreading.	dummyTurk
1164	0.0	Could you please refer to the newest experiment we respond to Reviewer2 due to the limitation of word number limitation.	dummyTurk
1165	0.0	We add the statistical significance of the results as you suggested.	dummyTurk
1166	0.0	We will shorten the paper within 8 pages and make sure it will not influence the soundness and readability of the paper.	dummyTurk
1167	0.0	"1) Deep RNN with 4 layers on encoder and decoder and skip connections, specifically the best configuration from ""Deep architectures for neural machine translation"" by Miceli Barone et al, WMT 2017"	dummyTurk
1168	0.0	2) We incorporate Vaswani 2017's ADAM settings, including Beta2 for the synchronous baseline: beta1=0.9 and beta2=0.98 and using their warmup and decay strategies.	dummyTurk
1169	0.0	3) Yes.	dummyTurk
1170	0.0	This is the point in training at which local optimizers stop being beneficial and we turn them off.	dummyTurk
1171	0.0	"4) ""Accurate, large minibatch SGD: training imagenet in 1 hour"" (Goyal et al.)"	dummyTurk
1172	0.0	says to scale the learning rate with batch size, which is why it should perform as good as the baseline.	dummyTurk
1173	0.0	However we see that this is not the case.	dummyTurk
1174	0.0	Nonetheless, this motivates our use of a higher average flexible learning rate.	dummyTurk
1175	0.0	5) We thank the reviewer for the suggestion.	dummyTurk
1176	0.0	We will use the space for wall-clock time table for experimental settings and compact section 2 to the details of the architecture that may differ from other papers.	dummyTurk
1177	0.0	####Comments#####	dummyTurk
1178	0.0	We thank the reviewer for their comments.	dummyTurk
1179	0.0	1) Thank you for pointing out inconsistencies we will fix.	dummyTurk
1180	0.0	LocalOPTs is indeed the same setting as LocalOpts as warmup.	dummyTurk
1181	0.0	2) Regarding the formula for scaling the learning rate, the equation form of line 296 is lr = 0.000000052 * number_of_target_words.	dummyTurk
1182	0.0	Momentum cooldown was tuned by doing a parameter sweep.	dummyTurk
1183	0.0	We found the optimal parameters to be beta1=0.91 and beta2=0.998 (changed from 0.9 and 0.999 respectively).	dummyTurk
1184	0.0	We will add this clarification to the camera ready version.	dummyTurk
1185	0.0	3) ADAM is scale-invariant, so scaling makes no difference.	dummyTurk
1186	0.0	Scaling the gradient by sentences or words is not the same as scaling the learning rate.	dummyTurk
1187	0.0	In a sense, our flexible learning rate is restoring what you are trying to achieve by scaling the gradient.	dummyTurk
1188	0.0	4) We apologize for the lack of table for the overall results in Section 3.	dummyTurk
1189	0.0	We will easily add that.	dummyTurk
1190	0.0	We thank the reviewer for their valuable feedback.	dummyTurk
1191	0.0	  The challenge corpus is mainly released with visual and audio information, and the test set used in the challenge was never released.	dummyTurk
1192	0.0	The challenge also did not concentrate on multimodal analysis, but mostly on video analysis, with the same methods often applied to audio without adaptation.	dummyTurk
1193	0.0	We have also included text information by sending request to the chalenge organisers to obtain the transcriptions.	dummyTurk
1194	0.0	The main aim of our research is to investigate the effect of adding different modalities in the personality recognition task, where this analysis was never performed before.	dummyTurk
1195	0.0	Sure, we will cite Polzehl et al.	dummyTurk
1196	0.0	"""Automatically assessing personality from speech""."	dummyTurk
1197	0.0	Please note that they did not make use of deep learning methods, and also analyzed a different corpus, but part of the audio analysis is similar.	dummyTurk
1198	0.0	-------	dummyTurk
1199	0.0	WeaknessArgument1: Thanks for the suggestion.	dummyTurk
1200	0.0	It is still our priority to add reliable human evaluation for the camera-ready version.	dummyTurk
1201	0.0	Human evaluation is quite difficult (time-taking/expensive) for CNNDM summarization, due to extremely long articles and subjective summaries; hence, for reliability+generalization of our results, we show statistically significant and state-of-the-art improvements on both versions of CNNDM (where we use the same hyperparameters), as well as statistically significant jumps on a test-only transfer to out-of-domain DUC2002 (without any retraining/retuning).	dummyTurk
1202	0.0	-------	dummyTurk
1203	0.0	WeaknessArgument2: We did run a fluency test using a language model trained on a large corpus (Sec6.4 Line782, SecC.3/Table1 of Supplementary) -- it shows that our summaries achieve lower perplexity (i.e., better fluency) than the reference summaries.	dummyTurk
1204	0.0	Thanks for the additional parser suggestion; we will incorporate this in the final version.	dummyTurk
1205	0.0	 We apologize for the confusion.	dummyTurk
1206	0.0	SummaRuNNer and bgru+runner use the same model structure (the model proposed in (Nallapati et al., 2017)), but different training methods (MLE vs. RL).	dummyTurk
1207	0.0	This point will be clarified in the next version of the paper.	dummyTurk
1208	0.0	 We use the same calculation as in Abigail See et al.???s paper to compare our model (trained on the un-anonymized data) with SummaRuNNer (trained on the anonymized data).	dummyTurk
1209	0.0	The disparity in the lead-3 scores is (+0.76 ROUGE-1, +1.76 ROUGE-2, +0.59 ROUGE-L) points respectively, and our best model scores  (+0.8 ROUGE1, +1.77 ROUGE-2, +1.25 ROUGE-L) points above lead3 of SummaRuNNer.	dummyTurk
1210	0.0	We take the difference and conclude that we obtain (+0.04 ROUGE1, +0.01 ROUGE-2, +0.66 ROUGE-L) points gain over SummaRuNNer.	dummyTurk
1211	0.0	Ideally, we???d like to run the model on the anonymized data and compare the models directly.	dummyTurk
1212	0.0	However, we were not able to find the anonymized version as in SummaRuNNer.	dummyTurk
1213	0.0	We thus used the un-anonymized version of the data and performed the comparison as described in Abigail See???s paper since ROUGE is the only available means of comparison with Nallapati et al.???s work.	dummyTurk
1214	0.0	In addition, we performed the human evaluation as explained in the response to review 2 weakness 1.	dummyTurk
1215	0.0	 we have given reference for the first statistical method used so we did not describe it again.	dummyTurk
1216	0.0	  merge in Minimalist program (Chomsky; 1995) and projection principle in X bar theory (Chomsky; 1986)	dummyTurk
1217	0.0	 'Chomsky adjoined' is a commonly used phrase in theoretical linguistics in which a syntactic phrase is joined to the preceding syntactic phrase under the rule known as 'adjunction' (Chomsky;1986).	dummyTurk
1218	0.0	Maybe explicit referencing and rephrasing will make things clearer.	dummyTurk
1219	0.0	 there are some obvious issues with numbering which is a gross oversight.	dummyTurk
1220	0.0	this will be corrected for camera ready paper if selected	dummyTurk
1221	0.0	 Other NLP tasks such as relationship detection, language prediction etc.	dummyTurk
1222	0.0	we did not mention them because we are using this parser to only enhance clinical entity detection at this point.	dummyTurk
1223	0.0	but we expect to implement it in other areas of NLP as implicated our organisational goals	dummyTurk
1224	0.0	 By Berekeley grammar, we mean the default english grammar model that comes in an open source download of the toolkit	dummyTurk
1225	0.0	 yes , these are obvious mistakes that have been overlooked.	dummyTurk
1226	0.0	it will be rectified in camera ready paper if selected	dummyTurk
1227	0.0	 yes, there could be more discussion on the significance of the results	dummyTurk
1228	0.0	 yes, a syntactic tree is ideal but bracketing label was adopted for limited space issues.	dummyTurk
1229	0.0	a tree would have taken more space and there are many examples we wanted to illustrate.	dummyTurk
1230	0.0	 licensing condition is delicate as this is health data.	dummyTurk
1231	0.0	but a parser trained on this health data will be available in the near future	dummyTurk
1232	0.0	 Penntreebank contains 325 FRAGs out of 49000 sents.	dummyTurk
1233	0.0	Other treebanks' F1 score on different parser engines as reported by Min Jiang et al (2015) (referred in our paper)	dummyTurk
1234	0.0	Best results were obtained by combining MiPACQ corpus with WSJ corpus, and that were	dummyTurk
1235	0.0	With Stanford parser, F1 = 84.15	dummyTurk
1236	0.0	With Bikel parser,       F1 = 78.03	dummyTurk
1237	0.0	With Charniak parser F1 = 83.59	dummyTurk
1238	0.0	Thanks a lot for your kind comments.	dummyTurk
1239	0.0	  In this paper, we use three ways to prevent the risk of overfitting limiting the scalability of this method.	dummyTurk
1240	0.0	(1)The objective function is the cross-entropy loss.	dummyTurk
1241	0.0	Here, we employ the L2 regularization term to avoid overfitting.	dummyTurk
1242	0.0	(2)We use early stop mechanism in the training process.	dummyTurk
1243	0.0	(3)The datasets of SemEval2017 task7 and Pun of the Day provide training sets and test sets.	dummyTurk
1244	0.0	In training sets, we divide the training and development sets to train the parameters.	dummyTurk
1245	0.0	After that, we apply 5-fold cross validation to prevent overfitting.	dummyTurk
1246	0.0	Thanks reviewer for good comments and hard work.	dummyTurk
1247	0.0	  Before the publication, we will continue to modify this paper for meeting the requirements of the revision.	dummyTurk
1248	0.0	(1)For table 3, the result of our method is higher than N-Hance (the second one in SemEval2017 task7).	dummyTurk
1249	0.0	Here, we apply Pun of the Day as training sets and SemEval2017 task7 as test sets.	dummyTurk
1250	0.0	To evaluate the performance of our model and N-Hance, we apply all of the 2250 homographic contexts.	dummyTurk
1251	0.0	(2)For table 4, the method Fermi, due to the organizer Miller (Miller, Hempelmann, Gurevych, 2017), it only evaluates on 675 of the 2250 homographic contexts.	dummyTurk
1252	0.0	When we apply the same data distribution (1575 contexts as training sets, 675 contexts as test sets), our method outperforms Fermi.	dummyTurk
1253	0.0	Miller T, Hempelmann C F, Gurevych I. SemEval-2017 Task 7: Detection and interpretation of English puns[C]//Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017).	dummyTurk
1254	0.0	Vancouver, BC.	dummyTurk
1255	0.0	2017.	dummyTurk
1256	0.0	  The Pun of the Day dataset is provided by Yang (Yang et al., 2015).	dummyTurk
1257	0.0	They use Pun of the Day to conduct recognition experiments.	dummyTurk
1258	0.0	The dataset only contains positive text.	dummyTurk
1259	0.0	In order to acquire negative samples, they sample negative samples from four resources, including AP News, New York Times, Yahoo!	dummyTurk
1260	0.0	Answer and Proverb.	dummyTurk
1261	0.0	They extract their negative instances in a way that tries to minimize such domain differences by (1) selecting negative instances whose words are all contained in our positive instance word dictionary and (2) forcing the text length of non-pun instances to follow the similar length restriction.	dummyTurk
1262	0.0	Yang, D., Lavie, A., Dyer, C., et al.	dummyTurk
1263	0.0	: Humor recognition and humor anchor extraction.	dummyTurk
1264	0.0	In: EMNLP, pp.	dummyTurk
1265	0.0	2367???2376 (2015).	dummyTurk
1266	0.0	We would like to express our sincere thanks to the reviewer for recognizing the novelty and providing very positive comments to our work.	dummyTurk
1267	0.0	  For homographic puns recognition, more clues are provided by the collocation of candidate pun words.	dummyTurk
1268	0.0	The candidate pun words consist of nouns, verbs, adjectives and adverbs in each pun.	dummyTurk
1269	0.0	For each type of candidate pun words, we design an attention mechanism to obtain useful weights.	dummyTurk
1270	0.0	Then we concatenate the four types to aggregate informative words.	dummyTurk
1271	0.0	We consider the relation of the candidate puns.	dummyTurk
1272	0.0	In Figure 5, the weight of the word [sleep] is highest in this sentence, compared to the word [deciding] and [mattress].	dummyTurk
1273	0.0	Reply to weakness1 as question1.	dummyTurk
1274	0.0	Reply to weakness2 as question 2.	dummyTurk
1275	0.0	    Comparison against a gold standard,	dummyTurk
1276	0.0	    Data-driven evaluation,	dummyTurk
1277	0.0	    Manual User-based evaluation,	dummyTurk
1278	0.0	    Application or task-based evaluation.	dummyTurk
1279	0.0	As there is no golden standard to compare the results with, the first evaluation method cannot be performed.	dummyTurk
1280	0.0	Data-driven evaluation which is the process of comparing ontology against existing data about the domain is exactly the procedure that has been followed to construct the ontology.	dummyTurk
1281	0.0	The third method; manual evaluation of ontology by an expert is what we did during the ontology building lifecycle and after it.	dummyTurk
1282	0.0	The built ontology before being applied in the application was examined by an expert to see if it satisfies the competency questions, if the relations, tags, properties and hierarchies are correct.	dummyTurk
1283	0.0	Any incorrect part of the ontology is fixed by the expert at this phase.	dummyTurk
1284	0.0	 For evaluating the ontology via an application, we applied it in our expert finding system.	dummyTurk
1285	0.0	For each query, the first 5 expert recommendations of the expert finding system are considered.	dummyTurk
1286	0.0	For each recommended expert, the evaluator checks if the expert???s papers which leads the system to this recommendation are relevant to the query or not.	dummyTurk
1287	0.0	Evaluation shows that 92.64% of ontological relations used by the system to recommend an expert were semantically true.	dummyTurk
1288	0.0	In other words %92.64 of the assessed papers are correctly associated to the query topics in the ontology.	dummyTurk
1289	0.0	Without using the ontology, the expert finding system finds experts whose papers (titles and abstracts) contain the exact keywords in the query and ignore those containing concepts related (synonym or hyponym or other relations) to the query.	dummyTurk
1290	0.0	Without the ontology although the precision is high as well, the recall is very low.	dummyTurk
1291	0.0	Actually the automatic evaluation we reported is not related to the application for which we constructed the ontology.	dummyTurk
1292	0.0	Different measurements of the depth and breadth of ontology, number of concepts and relations and properties, and dispersion measurements are used to show the features of the ontology of NLP independent of the expert finding system.	dummyTurk
1293	0.0	These will be useful for further comparisons with newer versions.	dummyTurk
1294	0.0	The task of natural question generation is first proposed in???Generating Natural Questions About an Image???from Mostafazadeh et al.	dummyTurk
1295	0.0	at ACL 2016.	dummyTurk
1296	0.0	Different from descriptive questions that can be answered with the content of the image easily, natural questions are related to feelings and inference of human being.	dummyTurk
1297	0.0	Descriptive questions is designed to evaluate object recognition tasks in CV, while natural questions involves deep understanding of the target image.	dummyTurk
1298	0.0	It can be used to raise a conversation for a chatbot.	dummyTurk
1299	0.0	Indeed, it is difficult to formulate the definition of ???naturalness??? like many other language concepts.	dummyTurk
1300	0.0	Therefore, we propose to use negative samples and reinforcement learning to teach the generator what the attribute really is.	dummyTurk
1301	0.0	 As mentioned in the paper, a natural question should have two attributes, ???human-written??? and ???natural???.	dummyTurk
1302	0.0	VQA dataset can help the system learn the attribute of ???human-written???.	dummyTurk
1303	0.0	Besides, as we described in the hierarchical structure, questions of ???human-written??? can also guide the generator to produce more natural questions than ???machine generated??? ones.	dummyTurk
1304	0.0	Thanks for pointing this out.	dummyTurk
1305	0.0	We will improve the table and figure in next version.	dummyTurk
1306	0.0	Teaching machine the skill of asking is important in a variety of areas, e.g., providing demonstration in child education, initializing an interesting conversation for chatbot, etc.	dummyTurk
1307	0.0	On the other hand, it can help the question answering task by constructing question sets automatically to reduce the labor of human annotation.	dummyTurk
1308	0.0	Furthermore, training a system to ask a natural question (not only answer a question) may imbue the system with what appears to be a cognitive ability unique to humans among other primates.	dummyTurk
1309	0.0	We did not perform subjective test on VQA dataset.	dummyTurk
1310	0.0	Questions collected from VQA is for the purpose of understanding an image in a shallow way.	dummyTurk
1311	0.0	It is different from natural questions in nature.	dummyTurk
1312	0.0	We can also identify the difference easily from sample questions of VQA and VQG.	dummyTurk
1313	0.0	In table 1, we provide the results for automatic evaluation.	dummyTurk
1314	0.0	Several relevance metrics are used.	dummyTurk
1315	0.0	From these relevance metrics, we can see how good our model is fitting the target questions.	dummyTurk
1316	0.0	Thanks for the insightful comments!	dummyTurk
1317	0.0	[Why topic models is suitable for short text classification?]	dummyTurk
1318	0.0	Short texts have severe data sparsity issue, which leads to the consequence that there are limited features for classification (as described in line 74 -- 83).	dummyTurk
1319	0.0	In this work, we employ topic models to capture corpus-level latent topics for enriching representations for short texts (described in line 94-112).	dummyTurk
1320	0.0	Besides, Phan et al.	dummyTurk
1321	0.0	(2008) has demonstrated that latent topics yielded by topic models can help alleviating data sparsity in short text classification.	dummyTurk
1322	0.0	Xuan Hieu Phan, Minh Le Nguyen, Susumu Horiguchi: Learning to classify short and sparse text & web with hidden topics from large-scale data collections.	dummyTurk
1323	0.0	WWW 2008: 91-100	dummyTurk
1324	0.0	[Will topic memory networks be helpful to long document classfication?]	dummyTurk
1325	0.0	Yes, it can be helpful.	dummyTurk
1326	0.0	This is potential future work to explore.	dummyTurk
1327	0.0	[Why your topic model is better than previous topic models?]	dummyTurk
1328	0.0	We engage neural topic models (NTM).	dummyTurk
1329	0.0	Distinguished from conventional topic models (e.g.	dummyTurk
1330	0.0	LDA), in NTM, latent variables are parameterized via neural networks and can be inferred by back-propagation.	dummyTurk
1331	0.0	NTM has been considered better than conventional topic models as shown in Miao et al (2017) and Srivastava and Sutton (2017).	dummyTurk
1332	0.0	Also, NTM allows us to jointly infer topics and classify short text instances in a neural network framework, which is beneficial to both tasks.	dummyTurk
1333	0.0	Yishu Miao, Edward Grefenstette, Phil Blunsom: Discovering Discrete Latent Topics with Neural Variational Inference.	dummyTurk
1334	0.0	ICML 2017: 2410-2419	dummyTurk
1335	0.0	Srivastava, Akash, and Charles Sutton: Autoencoding variational inference for topic models.	dummyTurk
1336	0.0	ICLR 2017.	dummyTurk
1337	0.0	Thanks for the insightful comments!	dummyTurk
1338	0.0	[Why we use the term ???topic model????]	dummyTurk
1339	0.0	Our neural topic model (NTM) can explicitly model the topic mixtures of documents and topic-word distributions.	dummyTurk
1340	0.0	This is consistent with the description in Steyvers and Griffiths (2007): ???Topic models are based upon the idea that documents are mixtures of topics, where a topic is a probability distribution over words.???.	dummyTurk
1341	0.0	Our NTM is extended from Miao et al.	dummyTurk
1342	0.0	(2017), which is interpreted as a generative probabilistic model.	dummyTurk
1343	0.0	Specifically, NTM employs neural networks to parameterize the document-topic and topic-word distributions.	dummyTurk
1344	0.0	As shown in line 275--277 of our paper, \theta is sampled from a Gaussian softmax prior (Miao et al.	dummyTurk
1345	0.0	2017), instead of computed by a deterministic process.	dummyTurk
1346	0.0	Steyvers Mark, and Tom Griffiths.	dummyTurk
1347	0.0	"""Probabilistic topic models."""	dummyTurk
1348	0.0	Handbook of latent semantic analysis 427.7 (2007): 424-440.	dummyTurk
1349	0.0	Yishu Miao, Edward Grefenstette, Phil Blunsom: Discovering Discrete Latent Topics with Neural Variational Inference.	dummyTurk
1350	0.0	ICML 2017: 2410-2419	dummyTurk
1351	0.0	[The comparison is weak?]	dummyTurk
1352	0.0	Our model is not ???RNN with some variational autoencoding information???.	dummyTurk
1353	0.0	Instead, our topic memory networks can be combined with any classifier (e.g.	dummyTurk
1354	0.0	CNN, RNN or RNN variants) (see line 226 -- 233 and Figure 1).	dummyTurk
1355	0.0	Our work focuses on exploiting latent topics to enrich features for short text classification with a topic memory mechanism (see line 106 -- 107) regardless of what classifier it combines.	dummyTurk
1356	0.0	In our experiments, we use CNN as the classifier because CNN has been effectively demonstrated on short texts (Wang et al.	dummyTurk
1357	0.0	2017).	dummyTurk
1358	0.0	Also, we compare with BiLSTM (a stronger variant of RNN) in classification evaluation, whose results are worse than CNN on all the four datasets (see Table 3).	dummyTurk
1359	0.0	Jin Wang, Zhongyuan Wang, Dawei Zhang, Jun Yan: Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification.	dummyTurk
1360	0.0	IJCAI 2017: 2915-2921	dummyTurk
1361	0.0	[Is NTM optimized by cross-entropy?]	dummyTurk
1362	0.0	No, we employ variational lower bound to optimize NTM (described in line 287), and use cross-entropy to optimize the classification output.	dummyTurk
1363	0.0	The loss function for the entire framework is defined in Eq.	dummyTurk
1364	0.0	7.	dummyTurk
1365	0.0	" ""Some related woks have studied the authorship prediction with the keystroke dynamics."""	dummyTurk
1366	0.0	We are aware of most prior work being concerned with author verification, which is an easier setup, as we point out and discuss in Section 5.	dummyTurk
1367	0.0	We would be grateful for pointers to papers which we seem to miss.	dummyTurk
1368	0.0	" ""There are a lot of works about the application of the recurrent neural networks (LSTM and GRU) for user verification based on keystroke dynamics"""	dummyTurk
1369	0.0	We only found one related paper (http://ecmlpkdd2017.ijs.si/papers/paperID236.pdf) which we will include, but we would again be grateful for pointers to those papers that we seem to miss.	dummyTurk
1370	0.0	" ""The theoretical novelty is weak except that the optimal results are found by analyzing the combination experiments of extra-linguistic factor and text factor."""	dummyTurk
1371	0.0	Our experiments are mostly empirical, but add to the theoretical insight that extra-linguistic factors are better disclosed in written text, while identity is better reflected in typing behavior.	dummyTurk
1372	0.0	We believe this is an important and novel contribution.	dummyTurk
1373	0.0	 We describe which embeddings are used and how we use them.	dummyTurk
1374	0.0	We are not sure which explanation should be included here.	dummyTurk
1375	0.0	Thanks very much for your advices!	dummyTurk
1376	0.0	The works of WSD are really meaningful fix-ups.	dummyTurk
1377	0.0	 This do is a key element in building commercial SDS.	dummyTurk
1378	0.0	From our perspective, most template libraries in commercial SDS are constructed manually.	dummyTurk
1379	0.0	In our paper, the template library we used is provided by a commercial SDS.	dummyTurk
1380	0.0	And, to follow the ACL anonymous policy, we do not illustrate the details of the construction process of these templates.	dummyTurk
1381	0.0	 Actually, we do ignore the relationships to the WSD.	dummyTurk
1382	0.0	This is really a helpful suggestion.	dummyTurk
1383	0.0	Thank you very much!	dummyTurk
1384	0.0	Reply to the works of ATIS task that use semantic structure: We will seriously compare our method with the previous works on ATIS that use the semantic structure.	dummyTurk
1385	0.0	Thank you for your advice.	dummyTurk
1386	0.0	Thanks a lot for your insightful comments.	dummyTurk
1387	0.0	W1: A dependency tree is an ordered graph which embraces the head-dependent relations as well as the relative position of a dependent to its head, either on the left or right-hand side.	dummyTurk
1388	0.0	Under the NMT context, we use two variant GRU networks with different parameters to model the left and right-hand side dependents.	dummyTurk
1389	0.0	Similar approaches have also been explored in the previous works of Socher et al.	dummyTurk
1390	0.0	(2013) and Yang et al.	dummyTurk
1391	0.0	(2017).	dummyTurk
1392	0.0	W2: Recent studies have shown that the top-down encoding method can better propagate and capture the global syntactic and semantic information (Chen et al., 2017; Yang et al., 2017).	dummyTurk
1393	0.0	The intuition behind our approach is that we first model the head words (most important information) followed by its dependents (the supplementary information).	dummyTurk
1394	0.0	The experimental results have demonstrated the effectiveness of this proposed method.	dummyTurk
1395	0.0	W3: The approaches of Chen and Yang are of the same principle.	dummyTurk
1396	0.0	We chose to re-implement Yang???s method considering that it is the most recent model which has been further extended to sub-word level in a tree model.	dummyTurk
1397	0.0	Q1: Due to space limitations, we did not draw the root node (h_0) in Figure 3.	dummyTurk
1398	0.0	The h_0 is initialized using the equation in line 438.	dummyTurk
1399	0.0	We will add the root node in Figure 3.	dummyTurk
1400	0.0	Q2: It is the common practice to use the mean of source side representations as the initial state for the decoding (e.g.	dummyTurk
1401	0.0	dl4mt: https://github.com/nyu-dl/dl4mt-tutorial, nematus: https://github.com/EdinburghNLP/nematus).	dummyTurk
1402	0.0	Comments1: We want to express ???the upper nodes are difficult to learn the information as expected.??? We will revise it in the final manuscript.	dummyTurk
1403	0.0	Comments4: It gives a big drop of 6.73 bleu scores (From Model #2 to #3 of Table 4) when the word sequence is excluded from the attention.	dummyTurk
1404	0.0	Which means the sequence information of a constituency model is mainly derived from the sequence level.	dummyTurk
1405	0.0	However, our model outperforms that of the conventional tree method (Model #5 against Model #3), without using the explicit word sequence, showing that our method is able to capture the word sequence information in some extent.	dummyTurk
1406	0.0	Thank you very much for your insightful review.	dummyTurk
1407	0.0	Both the datasets are limited, but the German one is even smaller than the Italian one, hence the difference in performance with the finetuning (or the single language setup) probably not having enough data to be trained.	dummyTurk
1408	0.0	It is a good point.	dummyTurk
1409	0.0	It is indeed also true that perfoming data sampling might remove too much data to prevent any kind of multilingual correlation to be learnt.	dummyTurk
1410	0.0	In any case we are planning in a future work to analyze the amount of data needed to train an affect system in the most effective way, by collecting even more data and experimenting to see the correlation between data size and performance.	dummyTurk
1411	0.0	We did not apply any specific preprocessing, if not feeding the squared signal in a different channel and randomize the input volume to better estimate energy.	dummyTurk
1412	0.0	We have some graphics, which we could not include for space constraints (not so related with our multilingual claim), where we apply FFT to the first layer parameters and clearly show that this layer is leveraging pitch and energy features.	dummyTurk
1413	0.0	We will consider including the plots and discuss them in the extra page if the paper is accepted.	dummyTurk
1414	0.0	The agreement was generally not high, but most of the times the disagreement was between one emotion class (especially happy) and the neutral (or garbage) class, or between sadness and anxiety.	dummyTurk
1415	0.0	We did discarded all the disagreement cases leaving only the samples where at least three annotators agreed.	dummyTurk
1416	0.0	The utterances annotated once only were always obtained by a pool of graduate students consisting from the authors of this paper and their colleagues in the same research group.	dummyTurk
1417	0.0	They were very confident about the task and were instructed to discard all the samples where an unambiguous decision could not be obtained.	dummyTurk
1418	0.0	Thank you for the thoughtful comments.	dummyTurk
1419	0.0	We agree that experimenting with other types of document structure is an important research direction and will add a discussion on this important point in the camera-ready version.	dummyTurk
1420	0.0	We mean that each question is paired with a small set of documents, but the number of questions is large.	dummyTurk
1421	0.0	We will clarify this in the camera-ready version.	dummyTurk
1422	0.0	Indeed relating our work to walkers on DOM structures is relevant, and we will add that for the camera-ready.	dummyTurk
1423	0.0	We thank the reviewer for the valuable and concise feedback.	dummyTurk
1424	0.0	We will address the concerns brought up in the review in turn:	dummyTurk
1425	0.0	* Indeed we rely on improvements in BLEU alone to conclude that useful properties must have been captured by the graphs when used with Emb/CNN encoders.	dummyTurk
1426	0.0	The argument about distance indeed has more to do with the graphs being non-trivial (as opposed to those obtained with recurrent encoders) than with them capturing linguistic generalisations.	dummyTurk
1427	0.0	We will make the argument more explicit.	dummyTurk
1428	0.0	* We use Google???s https://github.com/tensorflow/nmt for our baselines and trust that results from this toolkit are well regarded by the community.	dummyTurk
1429	0.0	We do not have external baselines, but our Emb/CNN/RNN baselines offer a direct contrast to our contribution, which we felt was more appropriate for a short submission.	dummyTurk
1430	0.0	* Perhaps the paper is dense because some of the techniques related to deep generative models are not yet popular in the field.	dummyTurk
1431	0.0	We intend to use a good portion of the extra page allowed for publication to address this issue.	dummyTurk
1432	0.0	Thank you for the good suggestions.	dummyTurk
1433	0.0	We will design some stronger baselines to evaluate the performance of our method as your suggestions.	dummyTurk
1434	0.0	Thank you very much, we will rethink this problem and rewrite this part.	dummyTurk
1435	0.0	We will add some related work in this paper as suggested by you.	dummyTurk
1436	0.0	Thank you for your suggestion.	dummyTurk
1437	0.0	We will revise the paper carefully and invite a native speaker to improve the paper.	dummyTurk
1438	0.0	The MIR system used in this paper ranked 1st in Wikipedia corpus based task and 2nd in arvix corpus based task on NTCIR-12.	dummyTurk
1439	0.0	We will add more information about this part, thank you so much for your suggestion.	dummyTurk
1440	0.0	 Our proposed model is that a personalized CWI model be used in both the CWI step and Substitution Selection step (see first paragraph, Section 3).	dummyTurk
1441	0.0	 The dataset was derived from those of Paetzold and Specia (2016) and Ehara (2010).	dummyTurk
1442	0.0	We will seek consent to release the dataset.	dummyTurk
1443	0.0	 We are happy to include an example in the final version of the paper.	dummyTurk
1444	0.0	First of all we would like to thank the reviewer for having reviewed our paper and for the useful comments.	dummyTurk
1445	0.0	Using character n-gram models alone achieves the best results while compared to other individual embedding models (To our knowledge, we are the first showing the performance of character n-gram models for bilingual terminology extraction from comparable corpora).	dummyTurk
1446	0.0	The second contribution is the usefulness of using external data in addition to specialized data.	dummyTurk
1447	0.0	We have shown that in-domain and out-of-domain combination always benefits terminology extraction.	dummyTurk
1448	0.0	The third contribution is the usefulness of combining different embedding models (using the ensemble approach).	dummyTurk
1449	0.0	For instance the concatenation of CharCBOW with CharSG or CharCBOW wit CBOW, etc.	dummyTurk
1450	0.0	The external resources are not homogeneous and this unstable issue obviously reflects the results.	dummyTurk
1451	0.0	What about languages for which wikipedia or common crawl do not give enough data?	dummyTurk
1452	0.0	One of the aims of this paper was to show that using any type of resource, homogeneous or not, is useful for bilingual terminology extraction.	dummyTurk
1453	0.0	Even if the results vary depending on the nature of the data sets, combining specialized and external data as well as different embedding models is helpful.	dummyTurk
1454	0.0	That was the main purpose of the paper.	dummyTurk
1455	0.0	The paper gives an example for languages where there already exist language models.	dummyTurk
1456	0.0	What about in cases when there is no such model available beforehand?	dummyTurk
1457	0.0	Table 1 gives examples of pre-trained embedding models, however, we only used the Wikipedia pre-trained models provided by Bojanowski (CharSg and CharCBOW) because they are available in French and English.	dummyTurk
1458	0.0	All other models used in our experiments, were trained by us using the tools mentioned in the paper.	dummyTurk
1459	0.0	I did not see much thoughts on the problem of handling ambiguity - especially in cases when common words can be ambiguous with specialized terms in a certain domain.	dummyTurk
1460	0.0	We didn't treat ambiguity explicitly.	dummyTurk
1461	0.0	We think that ambiguity is partially solved by combining models from different domains.	dummyTurk
1462	0.0	However, this statement needs to be confirmed in future work.	dummyTurk
1463	0.0	 What do the authors think about the improvements of the dependency-based and structure-based models?	dummyTurk
1464	0.0	(in relation to footnote 13).	dummyTurk
1465	0.0	If improved enough, they might be quite powerful.	dummyTurk
1466	0.0	We definitely think that improving dependency-based and structured-based models should improve the performance of ensemble approaches, especially because of the specificities of dependency relations which captures more precise relations and should be complementary to other models.	dummyTurk
1467	0.0	However, dependency-based models greatly depend on the quality of the dependency parser, also, it is very time consuming compared to the standard context-based models when using large data sets.	dummyTurk
1468	0.0	We let a deep study of dependency-based and structured-based models for future work.	dummyTurk
1469	0.0	We thank this reviewer for his helpful comments that help improving the paper.	dummyTurk
1470	0.0	AMR parsing contains two subtasks: (1) concept identification; (2) relation constructions.	dummyTurk
1471	0.0	In a transition-based model, we apply transition strategy to construct AMR graphs based on transition actions.	dummyTurk
1472	0.0	We can generate AMR graphs by transiting ConSets in the buffer and stacks.	dummyTurk
1473	0.0	We discussed in this paper that pervious models only consider word features, but ignore the extensive information in concepts.	dummyTurk
1474	0.0	These models cannot perform well on non-projectivity and reentrancy prediction when we meet some words which correspond to different concepts in different sentences.	dummyTurk
1475	0.0	In previous models, they only apply word embeddings in parsing.	dummyTurk
1476	0.0	However, word embeddings cannot capture the inherent semantic information when a word maps to different ConSets in different AMR graphs.	dummyTurk
1477	0.0	The basic component in an AMR graph are ConSets rather than words.	dummyTurk
1478	0.0	We apply a heuristic method to identify consets in parsing.	dummyTurk
1479	0.0	Each word maps to the most frequent ConSet.	dummyTurk
1480	0.0	This is a relatively simple and efficient method for ConSets identification.	dummyTurk
1481	0.0	In Section 1, we demonstrate that effective representation of a ConSet plays a vital role in recognition of relationship between concepts.	dummyTurk
1482	0.0	We evaluate the model without ConSet embeddings in Section 4.	dummyTurk
1483	0.0	Compared with the model without ConSet embedding, Our CEO parser have some improvements on smatch F1.	dummyTurk
1484	0.0	Due to the lack of space, Reentrancy identification and arc labeling evaluation of these two models are not mentioned in this manuscript.	dummyTurk
1485	0.0	We will add the comparison in Supplemental Material.	dummyTurk
1486	0.0	Bolding means the best result in comparison tables.	dummyTurk
1487	0.0	 The extra page in the final version will allow more discussion of the results.	dummyTurk
1488	0.0	 The extra page in the final version will allow more discussion of the errors.	dummyTurk
1489	0.0	The focus here has been on the methods.	dummyTurk
1490	0.0	 The discussion of mixed language texts was removed to focus on the main issue.	dummyTurk
1491	0.0	However, two points are relevant here	dummyTurk
1492	0.0	 The idea is to find the smallest number of characters that allows accurate predictions for a large number of languages.	dummyTurk
1493	0.0	We are trying to minimize sample size and 50 is the best we could do.	dummyTurk
1494	0.0	Thank you for your valuable and kind comments!	dummyTurk
1495	0.0	 Contrastive polarity is indeed an issue not only in sentiment and also irony/sarcasm task.	dummyTurk
1496	0.0	We are happy to mention this as future work in the revised version.	dummyTurk
1497	0.0	Thank you for your suggestion!	dummyTurk
1498	0.0	Thank you!	dummyTurk
1499	0.0	Upon acceptance, we will share a java implementation of our models, which should make it straightforward for others to apply or extend our work.	dummyTurk
1500	0.0	We agree that the best values in each column should be shown in bold, and we will do so in the final version.	dummyTurk
1501	0.0	We agree that it would be helpful to add standard deviations to the table and to report on statistical significance, and we will do so in the final version.	dummyTurk
1502	0.0	Note that from Table 3, which shows results for individual relation types, it is already intuitively clear that the improvements over the baselines are indeed significant, as there are very few relation types where the baselines perform best (although this is of course not a substitute for formal significance tests).	dummyTurk
1503	0.0	We also agree with the suggestion to add examples illustrating the strengths and weaknesses of the different models.	dummyTurk
1504	0.0	 Indeed, no agreement was computed, but all five law students were trained by the expert.	dummyTurk
1505	0.0	Also, the expert who validated the data manually annotates obligations and prohibitions as a core part of his/her main job based on strict guidelines of the company that provided the data.	dummyTurk
1506	0.0	 Indeed, it cannot be released due to confidentiality issues.	dummyTurk
1507	0.0	  A few indicative examples of errors can be easily added, but space does not allow us to include a more detailed error analysis.	dummyTurk
1508	0.0	 The main advantages of our method are	dummyTurk
1509	0.0	Thanks for the suggestions.	dummyTurk
1510	0.0	This will be clarified.	dummyTurk
1511	0.0	Tables 1 and 3 use the relation split data (which is the zero-shot task).	dummyTurk
1512	0.0	Figure 2 uses the entity split data.	dummyTurk
1513	0.0	This is only touched on in the data section, and should be repeated and clarified in the results section.	dummyTurk
1514	0.0	We will make the paper more self contained.	dummyTurk
1515	0.0	The zero-shot task (table 1) of finding the answer for a relation you haven???t encountered before is more difficult than answering  questions about entities you haven???t seen before (figure 2).	dummyTurk
1516	0.0	This isn???t made clear in the text and will be improved.	dummyTurk
1517	0.0	That???s a good idea, thanks.	dummyTurk
1518	0.0	It???s a random entity, so is unlikely to appear in the original passage.	dummyTurk
1519	0.0	Table 1 gives results for the zero-shot task in which the model tries to generalise to unseen relations.	dummyTurk
1520	0.0	This is much more difficult than generalising to unseen entities for a fixed set of relations, as shown in Figure 2.	dummyTurk
1521	0.0	The captions could definitely be improved.	dummyTurk
1522	0.0	Thanks for drawing our attention to this.	dummyTurk
1523	0.0	 Response selection produces responses by selecting one from candidate responses, which is heavily relied on the candidates and hard to be extended (the scale-up problem).	dummyTurk
1524	0.0	In contrast, response generation utilized in this paper could generate new responses without making a selection on candidate responses.	dummyTurk
1525	0.0	In this sense, our model does not have the scale-up problem.	dummyTurk
1526	0.0	 Evaluation for Natural Language Generation (NLG) is a challenging and under-researched problem [Why We Need New Evaluation Metrics for NLG, EMNLP2017].	dummyTurk
1527	0.0	Due to the diversity of natural languages, it???s significantly different about the referenced and human-generated responses [Mou et al., COLING2016].	dummyTurk
1528	0.0	So we introduce the unreferenced metrics, which the length of responses is employed in [Mou et al., COLING2016].	dummyTurk
1529	0.0	We also propose the number of nouns per response for evaluation, which shows the richness of responses in contents since nouns are usually meaningful.	dummyTurk
1530	0.0	We believe that longer and more-nouns responses are also better in diversity than general responses that are easy to generate.	dummyTurk
1531	0.0	Performances on all automatic metrics demonstrate the effectiveness of our model.	dummyTurk
1532	0.0	Due to labor-intensive and uncontrollable-quality human evaluation, our human evaluation is less detailed than the automatic evaluation.	dummyTurk
1533	0.0	We employ Cohen Kappa to quantify the agreement of annotations.	dummyTurk
1534	0.0	Cohen Kappa greater than 0.2 is valid [Liu, Chia-Wei, et al.	dummyTurk
1535	0.0	EMNLP2016], and our lowest Kappa is 0.32.	dummyTurk
1536	0.0	We will introduce the statistical testing and more detailed human evaluations in the revised version.	dummyTurk
1537	0.0	 We acknowledge that results are presented in a compressed form, mainly to meet the page limit.	dummyTurk
1538	0.0	Please note we also submitted a supplementary material to expand Table 1, detailing the scores at different Ks (see lines 275/276 on page 3).	dummyTurk
1539	0.0	If our work can be accepted, we will use the additional page to add detailed results.	dummyTurk
1540	0.0	We would like to highlight that the point of Table 1 is to show the difficulty of ATE, i.e., there is no one-size-fit-all method that works in any cases, as their performance always varies depending on the task, domain, dataset, etc.	dummyTurk
1541	0.0	We will discuss this more clearly in the result discussion.	dummyTurk
1542	0.0	 the suggestion of verifying word coverage is indeed interesting.	dummyTurk
1543	0.0	We admit that we did not study this in our experiment, but we believe this could be easily addressed in the next version of the paper.	dummyTurk
1544	0.0	We did not use a linear combination because we tested this and it is performing not as well as the current design.	dummyTurk
1545	0.0	The letter X in line 139 page 2 denotes either a term t_i or a document d_i (which should have been defined), please see the note in brackets between lines 140 and 141.	dummyTurk
1546	0.0	 as mentioned in our response above, we would like to refer our reviewer to the supplementary material that expands Table 1.	dummyTurk
1547	0.0	But again we should have clarified the main point that it is very difficult to develop an ATE method that works in all cases, which motivates this work, i.e., to develop some methods that can improve existing ATE methods in, potentially, any cases.	dummyTurk
1548	0.0	The reason for showing average P@K in Figure 1 is to present a summary of the key patterns, as it can be very difficult to present results at all five K's, for all 10 methods.	dummyTurk
1549	0.0	But we can add some clarifications with respect to the patterns observed at different K's.	dummyTurk
1550	0.0	We cannot use the space noticed by the reviewer to 'decompress' the content because short papers have a limit of 4 pages content+unlimited references.	dummyTurk
1551	0.0	We already have 4 full pages.	dummyTurk
1552	0.0	The additional 1 page is permitted for accepted paper to address reviewer comments.	dummyTurk
1553	0.0	Thanks for the appreciation of our work!	dummyTurk
1554	0.0	Indeed, one of our main contributions is to raise awareness to the possibility of leakage of sensitive information into vector representations derived from text data.	dummyTurk
1555	0.0	We also show that this cannot be removed completely using naive adversarial training, and stress the importance of directly measuring the leakage.	dummyTurk
1556	0.0	We believe it is important for people who work with textual data to be aware of these points.	dummyTurk
1557	0.0	 It might be caused by different sizes between English-Russian corpus and English-Chinese corpus that we used.	dummyTurk
1558	0.0	We randomly pick data from high-quality bilingual corpora to construct our training data in this task.	dummyTurk
1559	0.0	We have around 2 billion English-Chinese parallel sentence pairs, but 50 million in English-Russian.	dummyTurk
1560	0.0	Different sizes might lead to different degree of richness in the corpus.	dummyTurk
1561	0.0	The ratings were not distributed uniformly, however, we did not expect so either due to the tendency to avoiding the extreme cases.	dummyTurk
1562	0.0	The scale's strong wording ('Very, very high mental effort') might also be something that users simply do not identify themselves with often.	dummyTurk
1563	0.0	Even though the pre-study filtered segments, the inter-personal differences seem to be too high to ensure a uniform distribution.	dummyTurk
1564	0.0	We could also have used any other MT system.	dummyTurk
1565	0.0	However, we wanted to ensure we use a state-of-the-art MT to generate realistic errors.	dummyTurk
1566	0.0	As part of their translation degree studies all subjects participated in lectures and practice sessions on post-editing, with an introduction to Trados and covering basic PE concepts.	dummyTurk
1567	0.0	However, they are not professionals using these tools on a daily basis.	dummyTurk
1568	0.0	The scale comes from psychology and is frequently used in CL research.	dummyTurk
1569	0.0	It is a single question 'In solving or studying the preceding problem I invested' with 9 answer possibilities ranging from 'very very low mental effort' to 'very very high mental effort'.	dummyTurk
1570	0.0	They are familiar with Trados and post-editing as part of their translation degree studies.	dummyTurk
1571	0.0	They were not familiar with the exact keyboard, mouse, screen, and chair, as it was a lab study.	dummyTurk
1572	0.0	They had time to adjust in the beginning, however, this is obviously not the same as working with one's own equipment.	dummyTurk
1573	0.0	Our model in this paper is designed for task oriented system.	dummyTurk
1574	0.0	For non-task-oriented data, it's inappropriate to model intentions by latent variables with multi-nominal distribution.	dummyTurk
1575	0.0	Because the intentions are too diversified.	dummyTurk
1576	0.0	The samples of dialogue are stored in our hard drive, we provide one example here because of space limitation:	dummyTurk
1577	0.0	first turn	dummyTurk
1578	0.0	User                              : I need to find an expensive restaurant that's in the	dummyTurk
1579	0.0	                                     south section of the city.	dummyTurk
1580	0.0	our model, +db_vector, +RL       : what food are you looking for ?	dummyTurk
1581	0.0	second turn	dummyTurk
1582	0.0	User                              : no i don't care about the type of cuisine.	dummyTurk
1583	0.0	our model, +db_vector, +RL       : the good luck Chinese food takeaway serves Chinese	dummyTurk
1584	0.0	                                    food and is in the expensive pricertange.	dummyTurk
1585	0.0	third turn	dummyTurk
1586	0.0	User                              : what is their address ?	dummyTurk
1587	0.0	our model, +db_vector, +RL       : 82 Cherry Hinton Road Cherry Hinton .	dummyTurk
1588	0.0	first turn	dummyTurk
1589	0.0	User                              : I need to find an expensive restaurant that's in the	dummyTurk
1590	0.0	                                     south section of the city.	dummyTurk
1591	0.0	our model, +db_vector+RL         : what food are you looking for ?	dummyTurk
1592	0.0	second turn	dummyTurk
1593	0.0	User                              : no i don't care about the type of cuisine .	dummyTurk
1594	0.0	our model, +db_vector+RL         : there is The Good Luck .	dummyTurk
1595	0.0	would you like the	dummyTurk
1596	0.0	                                    information ?	dummyTurk
1597	0.0	third turn	dummyTurk
1598	0.0	User                              : what is their address ?	dummyTurk
1599	0.0	our model, +db_vector, +RL       : there address is 82 Cherry Hinton Road Cherry Hinton	dummyTurk
1600	0.0	                                     , is there anything else i can help you with ?	dummyTurk
1601	0.0	last turn	dummyTurk
1602	0.0	User                              : what is the food they serve ?	dummyTurk
1603	0.0	our model, +db_vector, +RL       : they serve Chinese food .	dummyTurk
1604	0.0	NVIL optimized model tends to generate short sentences and slot-related tokens due to high variances from gradients calculation.	dummyTurk
1605	0.0	However, MLE optimized model would generate more fluent and detailed response.	dummyTurk
1606	0.0	We would pay attention to typos.	dummyTurk
1607	0.0	 Please refer to Question 1.	dummyTurk
1608	0.0	  As pointed out in section 4.2, we ran several train and test runs, and reported the average (but for some models only had the time to do three runs).	dummyTurk
1609	0.0	We will add significance tests for the final version, as we now have sufficiently many runs of training and test; the results are stable and statistically significant.	dummyTurk
1610	0.0	 Our model consists of 1 word embedding layer (100 dimension), 1 bi-lstm, 1 lstm and several linear layers, the total size of parameters is around 5M, whilst the size in current state-of-the-art system (Qin et al.	dummyTurk
1611	0.0	2017) is more than 20M since they used 300 dimension word vectors and 512 neurons.	dummyTurk
1612	0.0	 The standard deviations for the accuracy in 11-way classification are around 0.81(PDTB-Lin),  0.65(PDTB-Ji) and 0.47(CV)	dummyTurk
1613	0.0	Specificity = how discriminative each caption is for finding its corresponding image.	dummyTurk
1614	0.0	Diversity = over all the generated captions, how often is the same caption repeated (high repetition = low diversity).	dummyTurk
1615	0.0	Novelty = how many generated captions were not just copies of training set captions.	dummyTurk
1616	0.0	Indeed, validating on R@1 would strengthen our argument, especially since we did improve this (though it dropped due to further training).	dummyTurk
1617	0.0	We will update the paper to include these annotations.	dummyTurk
1618	0.0	Consider a nearest neighbour caption retrieval approach: this system might have a high diversity (choosing different pre-existing captions each time) but would have zero novelty since it is restricted to only use captions it has already seen.	dummyTurk
1619	0.0	High novelty and diversity could be achieved by outputting nonsense captions.	dummyTurk
1620	0.0	However, this would lead to low specificity.	dummyTurk
1621	0.0	-----Reply to weaknesses-----	dummyTurk
1622	0.0	--standard deviation	dummyTurk
1623	0.0	Figure-3 showed that joint models outperform the best runs of s2s.	dummyTurk
1624	0.0	It just happens that the best run of s2s is within one std from mean.	dummyTurk
1625	0.0	--random selection	dummyTurk
1626	0.0	We used simulated situations to have more controlled experiments.	dummyTurk
1627	0.0	We also note that since test sets for IWSLT are taken from different documents than training/dev, our results are still valid.	dummyTurk
1628	0.0	--data size	dummyTurk
1629	0.0	We believe that low-resource is a relative concept -- 150K is enough for IWSLT, but low-resource for WMT.	dummyTurk
1630	0.0	As a result, we take 10K from IWSLT for more controlled experiments.	dummyTurk
1631	0.0	--dropout	dummyTurk
1632	0.0	We followed Zoph et al.	dummyTurk
1633	0.0	(2016) to use a big dropout rate.	dummyTurk
1634	0.0	--greedy decoding in Figure-4	dummyTurk
1635	0.0	Greedy decoding score on dev set is reported during training.	dummyTurk
1636	0.0	We did not save a separate copy after each epoch, making it impossible to report full decoding results after training is done.	dummyTurk
1637	0.0	-----Reply to questions-----	dummyTurk
1638	0.0	--tokenization	dummyTurk
1639	0.0	We used multi-bleu.pl and tokenizer.pl from Moses.	dummyTurk
1640	0.0	--monolingual data	dummyTurk
1641	0.0	We believe it???s orthogonal to our investigation, and it???s useful to add them in the future.	dummyTurk
1642	0.0	--high resource	dummyTurk
1643	0.0	In our preliminary experiments, our method outperforms the baselines until about one-third of the full corpus (50K).	dummyTurk
1644	0.0	We hypothesize that the model can effectively learn a peaked attention distribution given enough data, thus eliminating the need for hard attention.	dummyTurk
1645	0.0	 Through the auxiliary task of dependency relation prediction, similar/identical relation enforces participating words close to each other, i.e.	dummyTurk
1646	0.0	the model clusters words with similar syntactic functionalities in different domains.	dummyTurk
1647	0.0	Regarding semantics, we take pre-trained word embeddings as input to the recursive neural network.	dummyTurk
1648	0.0	The word embeddings are obtained through word2vec which captures semantic information that produces similar representations for semantically-similar words.	dummyTurk
1649	0.0	By taking them as input, together with the auxiliary task, our model encodes both semantic and syntactic information.	dummyTurk
1650	0.0	 Yes, the word vectors mainly distinguish in syntactic structure and semantic relatedness, instead of sentiment polarity, because the focus of our model is cross-domain aspect/opinion terms extraction.	dummyTurk
1651	0.0	We appreciate your concern and will investigate the more challenging problem of transferring both target terms and their sentiment polarities in our future work.	dummyTurk
1652	0.0	 For each training sentence, we built a recursive-neural-network based on the dependency tree.	dummyTurk
1653	0.0	Then we obtained the hidden representation h_n for each node and relation feature r_{nm} for each path in the tree, according to Section 4.1.	dummyTurk
1654	0.0	For each path, we also obtained the ground-truth relation label y^R_{nm}.	dummyTurk
1655	0.0	The set of tuples (r_{nm}, y^R_{nm}) corresponds to the inputs and labels for D_R.	dummyTurk
1656	0.0	 All meta-parameters are selected based on 3-round random-split validation on terms extraction task in the source domain and relation task over the source and target domain.	dummyTurk
1657	0.0	Specifically, for source-domain validation data, we evaluate the model's performance on both terms extraction task and relation prediction task.	dummyTurk
1658	0.0	For target-domain validation data, we evaluate the performance on relation task.	dummyTurk
1659	0.0	We understand this may not be an optimal approach to tune meta-parameters, but how to set optimal parameters for cross-domain setting is still an open issue.	dummyTurk
1660	0.0	Thank you for thoughtful, clear and constructive review and comments; they will certainly help to focus our improvements.	dummyTurk
1661	0.0	Weakness 1	dummyTurk
1662	0.0	Agreed.	dummyTurk
1663	0.0	It is a problem in a lot of rapidly written papers these days ??? there is a lot of notation, and typically many assumptions about the role and meaning of variables (e.g., scalar, vector, ?).	dummyTurk
1664	0.0	We will improve clarity in the submitted version.	dummyTurk
1665	0.0	Weakness 2	dummyTurk
1666	0.0	Good observation; intuitive motivation is often sacrificed in dense short technical papers.	dummyTurk
1667	0.0	The main intuition behind using Fourier here is because embeddings in Recurrent nets, in general, do not encode only attributional similarities between words but also encode similarities between pairs of words (Mikolov et al, 2013),  i.e.,  linguistic regularities/ relational similarities.	dummyTurk
1668	0.0	With this intuition, our aim is to use the Fourier analysis  to capture the regularity on the embedding axis.	dummyTurk
1669	0.0	 We will convey this in the adjusted camera ready version.	dummyTurk
1670	0.0	Weakness 3	dummyTurk
1671	0.0	We believe that the percentage of text doesn???t influence the performance, in fact a percentage restriction would likely, negatively affect performance.	dummyTurk
1672	0.0	For example in Table 5 we showed that even selecting more than 80% of the text doesn???t yield the same result.	dummyTurk
1673	0.0	We considered choosing the top-n%, but the question would be how do we define n?	dummyTurk
1674	0.0	Do we really know the relevant terms?	dummyTurk
1675	0.0	; might there be a bias in the dataset?	dummyTurk
1676	0.0	And what are the assumption when defining n?	dummyTurk
1677	0.0	Overall, a percentage subset selection couldn???t possibly reflect the distribution of meaning ??? n% of the text does in no way represent n% of its meaning.	dummyTurk
1678	0.0	Note however, that the network sometimes makes prediction based on ???hard wired??? information in the input.	dummyTurk
1679	0.0	For instance, Robinson et al., 2017 trained a model to identify colon cancer from patients electronic records; although the model performed well, they discovered that the model was focusing only on the fact that patients with colon cancer were actually sent to a particular clinic, rather than consider clues from the health records; this is acknowledged in another paper (Brown et al, 2012).	dummyTurk
1680	0.0	In addition, assuming that there???s always a top n, we train a network using attention, and found that attention tends to select fewer words; but when feeding to the network as a subset (not a subsequence), we found that the performance drops dramatically.	dummyTurk
1681	0.0	For this case, we trained an LSTM network (using attention) on the US-finance compliance from Kaggle (this experiment is not reported in the paper), although attention selects fewer words, we found that when feeding the subset to the network the accuracy is <0.2, while our approach achieved an accuracy >0.6.	dummyTurk
1682	0.0	In a nutshell,  from this experiment, we observed that it???s not always the case that fewer words would make sense to the network.	dummyTurk
1683	0.0	Weakness 4	dummyTurk
1684	0.0	Sure, we will acknowledge that and compare and cite both.	dummyTurk
1685	0.0	The exact task details and the dataset composition for the three experiments we used are not exactly the same.	dummyTurk
1686	0.0	So, it was not entirely trivial to train model on one dataset and test on another.	dummyTurk
1687	0.0	We will look into this and try to add results in the revised version.	dummyTurk
1688	0.0	In this paper, we have followed the same experimental set-up as used by the baselines we compared our model with, where they used cross-validation but did not report cross-dataset results.	dummyTurk
1689	0.0	SVM_ADD and SVM_CC use sum and concatenation of the word vectors as the only features, respectively.	dummyTurk
1690	0.0	Similarly for RF_ADD and RF_CC as well.	dummyTurk
1691	0.0	Thanks for the suggestions.	dummyTurk
1692	0.0	We will take care of these points in the revised version.	dummyTurk
1693	0.0	Because of the space constraints, we had only mentioned the summary of comparison with Glove embeddings in the main paper, while the detailed results have been put in the supplementary.	dummyTurk
1694	0.0	We will utilize the extra space for the camera ready version to add these results in the main paper.	dummyTurk
1695	0.0	The results obtained by DT-embedding are always better or at par with the best results obtained using Glove embeddings.	dummyTurk
1696	0.0	We agree with the reviewer???s point that while Figure 1 shows a better separation of co-hyponyms from meronyms and hypernyms by DT-emb than Glove, it does not translate into a huge performance gain.	dummyTurk
1697	0.0	We will clarify this point in the revised version.	dummyTurk
1698	0.0	Thanks for the suggestions.	dummyTurk
1699	0.0	To answer this question, we performed McNemar's test for each dataset in each experiment to compare the best classifier models  for DT-Emb and GloVe as specified in supplementary material.	dummyTurk
1700	0.0	The p values we obtained are as follows-	dummyTurk
1701	0.0	Experiment 1 :  p value of 0.00146 <  0.05	dummyTurk
1702	0.0	Experiment 2: Co-Hyp vs Random: p value of 0.66439 > 0.05	dummyTurk
1703	0.0	Experiment 2: Co-Hyp vs Hyper: p value of 0 <  0.05	dummyTurk
1704	0.0	Experiment 3: Co-Hyp vs Random: p value of 0.0015 <  0.05	dummyTurk
1705	0.0	Experiment 3: Co-Hyp vs Mero: p value of 0.00016 <  0.05	dummyTurk
1706	0.0	Experiment 3: Co-Hyp vs Hyper: p value of 0.80259 > 0.05	dummyTurk
1707	0.0	Thus, in 4 out of 6 cases, we found the difference between the best models to be statistically significant.	dummyTurk
1708	0.0	We will update this in the final draft.	dummyTurk
1709	0.0	Thanks for your suggestions.	dummyTurk
1710	0.0	We will look into these examples and add in the revised version	dummyTurk
1711	0.0	The code for building distributional thesaurus from corpus is proposed by Riedl and Biemann (2013) as well as node2vec proposed by Grover and Leskovec (2016) are publicly available.	dummyTurk
1712	0.0	Other than that we have used Weka tool (publicly available) for classifier implementation.	dummyTurk
1713	0.0	We will make the code for implementation of vector operations publicly available, detailing all the dependencies and hyper-parameter settings for reproducibility of the results.	dummyTurk
1714	0.0	In our paper, ???Our models??? refer to the two neural network models which we use for the tasks that we tackle.	dummyTurk
1715	0.0	"Specifically, in Section 5, we wrote ""In this section, we describe the different models used in our experiments.???."	dummyTurk
1716	0.0	We did not claim that all models are novel or proposed by us.	dummyTurk
1717	0.0	"We mentioned in Section 5 that ""For this paper, we use a stacked bi-directional LSTM model for sequence labeling and an attentive bi-directional LSTM model to determine the relations between two text spans in a sentence.???."	dummyTurk
1718	0.0	It should be clear to a reader that the model in Section 5.2 is used for semantic relation extraction.	dummyTurk
1719	0.0	We would like to thank the Reviewer for the very informative feedback.	dummyTurk
1720	0.0	 The main result is that intrinsic metrics lack correlation with extrinsic ones.	dummyTurk
1721	0.0	We illustrate this using Figure 1	dummyTurk
1722	0.0	 The issue of translation disagreement exists in cross-language word embeddings evaluation, and we will particularly highlight our mention of this issue in our paper (in Section 3, Item 1	dummyTurk
1723	0.0	 We agree with the need to compare our findings to the previous ones more explicitly.	dummyTurk
1724	0.0	Despite we have surveyed the most close works in ???Related work??? section, we should probably add more discussion about our results in the context of existing knowledge	dummyTurk
1725	0.0	 We agree that verbs are truly highly relational, so they probably should not be used in cross-language benchmarks.	dummyTurk
1726	0.0	Their amount in ???classic??? word similarity datasets is not significant.	dummyTurk
1727	0.0	Datasets that consist only of verbs should be considered separately.	dummyTurk
1728	0.0	 We would like to draw reviewer???s attention to the heatmap (Figure 1)	dummyTurk
1729	0.0	We would like to thank the Reviewer for the very informative feedback.	dummyTurk
1730	0.0	 We agree with the Reviewer that this distinction is not absolutely correct.	dummyTurk
1731	0.0	The main point is to compare benchmarks, not models.	dummyTurk
1732	0.0	We tried to find an answer to ???could we robustly estimate extrinsic performance given some intrinsic metrics????.	dummyTurk
1733	0.0	 There are only a few papers on evaluation of cross-language word embeddings (and we have made our best citing all key papers in that particular field), but we also could add some papers from a broader field.	dummyTurk
1734	0.0	 We agree that the paper lacks some details, but we tried to keep the paper concise and focused.	dummyTurk
1735	0.0	Thanks to this comment, we know that more details have to be added.	dummyTurk
1736	0.0	 We promised to release the code and the data.	dummyTurk
1737	0.0	We decided to omit some technical details to make the paper concise and easier to read.	dummyTurk
1738	0.0	Nevertheless, this comment is reasonable and we will try to add more details into the paper.	dummyTurk
1739	0.0	 Figure 1 contains a heatmap and a dendrogram.	dummyTurk
1740	0.0	The heatmap depicts mutual correlations of models scores on different datasets.	dummyTurk
1741	0.0	Lighter colors correspond to stronger positive correlation.	dummyTurk
1742	0.0	Dendrogram shows how the datasets cluster together.	dummyTurk
1743	0.0	From this Figure one can see that models behave very differently on different benchmarks, but there are roughly 4 clusters.	dummyTurk
1744	0.0	  The limitations addressed by the Reviewer listed in Section 3; we will make a reference to this section more explicit.	dummyTurk
1745	0.0	  We think that humans estimate similarity of words in different languages via similarity of concepts these words denote, and many words may correspond to multiple concepts (polysemy).	dummyTurk
1746	0.0	This may highly affect human assessments for different language pairs.	dummyTurk
1747	0.0	Even within one culture this effect takes place (e.g.	dummyTurk
1748	0.0	Russian word ???paradnaya??? denotes ???hall??? in Saint-Petersburg and ???dress??? in Moscow).	dummyTurk
1749	0.0	We should probably rewrite that statement in the paper.	dummyTurk
1750	0.0	 There are papers which state that (mono-lingual) human judgments of word similarity tend to be dependent of a background knowledge of an assessor	dummyTurk
1751	0.0	 We used dictionaries from (Conneau, 2017), which contain only one possible translation for each word (we omitted multi-sense words).	dummyTurk
1752	0.0	 English-Russian embeddings were trained on a comparable corpora of 16B/5B tokens, but parallel datasets are much smaller (the concatenation of EN-RU corpora available at http	dummyTurk
1753	0.0	  No, there was only one translation for each sentence.	dummyTurk
1754	0.0	Translations were produced manually by human translators which were instructed to paraphrase their translations.	dummyTurk
1755	0.0	We kindly appreciate your constructive feedback and suggesting improvements for the work.	dummyTurk
1756	0.0	On paper layout.	dummyTurk
1757	0.0	This paper is aimed at an audience that is a bit familiar with word embeddings, with the definition of a probabilistic model that is from an	dummyTurk
1758	0.0	exponential family, and with the challenges that are posed by the normalisation of such probabilistic models.	dummyTurk
1759	0.0	Therefore we refrained	dummyTurk
1760	0.0	ourselves to explain what a exponential family word vector space model is.	dummyTurk
1761	0.0	However, given more space, we aim to explain band construction more throughfully.	dummyTurk
1762	0.0	On more tasks.	dummyTurk
1763	0.0	We believe the vital justification point for this sampler was the demonstration of what it promises: faster learning and the lower variance (thus more guaranteed) sampling.	dummyTurk
1764	0.0	We think that is shown by the experiments.	dummyTurk
1765	0.0	We can show it also on other datasets, and alternative performance measures, but we don???t expect a dramatic change in the conclusions of the sampler	dummyTurk
1766	0.0	We agree that the explanation of the band construction is a bit short, and we	dummyTurk
1767	0.0	will extend it in the final version of the paper (considering the page limit).	dummyTurk
1768	0.0	We can use a larger learning rate and batch size to train sLSTM.	dummyTurk
1769	0.0	So, we can process more examples per second for sLSTM.	dummyTurk
1770	0.0	We have not done a thorough comparison in terms of training convergence.	dummyTurk
1771	0.0	We will include bits-per-character results should the paper be accepted.	dummyTurk
1772	0.0	- use as initial model to train regular RNN (as done in the paper).	dummyTurk
1773	0.0	- convert grammar/syntactic rules into RNN with learnable parameters (which can later be adjusted or adapted).	dummyTurk
1774	0.0	- provides an alternative way to combine FSM and RNN LMs.	dummyTurk
1775	0.0	- compress a finite-state model into a smaller RNN model.	dummyTurk
1776	0.0	We are using in-house datasets (anonymized to conform to double-blind review guideline).	dummyTurk
1777	0.0	More details can be included should the paper be accepted.	dummyTurk
1778	0.0	We did not compare to other state-of-the-art models.	dummyTurk
1779	0.0	Our dataset comes from a recently released dataset, which contains questions, documents and coarse-grain annotations on whether or not an entire assertion is a correct answer.	dummyTurk
1780	0.0	However, these coarse-grain annotations could not support explicit reasoning and deep question understanding.	dummyTurk
1781	0.0	To address these issues, we make a huge effort to do fine-grained annotations, resulting in a relatively large dataset, which we plan to release to the community.	dummyTurk
1782	0.0	In our preliminary experiment, we use a rule-based approach to detect anchor and measure the coverage of correct answers through 1-hop and 2-hop paths.	dummyTurk
1783	0.0	We observe that the coverage of 1-hop candidates is 55.6% and that of 2-hop candidates is 69.6%.	dummyTurk
1784	0.0	This indicates that deep/multi-hop inference is required in this dataset.	dummyTurk
1785	0.0	The input of our task includes a question, a document and assertions automatically extracted from the document.	dummyTurk
1786	0.0	The output is a subject or an object from the assertions which could correctly answer the question.	dummyTurk
1787	0.0	The task could be viewed as a ranking problem, in which candidate answers come from subjects and objects of the assertions.	dummyTurk
1788	0.0	Please kindly refer to line 116-124 for a formal definition.	dummyTurk
1789	0.0	Thus, we drop the triples that are not correctly extracted.	dummyTurk
1790	0.0	Assertions with pronouns are saved to call for multi-hop reasoning.	dummyTurk
1791	0.0	We will give more details about our implementation of Borders???s approach.	dummyTurk
1792	0.0	We plan to run a strong RCQA system as an additional baseline on our dataset.	dummyTurk
1793	0.0	 We recheck the paper of Zeng et al.	dummyTurk
1794	0.0	(2018) carefully.	dummyTurk
1795	0.0	Zeng et al.	dummyTurk
1796	0.0	(2018) only regard the extracted relation of one sentence with the highest probability as the relation of bag, and use the result of one sentence to provide reward for the selection, while not considering the result of the other sentences.	dummyTurk
1797	0.0	Thus, we think that they only use the mostly positive sentences.	dummyTurk
1798	0.0	We will revise the statements in the next version to make it clear.	dummyTurk
1799	0.0	 Our claim is that the selector can pick out the positive sentences for a query relation (r), and we use cross-entropy to optimize the POS-based module for better selection.	dummyTurk
1800	0.0	If using the cross-entropy loss in the Bag-level module of section 3.3.3, it will increase the probability of the given relation r_B of bag B, and reduce the probability of other relation types to a great extent.	dummyTurk
1801	0.0	The optimization of POS-based module will be inhibited and disturbed.	dummyTurk
1802	0.0	Meanwhile, in Bag-level module of section 3.3.3, the probability of each relation type is coming from different selection processes.	dummyTurk
1803	0.0	Thus, our idea is to rank positive class higher than negative ones in bag level, in the case of ensuring their independence.	dummyTurk
1804	0.0	The ranking loss method fits very well to our situation.	dummyTurk
1805	0.0	 When testing, we use the RL agent to distinguish the positive and negative sentences in a bag for each relation type (r), and use the positive and negative sets to get the probability of relation type (r) that corresponds to the bag.	dummyTurk
1806	0.0	Thus, there are 53 selection processes of each sentence bag.	dummyTurk
1807	0.0	 For NA class, we select the sentences which do not belong to other relation types as the positive bag, and regard the sentences which belong to other relation types as the negative bag.	dummyTurk
1808	0.0	 Thank you for your suggestion.	dummyTurk
1809	0.0	And we will carefully correct the typos and grammatical errors.	dummyTurk
1810	0.0	 The terminal state is the state of the last sentence of each bag.	dummyTurk
1811	0.0	Since before the selection of each bag is finished, we don???t know if the selection process is good or not.	dummyTurk
1812	0.0	So the reward of other sentences is set to zero.	dummyTurk
1813	0.0	Once the selection is finished, we can predict the relation of sentence bag and provide the delayed reward.	dummyTurk
1814	0.0	 For each training episode, we randomly disrupt the order of sentences in a bag.	dummyTurk
1815	0.0	And we expect that whatever the order of a sentence in a bag is, the agent can make the right decision, so we set the parameter \gamma to 1, which can ensure that the order of sentences in bag should not influence the results.	dummyTurk
1816	0.0	And the global reward of each state will be same.	dummyTurk
1817	0.0	 Thank you for your suggestion.	dummyTurk
1818	0.0	Because the decision-making process takes more time, our model needs more training time than the baseline models in an episode.	dummyTurk
1819	0.0	And we will make a more detailed comparison in the next version.	dummyTurk
1820	0.0	 In our task, the order of sentences in bag should not influence the predicated result, so the parameter \gamma is set to 1, and will not be tuned.	dummyTurk
1821	0.0	And we found that the effect is better when \alpha and (1-\alpha) are close, and the gap is not big, so set \alpha to 0.5.	dummyTurk
1822	0.0	We will provide more detailed description in the next version.	dummyTurk
1823	0.0	Thank you!	dummyTurk
1824	0.0	Weakness 1	dummyTurk
1825	0.0	We considered extra-linguistic information to comprise both communicative aspects of non-linguistic information (e.g., dynamics of prosody, voice quality, etc.)	dummyTurk
1826	0.0	as well as habitual characteristics of the speaker (e.g., gender, habitual voice, etc.)	dummyTurk
1827	0.0	and more coincidental aspects (e.g.	dummyTurk
1828	0.0	coughing).	dummyTurk
1829	0.0	We agree to conform to a more precise and well established definition as in Laver (1994), referring to the added side information as paralinguistic and extra-linguistic information.	dummyTurk
1830	0.0	Weakness 2	dummyTurk
1831	0.0	We shall cite the suggested paper which introduces GeMAPS features.	dummyTurk
1832	0.0	However, these features already overlap to a large extent with the IS'11 feature set we use (features implemented in OpenSMILE such as pitch; jitter; shimmer; loudness; spectral slope, along with temporal functionals), which we believe would not significantly alter the experimental results.	dummyTurk
1833	0.0	Further, the GeMAPS extended set has MFCCs which are omitted to filter out verbal features.	dummyTurk
1834	0.0	Building syntax requires reasoning about context.	dummyTurk
1835	0.0	The LSTM is gathering syntactic information by taking advantage of the context information given to it through the recurrent process at different depths.	dummyTurk
1836	0.0	But, it is a good point that we would like to rule out other simpler types of context providing most of the gain.	dummyTurk
1837	0.0	We ran the n-gram experiment suggested with the following results: on POS we get an accuracy of 74%; on the parent constituent task, 73%; on the grandparent constituent task, 61%; and on the great-grandparent constituent task, 47%.	dummyTurk
1838	0.0	These results are lower than those from the representations learned by the LSTMS from layers 1 and deeper, which generally perform 10-20% better.	dummyTurk
1839	0.0	The only learned representation that does not outperform this baseline is Layer 1 of the LM on the great-grandparent prediction task, which achieves around the same performance.	dummyTurk
1840	0.0	No, we did not try this, but it would be a good analysis to run with additional time.	dummyTurk
1841	0.0	Yes.	dummyTurk
1842	0.0	For each word, we used its GloVe embedding to predict the word's corresponding POS, parent, grandparent, and great-grandparent labels as a baseline.	dummyTurk
1843	0.0	Note that we actually use most frequent tag (MFT) baseline in our figures and other results because it outperforms GloVe in all cases.	dummyTurk
1844	0.0	It corresponds to the (word level) input embeddings specific each model.	dummyTurk
1845	0.0	These are word embeddings updated during the training of their respective model.	dummyTurk
1846	0.0	Thank you for catching this discrepancy; we will move the star to Layer 0 in the figure to reflect this exception.	dummyTurk
1847	0.0	Thank you for your review!	dummyTurk
1848	0.0	We agree that n-gram novelty does not accurately reflect how abstractive a summary is.	dummyTurk
1849	0.0	Based on our observation that SOTA models (evaluated by ROUGE and METEOR) tend to produce extractive summaries, we wanted to use n-gram novelty to show that our model has the ability to generate more novel text that is not from the input.	dummyTurk
1850	0.0	Furthermore, it would be really difficult to measure high-level abstractive concepts automatically.	dummyTurk
1851	0.0	We plan to design human evaluation in future work to measure this aspect.	dummyTurk
1852	0.0	Though our novel decoder encourages the generation of summaries that cover diverse semantic roles, in future work, we will design an explicit mechanism for removing redundancy.	dummyTurk
1853	0.0	We will also compare with redundancy-handling systems like MMR in our revision.	dummyTurk
1854	0.0	We would like to thank the reviewer for the feedback and time taken to provide it.	dummyTurk
1855	0.0	The datasets will be released upon acceptance and required information about them will be included in camera-ready version.	dummyTurk
1856	0.0	Thanks for your most useful questions; as weakness 3, Q1 and Q3 demand  lengthy answers that do not fit a 1000 word limit, the file with detailed answers [named: ???answers???] is available here:	dummyTurk
1857	0.0	https://drive.google.com/drive/folders/1CrZ2FeTGD7stLmRpE_RwsbjnWhEH2aFh?usp=sharing	dummyTurk
1858	0.0	Full confusion matrices, experiments grid, booster and negator lists are also there.	dummyTurk
1859	0.0	We suggest to go directly there instead of reading abridged answers.	dummyTurk
1860	0.0	Weakness3: Agreement score for SA	dummyTurk
1861	0.0	Short answer: 0.4 for words; 0.437 for texts on presence of the negative sentiment; 0.375 for texts on the presence of positive sentiment.	dummyTurk
1862	0.0	The problem of low agreement is addressed in the long answer.	dummyTurk
1863	0.0	Question1: FScore50%, accuracy competitive to random: elaborate on improvements.	dummyTurk
1864	0.0	As you ask, full confusion matrices and quality metrics are provided oat the above-given link, together with the detailed answer.	dummyTurk
1865	0.0	Short answer: there are three ways to calculate random baselines, one is very unfair for collections with unbalanced classe sizes, but compared to the other two, both lexicons perform visibly better, while PolSentiLex beats RuSentiLex is some tasks, but not in others.	dummyTurk
1866	0.0	The overall modest quality of all approaches for Russian, including those introduced in this paper and elsewhere, is a subject for further research.	dummyTurk
1867	0.0	Question2: SA features for ML.	dummyTurk
1868	0.0	Those were the unigrams that comprise one of the two tested lexicons (see page 4, section 4, paragraph 1).	dummyTurk
1869	0.0	Here we followed the procedure previously used in other experiments with the Russian language to make the results comparable.	dummyTurk
1870	0.0	Question3: held out data set.	dummyTurk
1871	0.0	A detailed description is available at the provided above link, it will be inserted in the final version.	dummyTurk
1872	0.0	We thank the reviewer for their insightful suggestions.	dummyTurk
1873	0.0	We agree that providing appropriate motivation for modeling choices is extremely important.	dummyTurk
1874	0.0	We???ll do better in future revisions, making sure to allocate sufficient space to motivation of design choices.	dummyTurk
1875	0.0	For example, initial experiments showed that conjunctions of various basic properties of the game state where extremely important for accurately predicting text descriptions.	dummyTurk
1876	0.0	Rather than enumerating higher-order conjunctions explicitly, our feature-encoding architecture is designed to learn conjunctions as needed.	dummyTurk
1877	0.0	Here, we wanted an architecture that was capable of reading in all the basic features and potentially merging information from each.	dummyTurk
1878	0.0	Recurrent architectures have been successful at doing this for ordered text -- we tried using the same architecture here even though our features don???t have a meaningful ordering.	dummyTurk
1879	0.0	We???ll surely include this type of motivation in future drafts.	dummyTurk
1880	0.0	We thank the reviewer for their helpful suggestions.	dummyTurk
1881	0.0	We included information about vocab-size and avg.comment length in Appendix A due to space limitations, but will include it in the paper-body in future versions.	dummyTurk
1882	0.0	The comment about syntactic diversity is interesting -- we???ll explore this and include our findings.	dummyTurk
1883	0.0	On ordering	dummyTurk
1884	0.0	The canonical order used was (1) move, (2) threat, and (3) score features.	dummyTurk
1885	0.0	Within each type, there were multiple features.	dummyTurk
1886	0.0	Their orderings is also arbitrary -- we???ll add specific details to the appendix in future revisions for reproducibility.	dummyTurk
1887	0.0	The embeddings are 192-dimensional.	dummyTurk
1888	0.0	We tried pretrained embeddings from Glove [Pennington, 2014], but that didn???t help.	dummyTurk
1889	0.0	The model has one hidden layer.	dummyTurk
1890	0.0	Both encoder and decoder LSTM cell-sizes are 384.	dummyTurk
1891	0.0	We restricted annotators to be from Anglophone countries.	dummyTurk
1892	0.0	We???ll add these details to future drafts.	dummyTurk
1893	0.0	Thanks for catching this!	dummyTurk
1894	0.0	For both Q1 and Q2, we find that GT, GAC(MT), GAC(MTS) and TEMP differ from an equal distribution significantly using chi-squared test.	dummyTurk
1895	0.0	For Fluency, we also perform a two-sided WS test on each pair of methods for each of the results to check whether their differences are significant.	dummyTurk
1896	0.0	For p<0.05 confidence, following differences are statistically significant.	dummyTurk
1897	0.0	TEMP & GAC-sparse, TEMP & GAC(MT), GAC(M) & GAC(MTS), TEMP & GT, TEMP & NN, GAC(MT) & GT, GAC(MT) & NN, GAC(MTS) & GAC-sparse, NN & GAC-sparse, GAC(M) & NN, TEMP & GAC(M), GAC(MT) & GAC-sparse, GAC(MTS) & NN, GAC(MTS) & GT	dummyTurk
1898	0.0	"""The present paper is not...""  Thank you for the suggestion."	dummyTurk
1899	0.0	We will add the analysis of this paper in our final version.	dummyTurk
1900	0.0	"""in the abstract,..."" Sorry for the confusion."	dummyTurk
1901	0.0	Here we means the query and its multiple response are not aligned, and thus forming a 1-to-n relashionship.	dummyTurk
1902	0.0	"""I'm not sure it is..."" We agree that the word ""alignment"" used here may be confusing."	dummyTurk
1903	0.0	We will take this suggestion and revise our description about this point.	dummyTurk
1904	0.0	"""the lines 108-110 are..."" Here, we means that in NMT, the multiple references are with the same meaning with some synonyms used, thus the multiple references can be treated as identical."	dummyTurk
1905	0.0	Therefore, it can be seen as the input and its output still forms a 1-to-1 relationship.	dummyTurk
1906	0.0	"""you might also want..."" Thank you for the suggestion."	dummyTurk
1907	0.0	We will cite the paper and add some comparisons in our final version.	dummyTurk
1908	0.0	 We thank the reviewer for his/her remarks.	dummyTurk
1909	0.0	We report multiple results for this paper, thus a long paper was more suitable.	dummyTurk
1910	0.0	We tried to fully explain the background and insisted on giving concrete examples on how to use the SRL coverage and MEANT.	dummyTurk
1911	0.0	A short paper would have been too short for all that.	dummyTurk
1912	0.0	 We forgot to add what SRL system we used, that was a mistake from our side, but we corrected that.	dummyTurk
1913	0.0	We are using ASSERT (Pradhan et al, 2004).	dummyTurk
1914	0.0	We also tried running other SRL systems such MATE and MATEPLUS.	dummyTurk
1915	0.0	"However, all the SRL parses we tried are trained on an old PropBank version which did not contain the frame ``to be""."	dummyTurk
1916	0.0	We could not retrain these SRL systems again since the training component is not publically available.	dummyTurk
1917	0.0	 The data is from the DARPA LORELEI program; thanks to the reviewer, we added that to the paper.	dummyTurk
1918	0.0	 After the submission of the paper, we realized that we made a small mistake.	dummyTurk
1919	0.0	Equation 1 should have been the number of labels/number of labeled words.	dummyTurk
1920	0.0	But to answer the reviewer's question, we thought that it was weird at first; we run Pearson correlation on the weights and we found that both coverages correlate positively 90% of the time.	dummyTurk
1921	0.0	Which explain why they had a similar impact on the translation.	dummyTurk
1922	0.0	We thank the reviewer for the feedback.	dummyTurk
1923	0.0	We added a thorough description of the SRL based evaluation metric MEANT in the paper.	dummyTurk
1924	0.0	Similarly to our approach, many works in SMT are not self-contained since they use tools like MOSES or LAMTRAM.	dummyTurk
1925	0.0	However, we propose a novel idea in this paper and show that by adopting the semantic evaluation MEANT to improve ITG training, we consistently improve the translation quality for multiple low resource languages.	dummyTurk
1926	0.0	We are using MEANT to initialize the outside probability in the inside-outside algorithm during the ITG training.	dummyTurk
1927	0.0	After that, we use Viterbi to extract the word alignment.	dummyTurk
1928	0.0	The intuition is to use MEANT to assess how good a training instance is by computing the SRL coverage of the sentence.	dummyTurk
1929	0.0	 We are using BITGs for word alignment.	dummyTurk
1930	0.0	Once we have the BITG alignment and the SRL BITG alignment, we plug the alignments into MOSES and compare the alignments to GIZA++ alignment.	dummyTurk
1931	0.0	The work of Saers et al, (2009) showed that using ITG alignments outperforms GIZA++ alignments.	dummyTurk
1932	0.0	In this paper, we use the same fashion of contrasting different alignment systems by relying on their impact on the translation quality.	dummyTurk
1933	0.0	 In the title, we try to reflect the main point of the paper which is doing semantic SMT for low resource languages even though no SRL parsers are available for these languages.	dummyTurk
1934	0.0	We specifically show that what makes the word alignment better is using MEANT (which heavily relies on SRL) to assess the training data.	dummyTurk
1935	0.0	We report the positive impact this has on improving translation for challenging low resource languages.	dummyTurk
1936	0.0	The take-home message we want to address is that we can inject semantics to translate low resource languages even though no SRL is available for languages like Tigrinya, Oromo, and Amharic.	dummyTurk
1937	0.0	 We are using MEANT to compute the SRL coverage in the English side of the data.	dummyTurk
1938	0.0	We compute the MEANT score between a sentence and itself.	dummyTurk
1939	0.0	We added a step-by-step description of 2 examples from Figure 1.	dummyTurk
1940	0.0	More precisely, for MEANT, if there is no structure (SRL), there is no score.	dummyTurk
1941	0.0	"If there is a partial SRL on a certain sentence like in ""[ARG0 I] [Target like] playing games."	dummyTurk
1942	0.0	""", only the SRLed parts influence the score."	dummyTurk
1943	0.0	 We added an alignment example comparing GIZA++, BITG and MEANT based BITG system	dummyTurk
1944	0.0	 The SRL coverage in figure 1 is a heuristic that we introduce in section 4 which represents many words have been SRLed.	dummyTurk
1945	0.0	"The equation contains a small typo, what it should be is (the number of labels/number of words labeled) not ""number of words""."	dummyTurk
1946	0.0	MEANT is a more efficient way to compute the SRL coverage of an English sentence.	dummyTurk
1947	0.0	We added an example where we show how was the MEANT score obtained for these examples step by step.	dummyTurk
1948	0.0	The same for the SRL coverage.	dummyTurk
1949	0.0	We greatly appreciate the helpful comments made by the reviewers regarding this paper.	dummyTurk
1950	0.0	Without your comments, we may take a mistake.	dummyTurk
1951	0.0	We will revise the introduction according to your suggestion.	dummyTurk
1952	0.0	If we have time enough for additional experiments, we will try to perform experiments on English language.	dummyTurk
1953	0.0	In many cases, Korean speakers can understand sentences without functional words because of selectional restriction of main verb.	dummyTurk
1954	0.0	So, we think that recall of content words is more important.	dummyTurk
1955	0.0	Without your comment, we may take a mistake.	dummyTurk
1956	0.0	We will revise it.	dummyTurk
1957	0.0	Thank you for your suggestion.	dummyTurk
1958	0.0	We will upload our system to Github.	dummyTurk
1959	0.0	The focus of this work is to model the persuasion strategy used in each sentence in Kiva???s avocation requests.	dummyTurk
1960	0.0	We expect our work to be the first step to deeply understand the persuasion process, and future research can build on our work to understand the correlations between the presence of different strategies and persuasion outcomes, as well as the ordering of such persuasion strategies.	dummyTurk
1961	0.0	There are 40629 messages (documents), and we received valid annotation for 504 documents, among which 291 were in the training corpus, 94 documents are in the development set, and 119 documents are in the test set (291+94+119 = 504, 504+40215=40629).	dummyTurk
1962	0.0	In the hierarchical semi-supervised model (as shown in Figure 2), the input to the document level LSTM encoder is the prediction result of sentence types, as specified in Section 2.2.3.	dummyTurk
1963	0.0	Documents and messages are used interchangeably, and we will keep notation consistent in the revised version.	dummyTurk
1964	0.0	Thanks for your fruitful and helpful comments and suggestions, which are very important to refine the paper.	dummyTurk
1965	0.0	 We agree that comparison with other related works are needed to make this paper more convincing.	dummyTurk
1966	0.0	The related works, such as Bengio et al.	dummyTurk
1967	0.0	2015 and MRT methods, will be discussed and compared in the refined version, including the differences you mentioned (all the points are correct).	dummyTurk
1968	0.0	Besides, we think our method is effective and simple to implement, and it can be directly applied to other sequence-to-sequence tasks.	dummyTurk
1969	0.0	For joint training procedure, we found 3 iterations are enough to reach convergence, and this detail is shown in the section of Implementation Details.	dummyTurk
1970	0.0	More analysis about joint training procedure will be added in the paper, such as the BLEU changes with the increase of iterations.	dummyTurk
1971	0.0	 We show the effectiveness of our method by comparing ???Transformer-Big+BT??? and ???Transformer-Big+BT+RT???.	dummyTurk
1972	0.0	We will add comparison results with ???Transformer-big+BT+JS??? and ???Transformer-big+RT??? to make it more sufficient in the camera-ready version.	dummyTurk
1973	0.0	 Good point!	dummyTurk
1974	0.0	In this paper, we compare our model with the vanilla encoder model directly learned without the weighting process.	dummyTurk
1975	0.0	Both Table 1 and Figure 5 show that the weighting policy significantly improves the performance.	dummyTurk
1976	0.0	Another important test could be comparing the proposed weighting policy and a simple uniform weighting policy.	dummyTurk
1977	0.0	We did perform this test, and mentioned in line 268 that ''Combining the definition embeddings with equal weighting demonstrate poor results''.	dummyTurk
1978	0.0	"The performance yielded by the uniform weighting policy is ""acc@1=0.11, acc@10=0.33, acc@100=0.56, MedRank=54"", which is poor, as one can expect."	dummyTurk
1979	0.0	We can certainly add this back in the revised version if that helps demonstrate the effectiveness of the proposed method.	dummyTurk
1980	0.0	 We thought about similar choices for the experiment section.	dummyTurk
1981	0.0	We aimed to evaluate our method from both intrinsic and extrinsic tasks.	dummyTurk
1982	0.0	The reverse dictionary task is an intrinsic evaluation to show the high quality of the learned definition encoder, so we decided to add a more general extrinsic task, like LM.	dummyTurk
1983	0.0	Identifying definitions in corpora is an interesting problem, which can be naturally combined with our model to develop a smart sense learning agent, as we mentioned in the conclusion section.	dummyTurk
1984	0.0	As for evaluation on other tasks, we plan to apply our method on applications like taxonomy construction as you mentioned, and knowledge representation in the future.	dummyTurk
1985	0.0	Many interesting tasks shall benefit from a high-quality definition encoding model.	dummyTurk
1986	0.0	 We appreciate the valuable suggestions you gave.	dummyTurk
1987	0.0	These papers use lexical resources as external constraints for word representation.	dummyTurk
1988	0.0	Since our major goal is to learn the embedding of a word directly from its definition, we did not compare with these methods.	dummyTurk
1989	0.0	Still, we agree that they are very relevant in the sense of word representation learning.	dummyTurk
1990	0.0	We will add them to the related work section of the revised version.	dummyTurk
1991	0.0	Thank you for pointing out!	dummyTurk
1992	0.0	 Good suggestion!	dummyTurk
1993	0.0	The former problem is  due to the ???multi-sense??? nature of words.	dummyTurk
1994	0.0	In downstream applications, this problem can be alleviated by simply adding the dimensions of word embeddings (Li et al.	dummyTurk
1995	0.0	2015).	dummyTurk
1996	0.0	However, when used as supervision for definition embedding, ???multi-sense embeddings??? can severely harm the performance of the encoding model.	dummyTurk
1997	0.0	This is the major motivation, and also the focus of this work.	dummyTurk
1998	0.0	The ???multi-source??? problem is also an important aspect, but is not the major problem we would like to solve here.	dummyTurk
1999	0.0	It is an inspiring comment though, that the encoder-blender model may benefit from multi-source redundancy.	dummyTurk
2000	0.0	We will leave this part as an interesting future work.	dummyTurk
2001	0.0	We appreciate all the invaluable comments and will take them all into account.	dummyTurk
2002	0.0	The utterances consists of both semi-automatically annotated utterances and human annotated utterances.	dummyTurk
2003	0.0	We will elaborate the annotation process in the final version.	dummyTurk
2004	0.0	The average utterance length is about 5.	dummyTurk
2005	0.0	All the utterances are speech recognized ones.	dummyTurk
2006	0.0	We cannot disclose the ASR accuracy because of legal reasons.	dummyTurk
2007	0.0	We can say that the ASR performance is sufficiently high to be used in commercial spoken language systems.	dummyTurk
2008	0.0	We also observed that some ASR errors can be resolved by using character embeddings, but we are not covering ASR related issues as it's another direction of the work.	dummyTurk
2009	0.0	"All the utterances are ""natural"" since we removed predefined invocation patterns from the utterances."	dummyTurk
2010	0.0	We set the maximum number and the minimum number of utterances for each domain so that the domain distributions are not too much skewed.	dummyTurk
2011	0.0	With one more allowed pages, we will further elaborate the data we used as much as possible including more detailed answers to the reviewers' questions.	dummyTurk
2012	0.0	(Please understand that we cannot disclose all the details of the real user data because of legal issues.)	dummyTurk
2013	0.0	In Amazon Alexa, there are more than 25,000 skills developed by external skill developers [Kumar et al., 2017].	dummyTurk
2014	0.0	We extracted 1,500 most frequently used skills in the US.	dummyTurk
2015	0.0	We propose an RNN classifier for proofreading systems which	dummyTurk
2016	0.0	(1) are automatically updated based on actual documents or articles,	dummyTurk
2017	0.0	(2) can be developed or customized for any user group, and	dummyTurk
2018	0.0	(3) can learn and treat subtle and complicated local rules.	dummyTurk
2019	0.0	We think one of the simplest approach is to cut the articles into short strings and classify them by RNN.	dummyTurk
2020	0.0	This approach is inspired from how human proofreaders detect errors intuitively.	dummyTurk
2021	0.0	We could not find related works that tried to classify short strings for proofreading.	dummyTurk
2022	0.0	We will do an additional literature survey.	dummyTurk
2023	0.0	We will test our approach with a publicly available data such as Japanese Wikipedia articles.	dummyTurk
2024	0.0	Please note that we are sharing our news articles to some of the universities and research institutions in Japan.	dummyTurk
2025	0.0	The RNN classifier can treat some common phrases such as	dummyTurk
2026	0.0	"""the stock index rises due to the weaker yen""."	dummyTurk
2027	0.0	"If we change the word ""weaker"" to ""stronger"", it is classified as an error."	dummyTurk
2028	0.0	That is, the RNN behaves as if it understands the meaning of phrases.	dummyTurk
2029	0.0	Yes, and we found that RNNs trained with single-error samples show good performance on combined-error samples either.	dummyTurk
2030	0.0	In an article with 555 characters, there are 49 numbers, 5 alphabets and 1 special symbol.	dummyTurk
2031	0.0	All other characters are included in our training and test data.	dummyTurk
2032	0.0	For example, in section 5.4, we randomly choose 2 different noises and apply them separately on each correct sample.	dummyTurk
2033	0.0	Therefore, the class distributions are correct : incorrect = 1:2 for both training and test data.	dummyTurk
2034	0.0	(1) The vocabulary is relatively small considering the training data size.	dummyTurk
2035	0.0	(2) New person/company names do not appear frequently.	dummyTurk
2036	0.0	A short string does not contain enough information for classification, while it is difficult for RNNs to deal with the information contained in a long string.	dummyTurk
2037	0.0	Therefore we expect there should be a peak of performance for a certain string length.	dummyTurk
2038	0.0	We assume that human proofreaders would also show the best performance for 11 or 13 characters.	dummyTurk
2039	0.0	We only mention the best and worst results obtained in the WMT shared task for the 3 selected test sets on the ranking task (footnote 7) as an additional indicative reference of the ranking of our systems relative to the worst and best systems.	dummyTurk
2040	0.0	The details of the best and worst approaches are published results available in the shared task findings papers and since their role was not central to our claims but only indicative, we focused on describing the more relevant details of the approach and experiments in the allocated space.	dummyTurk
2041	0.0	We acknowledge that there is no direct comparison to the method by Popovic (2012) on the en-es WMT 2012 test set.	dummyTurk
2042	0.0	We agree that adding this would be a plus in terms of comparison, but note that our main claim centres on our unsupervised approach outperforming the strong supervised baseline, a result which, to the best of our knowledge, no other unsupervised method, including Popovic (2012), has achieved.	dummyTurk
2043	0.0	We focused our comparison with other approaches on the better performing methods between the baseline and the supervised approaches, which happens to be the supervised baseline.	dummyTurk
2044	0.0	We believe that this comparison was more relevant for this short paper given its major claim of describing an unsupervised approach which outperforms supervised baselines.	dummyTurk
2045	0.0	We are not sure we understood this argument.	dummyTurk
2046	0.0	"The way it is formulated (""*a* supervised method""), it is incorrect, as we describe three scenarios where our method outperforms the supervised baselines."	dummyTurk
2047	0.0	If the argument was that there is no scenario provided where our method outperforms the *best* supervised methods, that would be correct but also unrelated to any of our claims.	dummyTurk
2048	0.0	 This is explained in footnote 7 of the paper.	dummyTurk
2049	0.0	"W1:""novelty_aspect_of_the_submission"""	dummyTurk
2050	0.0	We formulate the problem as multiview learning using two	dummyTurk
2051	0.0	meta-views: language and representation.	dummyTurk
2052	0.0	This is novel in entity typing, and to the best of our knowledge,	dummyTurk
2053	0.0	it is not used in relation extraction (which is a related task) either.	dummyTurk
2054	0.0	"Also, using attention to combine multiple ""entity"" representations,"	dummyTurk
2055	0.0	as well as improving singleview entity typing models is novel.	dummyTurk
2056	0.0	Also, we see our dataset as a good	dummyTurk
2057	0.0	evaluation for single/cross/multi lingual word embeddings.	dummyTurk
2058	0.0	We will discuss that in the possible final version.	dummyTurk
2059	0.0	"W2:""Related_work_does_not_tease_apart_the_salient_features_of_the_present_work"""	dummyTurk
2060	0.0	We agree that the related work was too short, we will expand it to make our	dummyTurk
2061	0.0	contributions/features more clear.	dummyTurk
2062	0.0	"Q1:""language_specific_matrices?"""	dummyTurk
2063	0.0	"This should have been ""view specific matrices""."	dummyTurk
2064	0.0	Each view is presumably in different vector space and therefore we first need	dummyTurk
2065	0.0	to transform them into a shared space, before applying ATT, MAX or AVG.	dummyTurk
2066	0.0	These view matrices are the transformation matrices which are trained like other model	dummyTurk
2067	0.0	parameters.	dummyTurk
2068	0.0	"Q2:""...present_technique...superior_to...other_techniques?"""	dummyTurk
2069	0.0	Attention-based multiview learning is very recent and just used in Qu et al.	dummyTurk
2070	0.0	(2017), based on	dummyTurk
2071	0.0	our knowledge, and for a different task (node representation).	dummyTurk
2072	0.0	Based on Qu et al., attention outperformed multiple strong baselines,	dummyTurk
2073	0.0	since it can use complementary	dummyTurk
2074	0.0	information of views and also weights them based on their importance.	dummyTurk
2075	0.0	Our attention model is similar, and therefore it also holds these	dummyTurk
2076	0.0	features and could beat baselines.	dummyTurk
2077	0.0	So it is a competent approach for multiview learning.	dummyTurk
2078	0.0	Text classification is different from entity typing, with longer history of work	dummyTurk
2079	0.0	in multiview learning.	dummyTurk
2080	0.0	However, our multi-view modeling is novel in entity typing.	dummyTurk
2081	0.0	Even in	dummyTurk
2082	0.0	text classification, we are not aware of any work with multiple languages	dummyTurk
2083	0.0	and multiple levels of representations, so even with respect	dummyTurk
2084	0.0	to text classification our method is new.	dummyTurk
2085	0.0	In addition to the ROUGE evaluation, we also conducted human evaluation on 20 random samples from CNN/DailyMail test set.	dummyTurk
2086	0.0	Three participants were asked to compare the generated summaries with the human summaries, and assess each summary from four independent perspectives: (1) Informative (2) Concise (3) Coherent (4) Fluent.	dummyTurk
2087	0.0	Each property is assessed with a score from 1(worst) to 5(best).	dummyTurk
2088	0.0	The average results are presented as following:	dummyTurk
2089	0.0	Method                      Informative          Concise           Coherent           Fluent	dummyTurk
2090	0.0	Lead-3                            3.48                 3.20                  3.83                  4.01	dummyTurk
2091	0.0	Seq2seq-baseline          3.08                 3.01                   2.96                  3.69	dummyTurk
2092	0.0	Coverage                        3.26                 3.22                   3.19                  3.74	dummyTurk
2093	0.0	Our Model                      3.73                  3.49                  3.51                  3.79	dummyTurk
2094	0.0	The results show that our model consistently outperforms the previous state-of-the-art method Coverage.	dummyTurk
2095	0.0	Due to the limit of words in responses, detailed analysis will be added in the revision of our paper.	dummyTurk
2096	0.0	Thanks for your inspiring comments about the challenge of sentence generation.	dummyTurk
2097	0.0	Since this work mainly focuses on the information selection process during document summarization, we didn???t analyze it explicitly in our paper.	dummyTurk
2098	0.0	According to the above human evaluation results, our model is able to generate more concise summaries than Coverage.	dummyTurk
2099	0.0	We will further explore this problem deeper in the following work.	dummyTurk
2100	0.0	This paper uses the non-anonymized data as See et al.	dummyTurk
2101	0.0	(2017) and Paulus et al.	dummyTurk
2102	0.0	(2017), which we believe is the favorable problem to solve because it requires no pre-processing.	dummyTurk
2103	0.0	In order to be directly comparable with the reported results in recent state-of-the-art methods, we use the same settings with Graph-attention (Tan et al., 2017).	dummyTurk
2104	0.0	Due to the limit of total words in responses, we are sorry that we can???t reply all arguments and questions.	dummyTurk
2105	0.0	Thanks for your extraordinarily careful review.	dummyTurk
2106	0.0	 (1) We have normalized every feature by z-scores when calculating the Euclidean distance.	dummyTurk
2107	0.0	We will describe more clearly in the revised version.	dummyTurk
2108	0.0	(2) Conducting a more in-depth analysis of the sentence complexity within the books will be part of our future work.	dummyTurk
2109	0.0	 Conducting the experiment where the ???simple??? test is performed by first-year English major students and graduates, etc.	dummyTurk
2110	0.0	is not very meaningful to the evaluation, because most of the results can be anticipated.	dummyTurk
2111	0.0	Nevertheless, we can not deny that it could make the evaluation more convincing.	dummyTurk
2112	0.0	 Your suggestion of evaluation by item analysis in testing theory is feasible.	dummyTurk
2113	0.0	Due to the page limit, we cannot use all the metrics.	dummyTurk
2114	0.0	Thus we chose the most suitable ways of evaluation for our task.	dummyTurk
2115	0.0	However, a more comprehensive evaluation will be part of our future work.	dummyTurk
2116	0.0	We thank the reviewer for the helpful comments!	dummyTurk
2117	0.0	 In the paper we directly included the scores of the logistic regression model as reported in previous work and were not able to do a full analysis on its results.	dummyTurk
2118	0.0	However, we will add the reviewer???s observation with appropriate discussions to the paper.	dummyTurk
2119	0.0	 During the training and testing of the model, we mask the subject entities in each example with a special SUBJ-<NER> token, where <NER> is the entity type of the subject.	dummyTurk
2120	0.0	We mask the object entities similarly.	dummyTurk
2121	0.0	The special tokens are finetuned during training as other words in the vocabulary.	dummyTurk
2122	0.0	We will make this clear in the paper.	dummyTurk
2123	0.0	 The POS/NER embeddings are initialized randomly and finetuned on the training set.	dummyTurk
2124	0.0	We will make this clear.	dummyTurk
2125	0.0	Reviewer One notes our method favors related languages.	dummyTurk
2126	0.0	We agree that this is a limitation.	dummyTurk
2127	0.0	However, CACO remains useful, especially for low-resource languages.	dummyTurk
2128	0.0	Our method works well even when we have little or no target language data, while previous methods need a large unlabeled corpus for the target language.	dummyTurk
2129	0.0	Reviewer One suggests to clarify the related work section and the conclusion.	dummyTurk
2130	0.0	We agree and will expand these sections.	dummyTurk
2131	0.0	Reviewer One asks why transferring from a Romance language to a North Germanic language works better than the other direction.This is an excellent question that we lack a compelling answer for.	dummyTurk
2132	0.0	Our best guess at the moment is more complex morphology in Danish/Swedish.	dummyTurk
2133	0.0	Thanks for the comment.	dummyTurk
2134	0.0	The experiment results are run by the authors as for the experiment data, there is no exact training-test data partition in the original publication of the baseline models.	dummyTurk
2135	0.0	Therefore, we re-implemented and run these models in our datasets.	dummyTurk
2136	0.0	Thanks for the comment.	dummyTurk
2137	0.0	We will add the explanation of the automatic evaluation metrics.	dummyTurk
2138	0.0	The 3 metrics are used to measure the semantic similarity between the generated responses and the ground truth.	dummyTurk
2139	0.0	They are verified to be more effective in conversation/dialogue generation than BLEU and Perplexity by Serban et al.	dummyTurk
2140	0.0	(2016a) and Serban et al.	dummyTurk
2141	0.0	(2017b).	dummyTurk
2142	0.0	Thanks for the comment.	dummyTurk
2143	0.0	We will add more details about the measures of human evaluation.	dummyTurk
2144	0.0	Due to the space limitation, we didn???t present the descriptions in the current version.	dummyTurk
2145	0.0	Thanks for the comment.	dummyTurk
2146	0.0	We will modify the related papers that support our claims here and cite the relevant papers in dialogue research.	dummyTurk
2147	0.0	The word embeddings are trained on the Ubuntu and Opensubtitles data respectively.	dummyTurk
2148	0.0	The dimension of the hidden layer is a tunable parameter.	dummyTurk
2149	0.0	Therefore, the parameter settings here are obtained when the corresponding models get the best performance in dev set.	dummyTurk
2150	0.0	Thanks for the comment.	dummyTurk
2151	0.0	We will revise the format of reference and carefully proofread the paper.	dummyTurk
2152	0.0	 Thanks for the comment.	dummyTurk
2153	0.0	To the best of our knowledge, our bidirectional dependency tree network is the first to adopt a bidirectional propagation approach on the dependency-based parse tree.	dummyTurk
2154	0.0	It can effectively extract the dependency information between words as shown in our experiments.	dummyTurk
2155	0.0	We believe that this network has the potential to be applied to other NLP tasks as well, such as Translation and NER.	dummyTurk
2156	0.0	 There are three differences between our work and the existing tree-LSTM model.	dummyTurk
2157	0.0	Firstly, we use a mapping function $r_(k)$ to map the syntactic relation type to its corresponding parameter matrix instead of using the children order.	dummyTurk
2158	0.0	Secondly, we encode the syntactic relation as in Eqs.	dummyTurk
2159	0.0	(2-5).	dummyTurk
2160	0.0	It is proven effective in Table 4.	dummyTurk
2161	0.0	Finally, our tree-LSTM model is bidirectional, which works better than one directional as shown in the ablation experiments.	dummyTurk
2162	0.0	$W_{r(k)}$ and $U_{r(k)}$ are different parameters.	dummyTurk
2163	0.0	The former regards syntactic relations type as another embedding like words.	dummyTurk
2164	0.0	The latter propagates hidden representations from children nodes to their parent.	dummyTurk
2165	0.0	It is proven correct and effective in our experiments.	dummyTurk
2166	0.0	Only a brief description of CRF layer and decoding processing is given with references because of the page limit and also because we thought that CRF is fairly well known and has been used in some other deep models.	dummyTurk
2167	0.0	We will add more description to the revised paper.	dummyTurk
2168	0.0	 Thanks for your suggestion.	dummyTurk
2169	0.0	We have performed statistical significance test using the t-test.	dummyTurk
2170	0.0	The p-value for L-14 is less than 0.01, and the p-value for R-16 is less than 0.05, which show that the improvements are significant.	dummyTurk
2171	0.0	The improvement for R-14 is not significant although we have a little higher average F1 scores based on 20 runs.	dummyTurk
2172	0.0	 Thanks for pointing these out.	dummyTurk
2173	0.0	We will tone them down.	dummyTurk
2174	0.0	Their advantages and shortcomings will be described in the revised version.	dummyTurk
2175	0.0	 Those empty slots indicate that the baselines did not use the corresponding datasets in their experiments.	dummyTurk
2176	0.0	We reproduced some of them because the source codes are not available, but the results have biases.	dummyTurk
2177	0.0	We finally decided to compare with their original results, which is appropriate as the test data and the evaluation program are the same (they are both provided by SemEval).	dummyTurk
2178	0.0	We will release our source code once the paper is accepted.	dummyTurk
2179	0.0	 The embedding size is a hyper-parameter, and 300 is commonly used in many embedding papers (including (Wang et al.	dummyTurk
2180	0.0	2016b)) and has been shown quite appropriate.	dummyTurk
2181	0.0	We also find 300 gives good results in our case.	dummyTurk
2182	0.0	300 can still be applied to other datasets.	dummyTurk
2183	0.0	Of course, it may not be optimal for every dataset.	dummyTurk
2184	0.0	 We can describe the data, the trope prediction task, and the contributions in the introduction.	dummyTurk
2185	0.0	 We can improve the component descriptions.	dummyTurk
2186	0.0	Perhaps it would help if we split the bullets that describe the components into categories.	dummyTurk
2187	0.0	For example, the ???Model??? category would contain the ???baseline vs. attn??? and ???ndialog??? bullets.	dummyTurk
2188	0.0	We provided some analysis in the Ablation Results but can elaborate more (e.g.	dummyTurk
2189	0.0	why skip500 performed better with a larger dataset).	dummyTurk
2190	0.0	 In general, it sounds like all three weaknesses are related to the clarity of the paper.	dummyTurk
2191	0.0	We appreciate the comments and hope that improved writing, more detail when necessary, and better organization can address the main concerns.	dummyTurk
2192	0.0	 First, although attn_3_fixedmem outperformed attn_3_triplet_fixedmem, the metrics were not *significantly* better (e.g.	dummyTurk
2193	0.0	0.678 vs. 0.663 Accuracy).	dummyTurk
2194	0.0	Second, we had an intuition that the full model would benefit from a larger ndialog, and we wanted to test both combinations.	dummyTurk
2195	0.0	 For the baseline and attention only models, we found comparable performance using only the characters??? lines vs. using all 3 types of lines.	dummyTurk
2196	0.0	However, we do see improved performance using all 3 types in the later experiments, and we can add discussion for why a model with greater capacity helps.	dummyTurk
2197	0.0	 We didn???t want to break double blind rules but will upload our data once the process is over.	dummyTurk
2198	0.0	Line513: We use an inner product.	dummyTurk
2199	0.0	Line593: We finetune pretrained Glove embeddings.	dummyTurk
2200	0.0	 Thank you for the suggestion.	dummyTurk
2201	0.0	We will add t-test as significance test for the camera-ready version.	dummyTurk
2202	0.0	As a pilot work, we simply extract keywords from the original story to represent the storyline.	dummyTurk
2203	0.0	And the experimental results demonstrate the fact that the storyline planning can indeed help generating better stories.	dummyTurk
2204	0.0	We agree with the reviewer's point and we will explore these in our future work and add discussions in the future work section.	dummyTurk
2205	0.0	 We did not think about carrying out significance test for the MT study, but we think this is a great suggestion, and we will add this in our revised version.	dummyTurk
2206	0.0	 We have been pondering on this question as well, and had a brief discussion in the future work section.	dummyTurk
2207	0.0	One direction we are trying to push is to use the event sequence to represent storyline.	dummyTurk
2208	0.0	This will require sophisticated event extraction tools which is hard given the current techniques.	dummyTurk
2209	0.0	We believe with the advancements of event extraction, the plan-and-write framework will further shine.	dummyTurk
2210	0.0	We have made sure that total parameter sizes are roughly the same across models (i.e.	dummyTurk
2211	0.0	one RNN variants including HSM uses hiddens twice the size of a RNN stream of POS-SM).	dummyTurk
2212	0.0	We believe devoting half of the parameter to predict essentially a random class in HSM is detrimental to performance.	dummyTurk
2213	0.0	Also, even with one RNN to predict POS first has gains over HSM (Tab-5 appendix; POS-SM(h)).	dummyTurk
2214	0.0	Indeed.	dummyTurk
2215	0.0	However, we???ve found it challenging to tune NMT with large vocabulary size.	dummyTurk
2216	0.0	We are still trying, however.	dummyTurk
2217	0.0	Indeed, we missed the reference of the POS tagger for German et al.	dummyTurk
2218	0.0	"""Pattern for python."""	dummyTurk
2219	0.0	Journal of Machine Learning Research 13.Jun (2012): 2063-2067.	dummyTurk
2220	0.0	Its accuracy is around 85%.	dummyTurk
2221	0.0	[1] And we choose the NLTK default POS tagger to parse English data.	dummyTurk
2222	0.0	The accuracy of English data is around 94%.	dummyTurk
2223	0.0	[2]	dummyTurk
2224	0.0	[1]https://www.clips.uantwerpen.be/pages/pattern-de	dummyTurk
2225	0.0	[2]https://explosion.ai/blog/part-of-speech-pos-tagger-in-python	dummyTurk
2226	0.0	Thanks for pointing this out; we'll make it more clear.	dummyTurk
2227	0.0	Note that we only use predictions of the tags of the future words in the analysis.	dummyTurk
2228	0.0	Thank you for your valuable comments!	dummyTurk
2229	0.0	Our GAN based generator and data relabeling method are components which can be integrated with other neural models.	dummyTurk
2230	0.0	We will apply our method to other neural models, cross-domain datasets or even other NLP tasks in future to verify the generalization and expansibility.	dummyTurk
2231	0.0	Our model without GAN based generator is a BGRU+ATT network, which achieves similar performance (PR curve=0.51) with the PCNN+ATT model on the dataset NYT.	dummyTurk
2232	0.0	 Annotations were done manually.	dummyTurk
2233	0.0	A pair of annotators sat together and performed coreference resolution and paraphrase detection together.	dummyTurk
2234	0.0	 The formal definition of explicit relations entails the use of discourse connectives such as 'because' and 'since' to signal explicit coherence.	dummyTurk
2235	0.0	According to this definition, 'and' is not a discourse connective and therefore the relation nature is implicit.	dummyTurk
2236	0.0	 This was developed as an attempt to create reading comprehension questions.	dummyTurk
2237	0.0	We are yet to test the effectiveness of our system on users.	dummyTurk
2238	0.0	There is indeed RL in the paper,we extend SL using ideas from RL and imitation learning.	dummyTurk
2239	0.0	SL does not optimize for future rewards,and we develop approaches (or suitable loss functions) to explicitly track the goal state, similar to reward shaping in the RL literature.	dummyTurk
2240	0.0	The output of the system is a sorted list of utterances.	dummyTurk
2241	0.0	Without humans in the loop the top utterance is used.	dummyTurk
2242	0.0	We evaluate this system, in internal testing and for offline evaluation.	dummyTurk
2243	0.0	The system is being tested in real world settings where customer satisfaction is paramount.	dummyTurk
2244	0.0	The chatbot is no where near perfect and complete automation is still not possible.	dummyTurk
2245	0.0	For details in how we automate most of the workflow see Section 3.	dummyTurk
2246	0.0	 BLEU has been shown to correlate well with human judgements in MT, typically when more than 1 reference is available.	dummyTurk
2247	0.0	Since we had only 1 reference, we performed extensive human evaluations, which are presented in the paper.	dummyTurk
2248	0.0	We introduced online-BLEU to overcome this issue.	dummyTurk
2249	0.0	Our goal in this paper is to ensure fluency as well as coherency, so we needed a per- utterance evaluation.	dummyTurk
2250	0.0	A score of 1 was assigned if the response looked reasonable to a human annotator (see Accuracy in Section 5.2)	dummyTurk
2251	0.0	Our model does not memorize the knowledgebase, the model memorizes the queries that need to be made to the knowledgebase.	dummyTurk
2252	0.0	That is rather than learning the value (is customer A a member) for all entries, it learns the api call query customer table with customer A.	dummyTurk
2253	0.0	 in Section 5.2	dummyTurk
2254	0.0	Some information was delexicalized by the company???s approved tool.	dummyTurk
2255	0.0	Some information could not be made public, especially, since this is proprietary data and to be anonymous for the paper review.	dummyTurk
2256	0.0	Names, ids, numbers and dates are tracked and filled back in before presenting to the user.	dummyTurk
2257	0.0	Sorry but the wording of the table caption is misleading.	dummyTurk
2258	0.0	The ???length??? values in Table 3 means length of raw behavioral steps (for example number of clicks) in model B+T, rather than the vector length of B+T.	dummyTurk
2259	0.0	We could have provided predictions, but due to limited space, we only provided number of steps, which is somehow corresponding to prediction time (fewer steps means less prediction time).	dummyTurk
2260	0.0	Yes, some actions can???t be observed in server side directly, such as the trace of the mouse.	dummyTurk
2261	0.0	Therefore, some scripts are deployed on client side legally to collect these actions and send them back to server side.	dummyTurk
2262	0.0	For security reasons, the dataset used in the paper cannot be disclosed at the moment.	dummyTurk
2263	0.0	We are trying to find public datasets to test our model.	dummyTurk
2264	0.0	Thanks for your review.	dummyTurk
2265	0.0	Sorry for not submitting source code and manually annotated data before.	dummyTurk
2266	0.0	We will upload source code and data later.	dummyTurk
2267	0.0	We will fix the naming accordingly.	dummyTurk
2268	0.0	Your understanding is exact on the corpus, which is indeed for knowledge base.	dummyTurk
2269	0.0	"""Minimal supervision"" refers to a very small set of seed pairs (equal to the dimension of the embedding space), we will add necessary explanation later for better understanding."	dummyTurk
2270	0.0	Your understanding on unregistered NEs is exact so.	dummyTurk
2271	0.0	We will modify the concerned part accordingly.	dummyTurk
2272	0.0	Many thanks for your valuable comments.	dummyTurk
2273	0.0	We acknowledge this reviewer for pointing that out, and we had compared with 3 different models on Universal Dependencies English dataset to show the importance of the ReLU function and residual connections:	dummyTurk
2274	0.0	  1.	dummyTurk
2275	0.0	A softmax attention model with residual connection: UAS = 84.4%.	dummyTurk
2276	0.0	  2.	dummyTurk
2277	0.0	ReLU non-local feature fusion without residual connection: UAS = 85.6%.	dummyTurk
2278	0.0	  3.	dummyTurk
2279	0.0	A softmax attention model without residual connection: UAS = 84.8%	dummyTurk
2280	0.0	The experimental results showed that the proposed method of ReLU non-local feature fusion outperforms softmax attention significantly, and the residual connection is helpful to train non-local models.	dummyTurk
2281	0.0	We will supplement this experiment results into the revised version.	dummyTurk
2282	0.0	The result of baseline system on PTB and CTB will be added in revised version.	dummyTurk
2283	0.0	In our prelimenary experiments, our baseline system achieved similar performance with Kiperwasser and Goldberg (2016).	dummyTurk
2284	0.0	Our Baseline        PTB UAS = 93.2% LAS = 90.9%	dummyTurk
2285	0.0	                                CTB UAS = 85.6% LAS = 84.4%	dummyTurk
2286	0.0	Kiperwasser and Goldberg (2016)           PTB UAS = 93.2% LAS=91.2%	dummyTurk
2287	0.0	                                                                         CTB UAS = 86.6% LAS=85.1%	dummyTurk
2288	0.0	Many thanks for your valuable comments!	dummyTurk
2289	0.0	We will carefully corrected the mistakes in writting in the revised version.	dummyTurk
2290	0.0	We used the arc-standard dependency scheme in the all experiments, and train/validation/testing data is splited following the method as proposed by Chen and Manning (2014).	dummyTurk
2291	0.0	The compared models for the PTB/CTB comparison are both fusion models, which is the most powerful model we can find in the prelimenary experiments on Universal Dependencies.	dummyTurk
2292	0.0	Our code will be released publicly in the future.	dummyTurk
2293	0.0	We used the same hyper-parameters setting as Kiperwasser and Goldberg (2016) except the learning rate, which was set to the default learning rate for Adam optimizer of MXNet framework.	dummyTurk
2294	0.0	In the experiments, the hyper-parameters were not carefully tuned.	dummyTurk
2295	0.0	It would be a little difficult to build this connection.	dummyTurk
2296	0.0	In fact, many models are facing with the difficulities in sparse issue when using Chinese data in Universal Dependencies for evaulation.	dummyTurk
2297	0.0	And non-local feature fusion is not proposed to address such sparsity problem.	dummyTurk
2298	0.0	We thank the reviewer for these helpful comments.	dummyTurk
2299	0.0	***Regexv***	dummyTurk
2300	0.0	The reviewer expressed concern about the usage of the term 'regexv'.	dummyTurk
2301	0.0	Regexv (as opposed to regexp) is the Python package that we wrote to sit on top of re, Python???s built-in regular expressions package.	dummyTurk
2302	0.0	Specifically, regexv takes any terminals denoted by angle brackets and expands them in the word embedding space.	dummyTurk
2303	0.0	Its purpose was to show a concrete implementation of embedding grammars, which is why our examples were contrasted against re, a concrete implementation of regular expressions.	dummyTurk
2304	0.0	Regrettably, we didn???t make that very clear in the paper and plan to do so in the revision period if accepted.	dummyTurk
2305	0.0	***Choice of Title***	dummyTurk
2306	0.0	The reviewer also raised concerns about our choice of title.	dummyTurk
2307	0.0	Although the four page format did not lend itself to a more in-depth explanation, the central thrust of this research really does lie in the construction of an embedding-based grammar, with regular expressions being used primarily as a means of testing the viability of the approach.	dummyTurk
2308	0.0	We will make that more clear in subsequent revisions.	dummyTurk
2309	0.0	***Additional Notes***	dummyTurk
2310	0.0	This paper was meant to show the power of embedding grammars.	dummyTurk
2311	0.0	Accordingly, the focus of this paper is not the concrete implementation of embedding grammars (regexv), but rather the idea of embedding grammars itself.	dummyTurk
2312	0.0	We feel that there are many exciting avenues of exploration inherent in this work, as the reviewers have pointed out.	dummyTurk
2313	0.0	This is a strong indication that embedding grammars are a meaningful area of research that deserves focused attention from many researchers.	dummyTurk
2314	0.0	Apropos the conjecture of systematic bias :  Our primary goal in the experiments was to compare NVSE to the baseline systems.	dummyTurk
2315	0.0	Since NVSE is present in both groups therefore our experiment can determine the rank of NVSE in comparison to BS, BM25, W2Vec and SetExpan.	dummyTurk
2316	0.0	For example, consider a hypothetical 'extreme' version of our experiment, where we compare NVSE to each baseline individually.	dummyTurk
2317	0.0	So we are doing comparison in 4 groups.	dummyTurk
2318	0.0	If the annotators had found NVSE better than 3 baselines but worse than 1 of the baseline then we will know exactly how NVSE lies in comparison to the other systems.	dummyTurk
2319	0.0	The only information lost is that we can not distinguish between the three system that were deemed inferior, but we do not care much for the ranking amongst the baselines anyway.	dummyTurk
2320	0.0	The empirical evaluation is not fully convincing...	dummyTurk
2321	0.0	As our stratified experiments show NVSE worked well in a middle-region and that BM25 baseline did well in more extreme settings.	dummyTurk
2322	0.0	Since we can find out at test time how many sentences an entity appears in, it is easy to switch between the two implementations in a real system.	dummyTurk
2323	0.0	Therefore our approach can complement existing approaches.	dummyTurk
2324	0.0	Secondly, regarding baselines, For the Word2Vecf baseline we were careful about the number of parameters per entity and therefore doubled the embedding size that we used in NVSE.	dummyTurk
2325	0.0	We also ran Word2Vecf to convergence.	dummyTurk
2326	0.0	Similarly for SetEx we followed all the recommended preprocessing steps.	dummyTurk
2327	0.0	For BS we did reasonable experiments for selecting the value of lambda and even implemented two other variants of bayesian-sets described in the appendix.	dummyTurk
2328	0.0	Generally speaking, we agree that a supervised system must perform better than an unsupervised system at least given large amounts of data.	dummyTurk
2329	0.0	The problem is of course that getting supervision in the form of click-through data is only possible for search engines with a large user base.	dummyTurk
2330	0.0	Automatic KBP and ESE on the other hand, is employed more in enterprise settings where a small user base within a company / or government may be interacting with textual data.	dummyTurk
2331	0.0	Therefore mostly unsupervised techniques have been proposed for ESE.	dummyTurk
2332	0.0	For example, DeepSets actually uses LDA to create supervised data.	dummyTurk
2333	0.0	So in some sense it's results can never be better than basic LDA since it is just learning to replicate LDA's results.	dummyTurk
2334	0.0	OTOH LDA has not been used for ESE previously therefore we did not compare to it.	dummyTurk
2335	0.0	We thank the reviewer for the positive opinion about our intuition and for the valuable comments.	dummyTurk
2336	0.0	We're happy to present additional analysis about the role of the parameters values in the additional page should the paper be accepted.	dummyTurk
2337	0.0	Thank you for the suggestion!	dummyTurk
2338	0.0	Please refer to the response to Reviewer#1's weakness argument_1-(3).	dummyTurk
2339	0.0	 Our word embeddings are randomly initialized and trained as parameters.	dummyTurk
2340	0.0	We will make the statement clearer in the revised version.	dummyTurk
2341	0.0	 The parameter is empirically tuned by referring to previous studies.	dummyTurk
2342	0.0	 Please refer to the response to Reviewer#1's weakness argument_1-(3).	dummyTurk
2343	0.0	Thanks a lot for all the reviews, suggestions and comments.	dummyTurk
2344	0.0	 We believe it???s important to go beyond current datasets to get a view into how models are really performing.	dummyTurk
2345	0.0	The created dataset helped in measuring performance of different SOTA systems in a more realistic way which could hopefully inform the future development in this area.	dummyTurk
2346	0.0	We also took some early steps towards improving a tagger???s accuracy on these ambiguous cases.	dummyTurk
2347	0.0	The current best reported number is still in the low 80???s which means the problem is still unsolved.	dummyTurk
2348	0.0	 We can do additional error analysis on the 0.3 drop in WSJ and add more information to a final version.	dummyTurk
2349	0.0	Assume a complex hotel reservation domain.	dummyTurk
2350	0.0	In addition to booking and payment, users may specify detailed preferences.	dummyTurk
2351	0.0	System designers splits the domain into three sub-tasks: hotel searching, booking, and payment.	dummyTurk
2352	0.0	However, since hotel searching requires much information from users, designers may decompose hotel searching into:	dummyTurk
2353	0.0	- hotel facility sub-task (meeting room, gym, pool),	dummyTurk
2354	0.0	- room facility sub-task (TV, bathroom),	dummyTurk
2355	0.0	- location sub-task (station, restaurants, and landmarks)	dummyTurk
2356	0.0	However, the separation of hotel and room facility can be blurry, since several information slots, e.g.	dummyTurk
2357	0.0	internet and breakfast, can be put into either sub-task.	dummyTurk
2358	0.0	Designers also need to define a valid action set, starting conditions, and terminating conditions for each sub-task.	dummyTurk
2359	0.0	We didn???t have empirical analysis, but we???ll provide analysis based on how HRL-OC_PVF behaves.	dummyTurk
2360	0.0	The use of clearly-defined inherent sub-goals benefits our sub-goal discovery.	dummyTurk
2361	0.0	Our model is more suitable for discovering user's inherent sub-goals that are executed in a fixed order.	dummyTurk
2362	0.0	On the other hand, in flexible execution order of inherent sub-goals, we cannot guarantee if the discovered sub-goals are still meaningful (i.e.	dummyTurk
2363	0.0	imitating inherent sub-goals).	dummyTurk
2364	0.0	The flexible order causes a sub-goal to have state transition clusters that appear in several different locations in the state space.	dummyTurk
2365	0.0	Nevertheless, the state transitions of a sub-goal is relatively invariant to the cluster location.	dummyTurk
2366	0.0	PVF, however, will consider these clusters as different yet redundant sub-goals even if they correspond to a single inherent sub-goal.	dummyTurk
2367	0.0	The OC of HRL-OC_PVF can mitigate this problem by adjusting the PVF's redundant sub-goals to maximize cumulative extrinsic reward.	dummyTurk
2368	0.0	However, these adjusted redundant sub-goals aren't guaranteed to imitate inherent sub-goals.	dummyTurk
2369	0.0	HRL-OC PVF with one option performs better than flat RL, since it involves critic (a part of OC).	dummyTurk
2370	0.0	Critic uses state-value function for bootstrapping, which can substantially reduce variance during learning.	dummyTurk
2371	0.0	HRL-OC_PVF with 2 or 3 options slightly outperforms HRL-OC_PVF with 1 option at the success rate, and also discover meaningful sub-goals (Table 2&3).	dummyTurk
2372	0.0	These sub-goals can be reused in a similar yet different dialogue domain, e.g.	dummyTurk
2373	0.0	payment sub-goal discovered in hotel domain might be reused in restaurant domain.	dummyTurk
2374	0.0	We thank the reviewers for their invaluable comments and constructive suggestions.	dummyTurk
2375	0.0	We address the issues raised in the reviews below.	dummyTurk
2376	0.0	Thank you for the suggestion.	dummyTurk
2377	0.0	This ambiguity was chosen deliberately for author anonymity.	dummyTurk
2378	0.0	We plan to describe the details on data upon release including the list of news media outlet, language traits, and a preprocessing methods used in Section 3.2 Dataset Generation.	dummyTurk
2379	0.0	For fair comparison, we trained each models (in Table 2) with FNC-1 dataset only.	dummyTurk
2380	0.0	In a single model case, feature-based machine learning method, the XGB, outperformed the deep learning models.	dummyTurk
2381	0.0	We conjecture this is caused by insufficient variations in the FNC-1 data that is used to train complex neural models.	dummyTurk
2382	0.0	The FNC-1 dataset is sizable (around 50,000 rows), yet it was built from only 1,683 distinct news articles.	dummyTurk
2383	0.0	The data instances were generated by modifying {headlines}, indicating that 29.7 data instances have identical {body text} on average.	dummyTurk
2384	0.0	Such redundancy likely affected the result.	dummyTurk
2385	0.0	However, given that in real systems it is uncommon for news  articles to have identical {body text}, we consider deep learning models to perform consistently well in the wild.	dummyTurk
2386	0.0	We presented relevant materials in Appendix C.	dummyTurk
2387	0.0	As much as we would like to do this, unfortunately, in most question generation scenarios using BLEU or METEOR as selection criterion is not possible.	dummyTurk
2388	0.0	This is because calculating BLEU or METEOR scores requires the ???ground-truth???, or reference questions.	dummyTurk
2389	0.0	However, during inference - question generation step - the model does not have access to the reference questions; instead, the reference questions are only used to evaluate the quality of the questions that a model has already generated.	dummyTurk
2390	0.0	The representations introduced in this paper can be used in real NLP scenarios as a way of encoding prior knowledge into the representations used by downstream applications.	dummyTurk
2391	0.0	In the paper we try to emphasize how these representations may be used to encode common-sense knowledge.	dummyTurk
2392	0.0	We considered attempting WSD, but found that it was beyond the scope of this paper.	dummyTurk
2393	0.0	We believe the pipeline could be adapted at two different stages: while parsing a corpus, making use of local syntactic information; or after applying SRL to large corpora and representing the derived global affordances in a graph, similarly to Moro et.	dummyTurk
2394	0.0	al (2014 TACL).	dummyTurk
2395	0.0	The underlying structure of our representations and relations is provided by language usage, as captured by SRL.	dummyTurk
2396	0.0	In future work, we plan to apply statistical relational methods in order to more effectively exploit this loosely defined structure.	dummyTurk
2397	0.0	Thanks for the insightful comments!	dummyTurk
2398	0.0	[What are the results of majority vote?]	dummyTurk
2399	0.0	[Accuracy]	dummyTurk
2400	0.0	In Eval I, majority vote archives 50% accuracy.	dummyTurk
2401	0.0	This is because we ensure the number of ongoing and ended conversations are the same in dataset construction.	dummyTurk
2402	0.0	In Eval II, majority vote has over 60% accuracy.	dummyTurk
2403	0.0	The reason is the number of conversation turns satisfy long tail distribution, as shown in Figure 1.	dummyTurk
2404	0.0	In the training and test data, most conversations contain only one residual turn, which is outputted by majority vote.	dummyTurk
2405	0.0	[F1 scores]	dummyTurk
2406	0.0	Because of the long-tail distribution of conversation turn number, in Eval II, the prediction becomes harder for conversations with longer residual life due to the sparsity of training instances.	dummyTurk
2407	0.0	Similar to the evaluation of information retrieval, we should measure F1 scores.	dummyTurk
2408	0.0	Because majority vote can only output 1 turn for ongoing conversations, the average F1 scores yielded by majority vote is outperformed by most machine learning models.	dummyTurk
2409	0.0	We will report the F1 scores of majority vote for all types of residual life in the revised paper.	dummyTurk
2410	0.0	[The results of significance test?]	dummyTurk
2411	0.0	We will report the results of significance test in the revised paper.	dummyTurk
2412	0.0	[Why are the avg scores for different quality terms seem similar?]	dummyTurk
2413	0.0	There is an error for the reported avg scores.	dummyTurk
2414	0.0	All correct scores should be 5 times larger, but the distribution is the same, i.e., responses with higher quality is predicted to have longer residual life.	dummyTurk
2415	0.0	We will fix the error in the revised paper.	dummyTurk
2416	0.0	We were not focusing on the syntactic analysis, indeed.	dummyTurk
2417	0.0	We just noted that structures formed by the network were often correct from the syntactic point of view.	dummyTurk
2418	0.0	If need be, we can address the specific issues, that you believe are necessary.	dummyTurk
2419	0.0	Could you, please, specify what do you mean under agreement setting?	dummyTurk
2420	0.0	The phonetic information for the words that were missing was either not provided at all or was obtained through a phonetic-heuristics that coarsely approximated phonetics of the word using its char-representation.	dummyTurk
2421	0.0	We used contracted words as they are for learning and saw them occasionally reproduced in generation when network was Shakespear-conditioned.	dummyTurk
2422	0.0	Thank you very much for your comments.	dummyTurk
2423	0.0	We are happy to add more generated examples in Russian and in English, if you find them valuable.	dummyTurk
2424	0.0	We didn't want to convince the reader that the method is flawless and can give examples of characteristic mistakes.	dummyTurk
2425	0.0	Table 1 intends to illustrate the idea of our proposed method to represent a word embedding using a number of attribute embeddings.	dummyTurk
2426	0.0	In our case, those latent attributes are automatically learned from the parallel training data.	dummyTurk
2427	0.0	In general, the number attributes and its values can be arbitrarily chosen only if it gives enough unique combinations to represent all words.	dummyTurk
2428	0.0	For simplicity, we chose to use only two attributes, and each of which contains \sqr(|V|) values.	dummyTurk
2429	0.0	That is, for a given vocabulary, we need at least |a_1| and |a_2| attribute values, such that |V| = |a_1| x |a_2|.	dummyTurk
2430	0.0	Our method is very different from that of Morin and Bengio (2005) in several aspects.	dummyTurk
2431	0.0	1) We use RNN to learn the embeddings of attribute values of a word representation which is context dependent; 2) We do not impose any prior hierarchical knowledge (as they use WordNet to obtain the word clusters) in the learning.	dummyTurk
2432	0.0	The attribute value representations of word embeddings are automatically learnt.	dummyTurk
2433	0.0	Our method is ready for any language without relying on other hierarchical decomposition tools; and 3) Our method is trained to improve the translation quality by better capture the syntactic and semantic mean of a word as well as reducing the time and space complexity, while their work only focuses on the latter aspect.	dummyTurk
2434	0.0	The details of the baseline model are presented in Section 2, while the used configurations are provided in Section 4.1.	dummyTurk
2435	0.0	To conduct a comprehensive comparison against the state-of-the-art architectures.	dummyTurk
2436	0.0	We have prepared the following English-German translation results of various models, including the adapted hierarchical probabilistic model in NMT by Chen et al.	dummyTurk
2437	0.0	(2016).	dummyTurk
2438	0.0	In the final paper, we will include this part of evaluations as suggested.	dummyTurk
2439	0.0	"Model			newstest13	newstest14	newstest15"	dummyTurk
2440	0.0	Luong et al.	dummyTurk
2441	0.0	"(2015)	-		-		22.0"	dummyTurk
2442	0.0	Jean et al.	dummyTurk
2443	0.0	"(2015)	-		19.4		-"	dummyTurk
2444	0.0	Sennrich et al.	dummyTurk
2445	0.0	"(2016)	-		-		22.8"	dummyTurk
2446	0.0	Chung et al.	dummyTurk
2447	0.0	"(2016)	-		21.3		23.5"	dummyTurk
2448	0.0	Chen et al.	dummyTurk
2449	0.0	"(2016)	21.0		19.0		21.9"	dummyTurk
2450	0.0	"Our Model		22.09		21.55		24.07"	dummyTurk
2451	0.0	"Our Model (Big)		23.34		22.27		25.39"	dummyTurk
2452	0.0	Czech treebank is rather large (1.5 M tokens) and is based on a manually annotated treebank.	dummyTurk
2453	0.0	It contains a decent variety of common Czech language constructions, and is relatively homogenous (news, business news and popular science from early 1990s).	dummyTurk
2454	0.0	The data that we add is from different domain and time period, but we still evaluate on the data from the treebank.	dummyTurk
2455	0.0	Next, we assume that there should be some limits of improvement: when we see a LAS of 93%, what accounts for the remaining 7%?	dummyTurk
2456	0.0	Possibly some annotation errors and annotation inconsistency within the treebank, rare constructions that are poorly represented in the training data, 'tricky' constructions that look indistinguishable for parsers, etc.	dummyTurk
2457	0.0	In addition, we should mention that the error analysis did not reveal any specific tendencies.	dummyTurk
2458	0.0	The corrected results will be included in the final version if the paper is accepted.	dummyTurk
2459	0.0	Thanks for pointing this out.	dummyTurk
2460	0.0	We agree, this additional sampling method should be removed from the paper.	dummyTurk
2461	0.0	We have proposed a contribution in the short paper format.	dummyTurk
2462	0.0	We have assumed that it is the proper format for a small focused contribution.	dummyTurk
2463	0.0	That is how we understand the information given in the call for papers.	dummyTurk
2464	0.0	Thus there is no extra space for longer paragraphs concerning CoNLL17 task.	dummyTurk
2465	0.0	However we agree that some important references should be added as well as a short tutorial on self-training.	dummyTurk
2466	0.0	Here is an example of an English sentence in the CONLLU format.	dummyTurk
2467	0.0	The sentence was parsed with the parser for which training data was enriched with artificial sentences (L341-345).	dummyTurk
2468	0.0	The 'orphan' relation is correctly predicted for token 12.	dummyTurk
2469	0.0	According to the UD guidelines, the 'orphan' label is used to annotate ellipsis.	dummyTurk
2470	0.0	The original parser predicts 'nmod' relation that is used for nominal dependents of another noun or noun phrase.	dummyTurk
2471	0.0	For more details on these relations and their definitions, please, see the UD guidelines (http://universaldependencies.org/u/dep/index.html).	dummyTurk
2472	0.0	"1	Where	where	ADV	WRB	_	3	advmod"	dummyTurk
2473	0.0	"2	I	I	PRON	PRP	_	3	nsubj"	dummyTurk
2474	0.0	"3	go	go	VERB	VBP	_	6	advcl"	dummyTurk
2475	0.0	"4	,	,	PUNCT	,	_	6	punct"	dummyTurk
2476	0.0	"5	she	she	PRON	PRP	_	6	nsubj"	dummyTurk
2477	0.0	"6	goes	go	VERB	VBZ	_	0	root"	dummyTurk
2478	0.0	"7	--	--	PUNCT	,	_	6	punct"	dummyTurk
2479	0.0	"8	and	and	CCONJ	CC	_	10	cc"	dummyTurk
2480	0.0	"9	the	the	DET	DT	_	10	det"	dummyTurk
2481	0.0	"10	kids	kid	NOUN	NNS	_	6	conj"	dummyTurk
2482	0.0	"11	with	with	ADP	IN	_	12	case"	dummyTurk
2483	0.0	"12	us	we	PRON	PRP	_	10	orphan"	dummyTurk
2484	0.0	"13	."	dummyTurk
2485	0.0	.	dummyTurk
2486	0.0	"PUNCT	."	dummyTurk
2487	0.0	"_	6	punct"	dummyTurk
2488	0.0	The difference across languages can be explained by the different origin and composition of the treebanks used in the evaluation:	dummyTurk
2489	0.0	The Czech treebank is very large in comparison to the others, and relatively homogenous (news, business news and popular science from early 1990s).	dummyTurk
2490	0.0	We add data from the web, i.e.	dummyTurk
2491	0.0	different domain and time period, but we still evaluate on data from the treebank.	dummyTurk
2492	0.0	It is not a big surprise that the method does not help in this configuration.	dummyTurk
2493	0.0	Both Finnish and English (especially English!)	dummyTurk
2494	0.0	treebanks contain web data.	dummyTurk
2495	0.0	Supposedly the evaluation data has more in common with the data we are adding, hence the improvement.	dummyTurk
2496	0.0	The Slovak treebank is small and contains a mixture of genres.	dummyTurk
2497	0.0	It includes texts from Wikipedia, i.e.	dummyTurk
2498	0.0	a specific subset of the web.	dummyTurk
2499	0.0	It mostly contains short sentences.	dummyTurk
2500	0.0	Adding longer sentences from the web may contribute more complex structures (as well as more lexical material).	dummyTurk
2501	0.0	That is a possible explanation of why data enrichment helps Slovak more than the other, larger treebanks.	dummyTurk
2502	0.0	Ellipsis is rare and as such it may not be the top priority for everyone but it does not mean that we do not want to get it right.	dummyTurk
2503	0.0	Of course it does not mean that parsing coordination will be solved by this.	dummyTurk
2504	0.0	An interesting point about ellipsis is that some treebanks use NULL nodes to represent the elided word.	dummyTurk
2505	0.0	Reconstructing NULL nodes in unseen data is an extra step beyond what is normally expected from a dependency parser.	dummyTurk
2506	0.0	It would be useful to solve that task and compare it to parsing ellipsis without NULL nodes; however, we need to solve the data sparseness first.	dummyTurk
2507	0.0	The performance of LSTM-CRF on OntoNotes* is (1) Dev.	dummyTurk
2508	0.0	: Pr(92.19), Re(92.63), F1(92.41); (2) Test: Pr(93.03), Re(92.60) F1(92.81).	dummyTurk
2509	0.0	POM scheme helps in most cases except the cases where different entities appear consecutively, like Figure 5(b); and consistent with your studies, we also find that such consecutive entities are rare.	dummyTurk
2510	0.0	POM relies on CRFs' capability of structure learning.	dummyTurk
2511	0.0	Generally, to extract a named entity, we need to identify its non-common word and determine its boundaries.	dummyTurk
2512	0.0	CRFs learns the structure and determines the boundaries, but using CRFs alone may fail in the cases (especially in domain-specific entity recognition) where common text contains phrases that have similar structure as named entities.	dummyTurk
2513	0.0	The non-common word helps distinguish the named entities from the phrases with similar structures.	dummyTurk
2514	0.0	The strength of POM scheme is protects the predictive power of non-common words as well as the features that depend on words, like POS tags; this further protects CRFs' structure learning capability.	dummyTurk
2515	0.0	BIO scheme instead undermines those properties.	dummyTurk
2516	0.0	Power law in entity length is discovered as a linguistic phenomenon, like Zipf's law in words' rank-frequency.	dummyTurk
2517	0.0	Its direct use is its mean: less than 2 words in an average entity (Line 394-398, Line 430-431).	dummyTurk
2518	0.0	NER includes two sub-tasks: named entity extraction and classification.	dummyTurk
2519	0.0	The extraction is related to syntax and the classification to semantics.	dummyTurk
2520	0.0	According to Chomsky's Syntactic Structures (1957, Page 93-94), syntax is not necessarily related to semantics and semantics does not necessarily affect syntax.	dummyTurk
2521	0.0	In Syntactic Structures, Chomsky critisizes the misunderstanding of syntax and semantics.	dummyTurk
2522	0.0	Defining the named entity extraction and classification as an end-to-end task is a kind of such misunderstanding.	dummyTurk
2523	0.0	Evidence from Chomsky's syntactic theory to our analysis (proper nouns are the syntactic features) and experiments (semantic features learned by LSTM-CRF for semantic classification does not help the syntactic extraction; Line 684-693, 725-729) suggests us to address the extraction and classification separately.	dummyTurk
2524	0.0	Chomsky's Syntactic Structures can be found at http://www.linguist.univ-paris-diderot.fr/~edunbar/ling499b_spr12/readings/syntactic_structures.pdf	dummyTurk
2525	0.0	In experiments, we incorporate entity types in the labeling tags but reports only the extraction results (Line 594-497, 610-612).	dummyTurk
2526	0.0	We sincerely thank all three reviewers for their detailed, insightful and helpful reviews!	dummyTurk
2527	0.0	Thank you for pointing this out!	dummyTurk
2528	0.0	We haven???t done error analysis at the time of submission.	dummyTurk
2529	0.0	But if given extra space, we would like to include analysis, especially the suggestion to look at local non-projective dependencies.	dummyTurk
2530	0.0	This is another good point!	dummyTurk
2531	0.0	We haven???t looked at projective languages, but would like to add those experiments if space allows.	dummyTurk
2532	0.0	We acknowledge this and will fix this in our next version, as well as the bibliographic recommendations in the additional comments.	dummyTurk
2533	0.0	See reply to weakness argument 1.	dummyTurk
2534	0.0	See reply to weakness argument 2.	dummyTurk
2535	0.0	We will revise our paper of describing what our model really addresses as you suggest.	dummyTurk
2536	0.0	We focus on generate descriptions that are correctly supported by the input data in this paper, thus only do human evaluation on facts examination from generated texts.	dummyTurk
2537	0.0	We will add fluency evaluation as many of you are interested.	dummyTurk
2538	0.0	Yes, there are phrases like 'triple-double' and 'consecutive win' requiring comparison or ranking over multiple columns.	dummyTurk
2539	0.0	We will explore those slightly complex operations in the future.	dummyTurk
2540	0.0	Yes, we use BLEU-4 as evaluation metric.	dummyTurk
2541	0.0	And every instance contains single reference.	dummyTurk
2542	0.0	As we can only crawl the single game headline in EPSN website.	dummyTurk
2543	0.0	Game results information are indispensable in headline generation, which requires the results of minus operator.	dummyTurk
2544	0.0	We therefore do not exclude the minus operator in experiments.	dummyTurk
2545	0.0	The method of Wiseman is very similar to seq2seq + copy, neither Wiseman nor Seq2Seq+copy involved pre-executed operations.	dummyTurk
2546	0.0	Besides the BLEU performance is also similar to seq2seq + copy method as well.	dummyTurk
2547	0.0	We therefore do not conclude human judgement on this method for the annotation cost concern.	dummyTurk
2548	0.0	We thank the reviewer for the helpful comments.	dummyTurk
2549	0.0	 We make the ???sequential??? assumption in which the visual representation of a sentence can be completely determined from previous sentences.	dummyTurk
2550	0.0	We agree that depending on the application, our model can be extended to do away with this assumption.	dummyTurk
2551	0.0	This can be an orthogonal improvement of the visual representation module, with the core idea behind DSMN staying the same.	dummyTurk
2552	0.0	 Since DSMN* performs much better than baselines even when visual supervision is provided for only a small fraction (~1% for HouseQA, ~3% for ShapeIntesection) of the data, we expect it to perform well (much better than baselines) even with the suggested weaker form of supervision.	dummyTurk
2553	0.0	However, we do expect that with the suggested weaker form of supervision, DSMN* would require more samples to achieve a similar performance in the case where supervision is available for only a portion of samples	dummyTurk
2554	0.0	 This can be an insightful ablation study.	dummyTurk
2555	0.0	We plan to add it in our next version.	dummyTurk
2556	0.0	We expect the 2D spatial memory to contribute more in the with-visual-supervision model as compared to the without-visual-supervision one.	dummyTurk
2557	0.0	 DMN+ computes features for p_i^t by using both elementwise absolute difference and elementwise multiplication (i.e.	dummyTurk
2558	0.0	for DMN+, p_i^t = [|m^(t-1) - s^(i)|; m^(t-1) \circ s^(i); |q - s^(i)|;  q \circ s^(i)], where |.| represents element-wise absolute value, and \circ represents element-wise multiplication ).	dummyTurk
2559	0.0	In order to have a similar structure for image features, we use two encoders	dummyTurk
2560	0.0	 We visualized the intermediate visual representations but when no visual supervision is provided they were not interpretable.	dummyTurk
2561	0.0	In case when visual supervision is provided, the intermediate visual representation is well formed and looked similar to the ground truth.	dummyTurk
2562	0.0	We will add some examples in the final version.	dummyTurk
2563	0.0	 For HouseQA, the variance in accuracy of DSMN is 8.36% and DSMN* is 12.93%.	dummyTurk
2564	0.0	For ShapeIntesection, the variance in RMSE of DSMN is 0.08 and DSMN* is 0.11.	dummyTurk
2565	0.0	 Thank you for the suggestions.	dummyTurk
2566	0.0	We will incorporate them in the next version.	dummyTurk
2567	0.0	 Essay representation is hard to evaluate unless fed with a concrete task.	dummyTurk
2568	0.0	In terms of the ASAP dataset, essay scoring is the only task we could perform.	dummyTurk
2569	0.0	Other than this, both the performances with or without reinforcement learning are included.	dummyTurk
2570	0.0	 We have to admit that our methods have limitations regarding to datasets.	dummyTurk
2571	0.0	The limitations are briefly analyzed in section 3.2.	dummyTurk
2572	0.0	The effect of rating schema is mostly contained in matrix E in the calculation of QWK.	dummyTurk
2573	0.0	As we can see, QWK value is computed with information of ranks from ranking scheme.	dummyTurk
2574	0.0	 Due to the paper length, we did not show more experiment results.	dummyTurk
2575	0.0	We consider our main contribution as the introduction of optimizing QWK.	dummyTurk
2576	0.0	By the existing experiment results, we do think we verified the effectiveness of our method.	dummyTurk
2577	0.0	 We will give the discussion about the similarity and the difference between our method and that listed in weakness 1.	dummyTurk
2578	0.0	 We will also give the discussion about the similarity and the difference between our method and other multimodal representation learning methods.	dummyTurk
2579	0.0	 We will add the experimental results of the human subjective evaluation.	dummyTurk
2580	0.0	 Perhaps we need further the analysis of the case.	dummyTurk
2581	0.0	The reason will be added in Section 3.2.	dummyTurk
2582	0.0	 We will add the performance of single MultiCap-{X,H,C,HC} in AIC-ICC dataset.	dummyTurk
2583	0.0	 Our approach does not align relations in two KGs.	dummyTurk
2584	0.0	By equivalent relation, we mean entities are about the same real-world object.	dummyTurk
2585	0.0	"Here ""relation"" is not the relations in KGs."	dummyTurk
2586	0.0	We would like to rewrite related sentences to avoid the ambiguity.	dummyTurk
2587	0.0	 We had tested Relational-GCNs in the KG alignment tasks, the model has too many parameters and it turns out the results are not good.	dummyTurk
2588	0.0	Due to the limit of paper length, we did not include the results in the paper.	dummyTurk
2589	0.0	According to our observations in the experiments, we don't have to distinguish every specific type of relations in the KG alignment tasks, the relation's ability to propagate equivalences is more important.	dummyTurk
2590	0.0	So we propose to use the functionality and inverse functionality to initialize the connectivity matrix.	dummyTurk
2591	0.0	Although the measures are simple, they do improve the results.	dummyTurk
2592	0.0	 The input attribute vectors are sparse and high-dimensional, which is determined by the input KGs.	dummyTurk
2593	0.0	Our approach can map the original attribute vectors to low-dimensional dense vectors by using GCNs.	dummyTurk
2594	0.0	The dimension of output attribute embeddings is set to 100.	dummyTurk
2595	0.0	The preprocess of attribute values does need some efforts, but they are indeed useful in improving the results.	dummyTurk
2596	0.0	 As we explained above, the model complexity of Relation-GCN is too large for KG alignment tasks.	dummyTurk
2597	0.0	 The parameter complexity of our approach is O(ed) and O(ad) for structural and attribute embedding, where ???a???, ???e???, ???d??? stand for the number of attributes, entities and the number of dimensions of output vectors.	dummyTurk
2598	0.0	 Our approach does not need pre-aligned relations and it also does not generate any relation alignments.	dummyTurk
2599	0.0	 Structure embedding and attribute embedding are performed in two independent processes.	dummyTurk
2600	0.0	The model for structure embedding and attribute embedding share the same framework, which is described in section 4.1 ~ 4.3, including the convolutional computation, object function, etc.	dummyTurk
2601	0.0	The attribute embedding and structure embedding differ in the initialization of inputs of GCNs.	dummyTurk
2602	0.0	In attribute embedding, the input feature vectors are attribute vectors of entities; in structure embedding, the input feature vectors are randomly initialized.	dummyTurk
2603	0.0	 Beta is empirically set.	dummyTurk
2604	0.0	Thank you for your precise advice.We will fix ???lines 185-191???.	dummyTurk
2605	0.0	Fig.3 is explained by the line 448.	dummyTurk
2606	0.0	Firstly, the input is given to the function f_E (feature extractor).	dummyTurk
2607	0.0	Then, f_E generates a hidden state h. Let the hidden state for the high-resource language h^(s) and for a low-resource language h^(t).	dummyTurk
2608	0.0	We learn parameters to make the critic generate higher score for h^(s) and lower score for h^(t).	dummyTurk
2609	0.0	We are sorry for forgetting to describe the parameters used for the critic g_pi.	dummyTurk
2610	0.0	We will add it to the camera-ready version.	dummyTurk
2611	0.0	In the original Ubuntu IRC logs, users often specify addressees with their usernames at the beginning of utterances.	dummyTurk
2612	0.0	Following [Lowe et al., 2015] and [Ouchi and Tsuboi, 2016], if the first word is identical to a username, we treat it as the gold addressee.	dummyTurk
2613	0.0	We created a lead-only baseline by taking the first sentence of each paragraph and calculated the scores for the generated summaries.	dummyTurk
2614	0.0	The context vector c_t is shown in Figure2.	dummyTurk
2615	0.0	The HRL representation is highly abstractive.	dummyTurk
2616	0.0	A magnified image of the HRL can be added for clarification.	dummyTurk
2617	0.0	References will be added and the inconsistencies will be corrected.	dummyTurk
2618	0.0	"The ""action"" edges represent the same thing."	dummyTurk
2619	0.0	Two ways of representation exist in the literature.	dummyTurk
2620	0.0	The authors in [Kulkarni] use your suggested format however the HRL in [Peng] uses the same depiction as ours.	dummyTurk
2621	0.0	"Knowing there are ""anonymized"" and ""non-anonymized"" versions of CNN/Daily-mail, Table2 represents the comparison of our HRL model with the method on non-anonymized version and our implementation of RL."	dummyTurk
2622	0.0	"The recent work of [Paulus] uses a different preprocessing, training and evaluation pipeline on the ""anonymized"" version, therefore, the direct comparison is hard."	dummyTurk
2623	0.0	However, to show the superiority of our HRL model over their RL model we implemented the RL model like that of [Paulus] and the results are shown in Table2.	dummyTurk
2624	0.0	We believe Table2 can show the effectiveness of our HRL over RL.	dummyTurk
2625	0.0	Table3 represents the reported values by the existing methods on the anonymized version of CNN/daily-mail.	dummyTurk
2626	0.0	Even if our implementation and results of RL is not acceptable by readers, we have Table3 to show our HRL agent can beat the original results (RL+ML) in [Paulus] in terms of ROUGE-1 and ROUGE-2.	dummyTurk
2627	0.0	 We think it is important that our model reliably performs competitively across a range of tasks.	dummyTurk
2628	0.0	It is not at the top of the leaderboard for any of these tasks, our aim in this paper was the thorough exploration of a linguistically interesting model, not to set a new record on a particular task.	dummyTurk
2629	0.0	 See the response to weakness #1 of reviewer #1.	dummyTurk
2630	0.0	 While it is an intuitive idea to have recurrent RNs without tree constraints, theoretically it is less clearly beneficial since all nodes are connected to all nodes.	dummyTurk
2631	0.0	We did, however, experiment with this setup and found that the results are not as good.	dummyTurk
2632	0.0	We will include the results in the revised version of the paper.	dummyTurk
2633	0.0	 We will include results on two IWSLT English-Vietnamese and WMT German-English.	dummyTurk
2634	0.0	 The value of T is set to 3 in the experiments, which was chosen based on the scores of dev set.	dummyTurk
2635	0.0	We???ll include a curve of accuracies of dev set w.r.t.	dummyTurk
2636	0.0	the value T in the revised version of the paper.	dummyTurk
2637	0.0	 No, we didn???t, but this is a good question.	dummyTurk
2638	0.0	We did because for supervised trees a node may have no child or multiple children, making it hard to incorporate their messages in a principled way.	dummyTurk
2639	0.0	We are very appreciated for your strong support to our paper, but it seems that the other two reviewers	dummyTurk
2640	0.0	are not convinced of our paper as a replication paper with a multilingual and multi-domain extension.	dummyTurk
2641	0.0	Also, thanks to your wording and citation suggestions, and we will revise our paper correspondingly.	dummyTurk
2642	0.0	Thanks for your suggestion, and we will fill the blank spots in the results table.	dummyTurk
2643	0.0	We will improve the system diagram and description for a better understanding.	dummyTurk
2644	0.0	We thank the reviewer for their positive feedback.	dummyTurk
2645	0.0	We address the specific concerns below:	dummyTurk
2646	0.0	  Given more space, we will add more error analysis as we realize that this is quite condensed in the current version.	dummyTurk
2647	0.0	We will include more details about what types of categories are easier/harder to classify along with more examples.	dummyTurk
2648	0.0	  One reason for the difficulty in categorizing Reiss labels is that they are more fine-grained, resulting in each category occurring less often and similar patterns triggering different labels.	dummyTurk
2649	0.0	 We agree that out baseline results demand for enriching existing models to attain better state-of-the-art.	dummyTurk
2650	0.0	We will include a discussion on potentially fruitful directions of future research.	dummyTurk
2651	0.0	We agree with the assessment of the technical sophistication, but think that we should pursue as simple solutions as possible.	dummyTurk
2652	0.0	Given the task at hand, we argue that this level of sophistication suffices.	dummyTurk
2653	0.0	The predictions of human concreteness and imageability ratings are useful primarily in two scenarios: (1) psycholinguistic research (preparing stimuli for experiments etc.)	dummyTurk
2654	0.0	and (2) NLP applications such as metaphor detection.	dummyTurk
2655	0.0	Both applications are touched upon in the introduction.	dummyTurk
2656	0.0	We will make these more clear.	dummyTurk
2657	0.0	An analysis of predictability of various embedding dimensions was already performed during the experiments and will be briefly presented in the final version of the paper (extra page).	dummyTurk
2658	0.0	"We will try to expand (extra page) on the current description ""trained with fastText (Bojanowski et al., 2016) on Wikipedia dumps, with embedding spaces aligned between languages with a linear transformation learned via SVD on pseudodictionaries of identical words (Smith et al."	dummyTurk
2659	0.0	"2017)"""	dummyTurk
2660	0.0	Regarding the usefulness of standard deviation (SD) in the databases used in the research, higher SD encodes lower agreement among the human responses.	dummyTurk
2661	0.0	Low agreement might be to different reasons, one being lexical ambiguity, another being complexity of the task given.	dummyTurk
2662	0.0	When such databases (either human responses or predictions) are used for preparing psycholinguistic experiments, entries with higher SD are mostly not used, except in specific cases where high response variation is the primary interest.	dummyTurk
2663	0.0	When using such databases for downstream NLP tasks such as metaphor detection, SD can be exploited either to select entries with low SD, or to take varying SD into account as a quantification of uncertainty.	dummyTurk
2664	0.0	For the question of reporting Spearman and not Pearson correlation (although we calculated both and they both bring to the same conclusions), the original data are averages over human Likert scale responses, for which there is an ongoing discussion whether these variables are ordinal or interval, with stronger arguments towards ordinal.	dummyTurk
2665	0.0	Given that ordinal variables encode orderings / ranks and not linear relationships, it is safer to apply Spearman correlation over Likert scale responses.	dummyTurk
2666	0.0	We will add the hyperparameters used in our SVM regressor, as well as share our full code via github together with the predictions in the 78 languages.	dummyTurk
2667	0.0	For the dimensionality of the embeddings, it is stated in the description of the embedding collection used that we refer to in line 198.	dummyTurk
2668	0.0	We agree that this datum (300 dimensions) should be stated directly in the paper and will add it in the final version.	dummyTurk
2669	0.0	W1: The paper should be read and proofread.	dummyTurk
2670	0.0	There are many grammatical inconsistencies.	dummyTurk
2671	0.0	R1: Thanks for your comment.	dummyTurk
2672	0.0	We will ask native speakers to rewrite our paper in the revised process.	dummyTurk
2673	0.0	W2: Figures 2a, 2b are informative.	dummyTurk
2674	0.0	Equally informative would be to include examples where the method fails to capture the relationship or misclassifies it.	dummyTurk
2675	0.0	R2: Thanks for your kind suggestion.	dummyTurk
2676	0.0	Your comment will be fully taken into account of the revised version.	dummyTurk
2677	0.0	W3: The authors do not indicate whether the model/code would be shared with the research community.	dummyTurk
2678	0.0	R3: We will share our model/code online, such as Github.	dummyTurk
2679	0.0	Our initial idea was to fit the model to an existing corpus and generate semantic and coherent text.	dummyTurk
2680	0.0	Future work can be applied to image descriptions, machine translations, and other conditional text generation tasks	dummyTurk
2681	0.0	We will add it in later revisions	dummyTurk
2682	0.0	 The experimental results show that we can reach competitive results with the proposed weighted 2D-seq2seq model compared to a well-tuned attention model.	dummyTurk
2683	0.0	These are preliminary results and need to be fine-tuned.	dummyTurk
2684	0.0	One of the future works is to fine-tune the hyperparameters.	dummyTurk
2685	0.0	Thanks for your constructive suggestions.	dummyTurk
2686	0.0	Our replies are as follows:	dummyTurk
2687	0.0	 It???s true that parallel corpus is supposed to produce better word alignment.	dummyTurk
2688	0.0	But only a few language pairs have substantial parallel corpora.	dummyTurk
2689	0.0	Since the ultimate goal of our work is to extend sememe-based KB to various languages, especially low-source languages, we don???t utilize parallel corpus for Chinese-English pair specifically.	dummyTurk
2690	0.0	Your suggestions about article organization are constructive.	dummyTurk
2691	0.0	We???ll add a schema and the references you mention in the revised version.	dummyTurk
2692	0.0	In Fig.	dummyTurk
2693	0.0	1, the four child sememes are actually modifiers of ???computer???, i.e.	dummyTurk
2694	0.0	???computer??? has the ???PatternValue??? of being ???able??? to ???bring???, and it can also have a SpecificBrand.	dummyTurk
2695	0.0	Thank you for pointing out the typo in line 503 and we???ll correct it when submitting the revised paper.	dummyTurk
2696	0.0	Unfortunately, we can't find any related research regarding echoing problem or something similar.	dummyTurk
2697	0.0	"We can explain the ""novelty"" of this problem as a result of our more sophisticated evaluation process."	dummyTurk
2698	0.0	Most of the prior papers evaluate the quality of their models on samples containing one positive and small (fixed) amount of negative responses (for instance see Ubuntu Dialog Corpus).	dummyTurk
2699	0.0	These negative responses are selected randomly, or extracted by some heuristics, so the chance to pick negative responses very similar to the context is not so high.	dummyTurk
2700	0.0	As for us, we compute the quality across all the given test dataset with hundreds candidate responses, so the chance to meet echo-responses is significant.	dummyTurk
2701	0.0	That's how we came across the echoing problem.	dummyTurk
2702	0.0	Regarding the comparison with intent detection-based systems: it's an interesting question if these systems are exposed to the echoing problem (since usually response structure for each intent is quite fixed).	dummyTurk
2703	0.0	But that's another big idea to study, thanks for your remark.	dummyTurk
2704	0.0	Yes, we understand an importance of evaluation on public and widely used datasets (in particular, we already performed some experiments on Ubuntu Dialog Corpus), but due to limitation on the paper size we could not provide quality metrics, result samples and their interpretation for such datasets.	dummyTurk
2705	0.0	So we decided, on the one hand, to share our own data, on the other hand ??? to provide evaluation results on it.	dummyTurk
2706	0.0	There is information flow between different node types through the shared parametes, but not as much as chain-LSTM structure.	dummyTurk
2707	0.0	which is the feature of Tree-LSTM architecture.	dummyTurk
2708	0.0	It won't lead to data sparsity.	dummyTurk
2709	0.0	In fact, our method only increases a few parameters due to the introduction of tree types.	dummyTurk
2710	0.0	we merged different lexicons and do some process such as duplicate removal and so on.	dummyTurk
2711	0.0	Our method outperforms the result (such as Tree-LSTM, TreeBiGRU)listed in experiment section, which gives exact the example you concerned.	dummyTurk
2712	0.0	if different subtypes of nodes collapseto one, it becomes a typical Tree-LSTM, the result of typical Tree-LSTM is listed in Table 3.	dummyTurk
2713	0.0	Yes, you are right.	dummyTurk
2714	0.0	This work mainly focuses on transferring the relation classification knowledge across languages, rather than an end-to-end system which is completely language independent.	dummyTurk
2715	0.0	The reason might be that BiLSTM is fitter for encoding order information and long-range context dependency for sequence labeling problem, while CNN is suited to extracting local and position-invariant features.	dummyTurk
2716	0.0	For relation classification, the essential features are always distributed around the entity mentions, thus CNN may better utilize this information.	dummyTurk
2717	0.0	On the other hand, the work of ???Yin et al., Comparative study of CNN and RNN for natural language processing??? (Table 1) also demonstrated the conclusion that CNN is slightly better than LSTM for relation classification.	dummyTurk
2718	0.0	Line 235: A pseudo-parallel sentence is the automatically translated text word-by-word by the MT module from English to Chinese.	dummyTurk
2719	0.0	Line 486: The relation types of English dataset is the same as Chinese dataset, while the texts are not translated between English and Chinese.	dummyTurk
2720	0.0	We will clarify these issues in the revised version.	dummyTurk
2721	0.0	Reply-to-weakness-argument#1: ???	dummyTurk
2722	0.0	The confidence score is a multiplicative factor for the learning rate and what we do is tuning the step-sizes dynamically in the backpropagation.	dummyTurk
2723	0.0	This is an elegant way for incorporating the confidence and better learning pace is rooted in the fact that we have a better control on the learning rate (similar to adaptive learning rate: http://proceedings.mlr.press/v28/ranganath13.pdf).	dummyTurk
2724	0.0	We will add a mathematical explanation on what we do and how this helps based on this discussion in the camera-ready.	dummyTurk
2725	0.0	The confidence network receives the weak label as the input besides the main input and aims at learning the ???difference??? between the weak and true label which is simpler than learning to predict the label.	dummyTurk
2726	0.0	In a way, confidence network needs to learn, kind of, residual information about the labels.	dummyTurk
2727	0.0	We will make this point more explicit in the camera-ready version.	dummyTurk
2728	0.0	This is the exact point that we mentioned as the immediate future work in our paper.	dummyTurk
2729	0.0	There are many factors involved like the |V|:|U| ratio, the quality of the weak annotation, etc.	dummyTurk
2730	0.0	We have done some experiments in which we used weak annotators with different quality and we noticed that the if the weak annotations are extremely bad, it hurts the quality of the learned representation and if it is extremely good, there is less room for improvement.	dummyTurk
2731	0.0	We are also aware that a group of researchers who employed our model on the task of intent classification in dialogue systems which is a multi-label classification in which CWS achieves good results.	dummyTurk
2732	0.0	We thank the reviewer for his/her thoughtful remarks.	dummyTurk
2733	0.0	We agree that the automatically built corpus includes noisy samples, which might affect the quality of the intrinsic evaluation.	dummyTurk
2734	0.0	Such a task is actually in need of a manually labeled corpus for accurate intrinsic evaluation.	dummyTurk
2735	0.0	In the absence of a manually labeled corpus, we would like to stress the quality of the available automatically built corpus.	dummyTurk
2736	0.0	This can be deduced from the quality of the word embeddings as shown in Table 1.	dummyTurk
2737	0.0	In addition, we have been using the science-domain word embeddings in our internal experiments for other science-related NLP projects such as question generation for science textbooks and we are seeing significant performance improvements.	dummyTurk
2738	0.0	We accept this point and will work on addressing it in the final version.	dummyTurk
2739	0.0	We have focused our examples on words that have a certain meaning in the science domain and another dominant meaning in other domains.	dummyTurk
2740	0.0	This is the basis for the examples chosen in Table 1, which we found to exhibit the most significant difference in the sample that we examined.	dummyTurk
2741	0.0	Other than that, the process was not very selective but the differences were apparent.	dummyTurk
2742	0.0	Our only concern in expanding to the World Wide Web is the process of automatically scraping the pages--a process that would either require expensive manual labor, or introduce significant noise.	dummyTurk
2743	0.0	Nevertheless, the underlying algorithm is not designed specifically to the content of Wikipedia and should be applicable to any set of text articles of multiple domains.	dummyTurk
2744	0.0	This question might be the result of a misunderstanding.	dummyTurk
2745	0.0	The use of 100 Wikipedia articles as a seed set is an artificial setup that we don???t expect the user to have.	dummyTurk
2746	0.0	In other words, if a domain in Wikipedia is only of size 100, we don???t expect the user to use any of these articles as a seed set.	dummyTurk
2747	0.0	We expect the user to have a certain set of articles pertinent to his/her application collected from somewhere other than Wikipedia, and would only use Wikipedia to augment that initial corpus.	dummyTurk
2748	0.0	Addressed above in reply to weakness argument 3.	dummyTurk
2749	0.0	The baseline is calculated with the balanced log scores (no model involved), and the balanced log scores are also used as a feature in our model.	dummyTurk
2750	0.0	In the ablation studies, ???only log score??? refers to our model using the balanced log score as the only feature.	dummyTurk
2751	0.0	The non-balanced log score was not used.	dummyTurk
2752	0.0	We will make this point clearer in the paper.	dummyTurk
2753	0.0	 The majority baseline obtains 0.54 for formality and 0.55 for complexity (class-based accuracy, line 426).	dummyTurk
2754	0.0	Indeed, we refrain from making promises that we are not sure we can keep.	dummyTurk
2755	0.0	We will work internally to publish the evaluation sets, consisting in: evaluation phrases; gold modifier constituents; annotated interpretations.	dummyTurk
2756	0.0	The outcome is not guaranteed, given that the evaluation phrases are search queries and given recent news-grabbing events in the larger area of user privacy.	dummyTurk
2757	0.0	We do not expect the data source of ~1 billion search queries to be released in the near or medium future.	dummyTurk
2758	0.0	As noted earlier, especially in light of recent events and discussion around user privacy, it would be unreasonable to expect the large set of queries to be released any time soon.	dummyTurk
2759	0.0	The size of the evaluation sets is in fact larger than the evaluation sets used previously in tasks related to the analysis of compositional noun phrases or concepts.	dummyTurk
2760	0.0	It would be nice if there were some resource, from which gold interpretations could be collected automatically, or at least that would reduce the manual effort involved in creating evaluation sets.	dummyTurk
2761	0.0	Just as others failed to find such a resource, we have too so far.	dummyTurk
2762	0.0	For single-modifier interpretation, candidate interpretations are ranked in two stages.	dummyTurk
2763	0.0	The first stage has priority over the second stage.	dummyTurk
2764	0.0	In the first stage, the interpretations are ranked in increasing order of penalty scores, with lower penalty scores being better.	dummyTurk
2765	0.0	In the second stage, interpretations whose penalty scores are identical are ranked among one another in decreasing order of their frequency scores, with higher frequency scores being better.	dummyTurk
2766	0.0	Interpretations with a lower penalty score will be ranked higher (better) overall than interpretations with a higher penalty score, regardless of their frequency score.	dummyTurk
2767	0.0	The penalty score of a candidate interpretation estimates the number of extraneous tokens that appear in the candidate interpretation.	dummyTurk
2768	0.0	"For the phrase ""failed banks"", a candidate interpretation ""banks failed in 2009"" contains the token ""2009"", which is extraneous in the sense that it might cause the candidate interpretation to be too specific, and therefore not that relevant - or irrelevant - for the given phrase."	dummyTurk
2769	0.0	Fewer extraneous tokens is better.	dummyTurk
2770	0.0	The frequency score of a candidate interpretation is the sum of the frequencies of queries that contribute to extracting a candidate interpretation.	dummyTurk
2771	0.0	  Thanks for pointing it out.	dummyTurk
2772	0.0	The points raised are typos/description mistake.	dummyTurk
2773	0.0	L(s, \pi) should be L(u, \pi) and the suggested cross-entropy equations for eqn-6 and eqn-7 are the correct ones, which we followed in our implementation.	dummyTurk
2774	0.0	We will rectify these typos in our final draft.	dummyTurk
2775	0.0	  The vocab size used in our experiment was 30 and maximum length of utterance is set to 10.	dummyTurk
2776	0.0	We played with the simulator and trained and evaluated the system at the same time.	dummyTurk
2777	0.0	At each RL step, user issues an utterance based on the current observation/image (state of the car in the simulator) and the proposed LG-DDPG algorithm interprets the utterance to take an appropriate action.	dummyTurk
2778	0.0	This leads to a new state (observation/image) and user assesses whether the chosen action is the intended one or not and provides feedback accordingly.	dummyTurk
2779	0.0	The goal of the proposed framework is to leverage human guidance in accelerating RL learning, where through utterance and feedback, the human user conveys the notion of permissible and non-permissible actions to the system.	dummyTurk
2780	0.0	This causes reduction in exploration space for actions and speeds up RL.	dummyTurk
2781	0.0	The objective requires grounding the utterance to action space through which the information is conveyed to the system.	dummyTurk
2782	0.0	For our lane keeping task, we tested the proposed method with 40 various utterances (implicit/explicit steering control instructions with paraphrasing) and found that the proposed language interpreter learns quite accurately and helps to guide the exploration, which is also reflected in our results.	dummyTurk
2783	0.0	For solving other RL tasks that require simpler or more complex language grounding, we can modify the language interpreter architecture accordingly as per requirements.	dummyTurk
2784	0.0	However, our overall proposed concept of leveraging language guidance for speeding up RL remains general irrespective of applications.	dummyTurk
2785	0.0	We will provide more analysis in our final draft.	dummyTurk
2786	0.0	 We will incorporate the suggested papers in our final draft.	dummyTurk
2787	0.0	The parsing community has been overly-reliant on a single test set, WSJ23.	dummyTurk
2788	0.0	In machine translation, there are new test sets every year from venues like IWSLT and WMT, so reliance on a single test set is less of an issue in that community.	dummyTurk
2789	0.0	We agree that there are many problems with BLEU as a metric.	dummyTurk
2790	0.0	For this reason, we tried in the paper to side-step the question of whether BLEU is good or not, and focus only on a narrower question: if a researcher decides to use BLEU, how should she compute it?	dummyTurk
2791	0.0	A convention is needed so that numbers can be compared, and so that we are not adding further problems to BLEU.	dummyTurk
2792	0.0	(We have however added chrF to our tool, which is an easy-to-compute metric similar in spirit to BLEU but which works at the character level and is therefore better for morphologically rich languages.	dummyTurk
2793	0.0	Other metrics could be added but we have no immediate plans to do so.)	dummyTurk
2794	0.0	We have not added significance tests, but this is something we could easily do.	dummyTurk
2795	0.0	 Reviewer 3 raises a good point that has also been raised by Reviewer 1.	dummyTurk
2796	0.0	 We agree that the BiLSTM may have an advantage on large data tasks.	dummyTurk
2797	0.0	We should make this observation more explicit.	dummyTurk
2798	0.0	 We agree with the reviewer that our discussion of the GO token could be improved.	dummyTurk
2799	0.0	Our main point in this work is that a models with simple architectures can outperform complex architectures.	dummyTurk
2800	0.0	The fact that we need this additional GO token does not contradict this point.	dummyTurk
2801	0.0	Although we agree that further investigation is necessary to fully understand its role in the decoding process.	dummyTurk
2802	0.0	We will expand Section 2.3 with more description and examples.	dummyTurk
2803	0.0	 We agree that our description of the GO token is unclear, please see response given to Reviewer 2.	dummyTurk
2804	0.0	Thank for your useful suggestions, we address you questions as follows:	dummyTurk
2805	0.0	For weakness1: To verify the influence of pre-trained word embeddings, we use the same model to conduct experiments with random initialized word embeddings.	dummyTurk
2806	0.0	The results show that word embeddings improve performances about 0.3%~0.6% and 0.9%~1.4% on English and Spanish dataset respectively.	dummyTurk
2807	0.0	The performance of our model also demonstrated that our model can perform well without pre-trained word embeddings.	dummyTurk
2808	0.0	We will clarify the influence of word embeddings in revised version.	dummyTurk
2809	0.0	For weakness2: We will give more details later in revised version.	dummyTurk
2810	0.0	"In fact, ""Neutral baseline"" is a CRF model with word embeddings as features and the ""Discrete baseline"" is a CRF model with four kinds of features such as surface features, linguistic features, cluster features and sentiment features, described in (Zhang et al., 2015)."	dummyTurk
2811	0.0	For weakness3: When obtaining the target and sentiment labeling score matrix, we use Viterbi algorithm to find the path with the highest score in the score matrix.	dummyTurk
2812	0.0	The optimal path stands for the label of each word.	dummyTurk
2813	0.0	We will add these details of finding optimal path when space is allowed.	dummyTurk
2814	0.0	For question1: Thanks for your suggestion.	dummyTurk
2815	0.0	We will add more details and citation for baselines.	dummyTurk
2816	0.0	For question2: This is a good question.	dummyTurk
2817	0.0	"We have used  ""SBi-LSTM"", ""S-GRU"" and ""Bi-GRU"" to conduct experiments."	dummyTurk
2818	0.0	"The results of ""SBi-LSTM"" are almost the same as SBi-GRU, but LSTM has more parameters than GRU."	dummyTurk
2819	0.0	Therefore, we chose GRU rather than LSTM.	dummyTurk
2820	0.0	"The performances of ""S-GRU"" are worse than bidirectional model, and the reason is that bidirectional model can capture past and future features, such as boundary features."	dummyTurk
2821	0.0	"As for ""single layer"" model, many works have demonstrated the effectiveness of ""stack RNN"", e.g."	dummyTurk
2822	0.0	"""Speech Recognition with Deep Recurrent Neural Networks; Graves et al."	dummyTurk
2823	0.0	"(2013)"", because ???stack RNN??? can build up progressively higher level representations of sequence data."	dummyTurk
2824	0.0	"We also find that ""SBi-GRU"" outperforms ""Bi-GRU"", so we chose 'multi-layer' model."	dummyTurk
2825	0.0	Because of limited space, we don't show the results of those variants.	dummyTurk
2826	0.0	We will add them in the extra page if this paper is accepted.	dummyTurk
2827	0.0	We will incorporate a detailed contrast of our work to the pointers you sent to us.	dummyTurk
2828	0.0	- Li et al.	dummyTurk
2829	0.0	(2016) model sequence tag prediction and sequence labeling as adversarial model parts.	dummyTurk
2830	0.0	This is not applicable to domain adaptation or cross-lingual settings.	dummyTurk
2831	0.0	- Yasunaga et al.	dummyTurk
2832	0.0	(2017) use adversarial training in the form of perturbations of word embeddings.	dummyTurk
2833	0.0	They do not use adversarial loss functions with a discriminator model part.	dummyTurk
2834	0.0	We instead use adversarial loss functions for cross-lingual sequence tagging.	dummyTurk
2835	0.0	As you pointed out, this is the first time to use adversarial loss for cross-lingual sequence tagging.	dummyTurk
2836	0.0	We will clarify our statements in the paper.	dummyTurk
2837	0.0	Indeed, in this first study on the utility of adversarial domain adaptation for sequence-level prediction tasks we opt for a language pair from the same family, but the algorithm is general enough to be applicable to language pairs also from different language groups.	dummyTurk
2838	0.0	We leave it for the future work and will add a discussion on this.	dummyTurk
2839	0.0	We explained the results as much as possible given the space constraints of a short paper.	dummyTurk
2840	0.0	We will be happy to use the additional page of the camera-ready version if the paper gets accepted to extend our explanations.	dummyTurk
2841	0.0	(please also see our response to Question 2).	dummyTurk
2842	0.0	For the dataset we used, 0.5-1% absolute LAS is usually reported as a significant improvement.	dummyTurk
2843	0.0	So, we assume that the performance difference is significant but will check that in more detail for the camera-ready version.	dummyTurk
2844	0.0	In GAN and WGAN, the generator is updated by maximizing the likelihood of the discriminator assigned to the wrong language id.	dummyTurk
2845	0.0	We found that in practice -- even with careful tuning -- the discriminator can easily correctly predict the language id, which results in vanishing gradient for the generator, e.g., when the discriminator becomes too strong, the generator stops learning (a common stabilization problem in GAN learning).	dummyTurk
2846	0.0	In contrast, in GR we simply take the inverse of the gradient w.r.t.	dummyTurk
2847	0.0	to maximizing the probability of the correct language id, which perhaps results in better training signal for the generator.	dummyTurk
2848	0.0	We argue that semantic compression is a semantic task because the system needs to consider the meaning of words and the meaning of the sentence.	dummyTurk
2849	0.0	This is independent of the modeling as a sequence-tagging task.	dummyTurk
2850	0.0	We are sure that the system does not only use POS to solve the task because we ran an experiment without words which resulted in a large performance drop.	dummyTurk
2851	0.0	Moreover, Andor et al.	dummyTurk
2852	0.0	(2016) demonstrated that extractive sentence compression benefits the most from capturing long-range dependencies with global training objectives, which suggests higher complexity of the task with strong semantic constraints on the output label sequence to produce coherent and informative summary.	dummyTurk
2853	0.0	We will integrate your suggestions in the final version of the paper.	dummyTurk
2854	0.0	Here are answers to your questions:	dummyTurk
2855	0.0	- POS taggers: We use a simple biLSTM taggers trained on the full Fr or Es dataset.	dummyTurk
2856	0.0	- Bilingual word embeddings: The bilingual word embeddings are built based on contextual information, dependency information and word alignment between the two languages, as proposed by Soricut and Ding (2016).	dummyTurk
2857	0.0	- Size of test set for DP: The test set has 7,018 tokens.	dummyTurk
2858	0.0	- Monolingual FR results for DP: Using the full FR training set, we get a score of 80.21.	dummyTurk
2859	0.0	Thus, adding ES data with adversarial training improves this result.	dummyTurk
2860	0.0	 We thank the reviewer???s suggestion on providing an estimation of the value of the context-sense dot product at saturation!	dummyTurk
2861	0.0	We???ll add a curve of this in Figure 2 to corroborate our explanation in Footnote 2.	dummyTurk
2862	0.0	Apologies for the confusion regarding the temperature.	dummyTurk
2863	0.0	SASI also applies a temperature to force a peaked distribution, and all of the results in Tables 1-4 were obtained with a temperature of 0.5.	dummyTurk
2864	0.0	We deleted an example of the prompt for the human evaluation due to lack of space; we will add it back if accepted.	dummyTurk
2865	0.0	 Yes, we tried training with a temperature of 0.01, but it quickly led to gradient explosion.	dummyTurk
2866	0.0	 We did not try using the straight-through Gumbel-softmax since we wanted to use the true gradients for attention.	dummyTurk
2867	0.0	Thanks for the suggestion; we will think about adding a comparison if accepted.	dummyTurk
2868	0.0	Currently, we have inter-annotator agreement for the transcription and code-recode reliability for each layer of the annotation.	dummyTurk
2869	0.0	An annotation platform is being built to facilitate the annotation process.	dummyTurk
2870	0.0	Inter-annotator agreement of the annotation will be presented later.	dummyTurk
2871	0.0	Our objective is to introduce a new theory of conversational structures and actions that is distinctively different from and complementary to the existing ones.	dummyTurk
2872	0.0	Also, we strive to exemplify how this theory can be used in annotating conversational data in practice.	dummyTurk
2873	0.0	It aims to contribute to CL research on conversational understanding by large.	dummyTurk
2874	0.0	We appreciate this suggestion and will compare our work to the PDTB.	dummyTurk
2875	0.0	We respectfully disagree with the reviewer on this point.	dummyTurk
2876	0.0	The multi-layer hierarchical conversational structure is at the core of our work.	dummyTurk
2877	0.0	The inter-annotator agreement for the transcription is calculated with character error rate (298-299), similar to word error rate (Klakow & Peters, 2002).	dummyTurk
2878	0.0	Currently, we have inter-annotator agreement for the transcription and code-recode reliability for each layer of the annotation.	dummyTurk
2879	0.0	An annotation platform is being built to facilitate the annotation process.	dummyTurk
2880	0.0	Inter-annotator agreement of the annotation will be presented later.	dummyTurk
2881	0.0	Our objective is to introduce a new theory of conversational structures and actions that is distinctively different from and complementary to the existing ones.	dummyTurk
2882	0.0	Also, we strive to exemplify how this theory can be used in annotating conversational data in practice.	dummyTurk
2883	0.0	It aims to contribute to CL research on conversational understanding by large.	dummyTurk
2884	0.0	???Sequence organization??? and ???action formation??? in CA are roughly equivalent to ???conversational structures??? and ???conversational actions???.	dummyTurk
2885	0.0	We intend to relate the concepts in sociology to those in CL.	dummyTurk
2886	0.0	An occasion of conversation is one complete conversation (beginning to end.)	dummyTurk
2887	0.0	Yes, we followed the CA conventions.	dummyTurk
2888	0.0	Besides verbatim, our transcription contains intonation and non-verbal activities information.	dummyTurk
2889	0.0	The corpus was originally constructed for a CA study of medical communication behaviors; we then developed the annotation scheme based on the corpus.	dummyTurk
2890	0.0	It should be noted that although CA transcribing conventions allows analysts to capture a wide range of speech production features, it does not attempt to capture all the aspects in one transcript.	dummyTurk
2891	0.0	The granularity of the transcription depends on the objective of the study.	dummyTurk
2892	0.0	For example, if the study objective is to analyze the role of pitch in action formation, the transcripts are produced with a heavier emphasis pitch.	dummyTurk
2893	0.0	If a study aims to analyze the role of grammatical formats of some actions (e.g., interrogative vs. declarative form of request), the transcripts are produced with special concern with this respect.	dummyTurk
2894	0.0	Our study is of the latter type.	dummyTurk
2895	0.0	Please refer to our reply to weakness argument 2.	dummyTurk
2896	0.0	'Sequence' is defined as a course of action, with one base adjacency pair and its expansions (162-166).	dummyTurk
2897	0.0	We believe that the distinction between ???sequence??? and ???overall organization??? and the link between ???adjacency pair??? and ???sequence??? can be clarified by adding a figure for demonstration.	dummyTurk
2898	0.0	Since our scheme aims to capture the organization of a complete conversation which usually involves a long stretch of talk, we were unable to do so in the submitted version.	dummyTurk
2899	0.0	We will include a figure to facilitate our textual description in the final version.	dummyTurk
2900	0.0	The idea that ???a coherent sequence for a course of action is composed of one base adjacency pair and its expansions??? is explained through an example in Table 1 (TID 60 to 63).	dummyTurk
2901	0.0	Again, we believe a figure can help us clarify this idea.	dummyTurk
2902	0.0	We appreciate the reviewer's note on the distinction between domain-specific and task-specific.	dummyTurk
2903	0.0	The PS layer is task-specific in the domain of medical conversation.	dummyTurk
2904	0.0	We annotated every physician-initiated treatment recommendation (B) in each medical consultation, including both antibiotic type and non-antibiotic type.	dummyTurk
2905	0.0	Q9.	dummyTurk
2906	0.0	Corpus Statistics	dummyTurk
2907	0.0	Please refer to our reply to weakness argument 2.	dummyTurk
2908	0.0	Since we are expanding our corpus size and will annotate our new data with the proposed scheme, being able to receive feedback from the ACL community will be extremely helpful.	dummyTurk
2909	0.0	Q10.	dummyTurk
2910	0.0	541-544	dummyTurk
2911	0.0	We appreciate that the reviewer pointed this error out.	dummyTurk
2912	0.0	The correct citation is Robinson (1999).	dummyTurk
2913	0.0	Robinson, J. D. (1999).	dummyTurk
2914	0.0	The organization of action and activity in primary-care, doctor-patient consultations (Ph.D. Dissertation).	dummyTurk
2915	0.0	University of California, Los Angeles, California, Los Angeles, California, USA.	dummyTurk
2916	0.0	We will change the legend into ???# of action sequences in a phase???.	dummyTurk
2917	0.0	Indeed.	dummyTurk
2918	0.0	Explicit requests for antibiotic prescriptions can be perceived as a direct challenge to physicians' medical authority, thus are rarely used in real medical conversation.	dummyTurk
2919	0.0	In our opinion, this is what makes the study of caregiver advocating strategy and its impact on antibiotic over-prescribing interesting.	dummyTurk
2920	0.0	Similar findings are reported in American pediatric setting ??? explicit requests are rarely used (Stivers, 2002).	dummyTurk
2921	0.0	We do not understand this question very clearly.	dummyTurk
2922	0.0	However, the turn following caregiver's advocacy that clearly indicates the physician???s willingness to prescribe antibiotics is annotated as acceptance; otherwise, a non-acceptance is annotated.	dummyTurk
2923	0.0	We acknowledge that many of the existing schemes can integrate long-distance link between a pair of turns (670-676).	dummyTurk
2924	0.0	Thus, our proposed scheme (esp.	dummyTurk
2925	0.0	the adjacency pair layer) is in line with the prior work.	dummyTurk
2926	0.0	However, the proposed scheme adds to the existing work by capturing two additional higher-level structural organizations of conversation (i.e., sequence and overall organization).	dummyTurk
2927	0.0	We are grateful to the reviewer for pointing us to Hardy et al.	dummyTurk
2928	0.0	(2003) and Moeshler???s work.	dummyTurk
2929	0.0	Since Hardy et al.	dummyTurk
2930	0.0	(2003) and several other studies adopted the DAMSL scheme (Core and Allen, 1997; Jurafsky et al.	dummyTurk
2931	0.0	1997), which was partially based on the discourse structure theory proposed by Grosz and Sidner (1986), we decided to cite the original papers of the DAMSL scheme and discourse structure theory.	dummyTurk
2932	0.0	We are happy to include Hardy et al.	dummyTurk
2933	0.0	(2003) and Moeshler's work in the final version.	dummyTurk
2934	0.0	Thanks for your constructive and thoughtful comments!	dummyTurk
2935	0.0	We realized that we have caused a confusion by missing out some details.	dummyTurk
2936	0.0	We actually did not tune on test.	dummyTurk
2937	0.0	There are five folds in our cross validation setting, which strictly follow Peng et al., (2017).	dummyTurk
2938	0.0	Their corpus does not contains a held-out devset in each fold.	dummyTurk
2939	0.0	Therefore, for the first fold, we take 200 sentences out of the training data as the devset, using the remaining for development training.	dummyTurk
2940	0.0	In the end the 200 sentences are merged back into the training set.	dummyTurk
2941	0.0	The number of iterations is decided on the devset, and used for all five folds.	dummyTurk
2942	0.0	For all the other hyper parameters, we simple followed Peng et al., (2017) and did not do tuning.	dummyTurk
2943	0.0	Below are the numbers of different transition iterations on the devset of that fold.	dummyTurk
2944	0.0	#transition iter: 1  2  3  4  5	dummyTurk
2945	0.0	#F1: 72.0  77.5  80.5  84.0  84.5	dummyTurk
2946	0.0	We used this fold for showing the effectiveness of iterations and single-direction information exchange, where the figure is drawn on the test portion of that fold.	dummyTurk
2947	0.0	We mentioned that ``we used 5 as the number of iterations for remaining experiments'' at the end of the section, which sounded as if we tuned on test.	dummyTurk
2948	0.0	We did not realize this confusing statement in our draft.	dummyTurk
2949	0.0	We will give all the above details in our revision!	dummyTurk
2950	0.0	We will try to play down the claims that you mentioned.	dummyTurk
2951	0.0	The title is inherited from  Peng et al., (2017) for consistency.	dummyTurk
2952	0.0	The task we try to solve pre-specifies the arguments (Section 3), while our model can actually be used for cases with flexible (with a maximum), if a mask representing the number of entities are applied to the features (input of logistic regression).	dummyTurk
2953	0.0	"The possible relations {""resistance or non-response"", ""sensitivity"", ""response"", ""resistance"", ""None""} are defined in Peng et al., (2017), and we follow them to group the first four as positive and treat ""None"" as negative."	dummyTurk
2954	0.0	Thus the problem is a binary classification problem, and accuracies are classification precisions.	dummyTurk
2955	0.0	We will add the details to our draft.	dummyTurk
2956	0.0	The dataset is also available online.	dummyTurk
2957	0.0	We will expand the related work section as you suggest.	dummyTurk
2958	0.0	[1] is an interesting paper, and we will to cite and compare with it in the next version.	dummyTurk
2959	0.0	We agree that replicability is a concern - we hope to release the annotated dataset that is associated with the Children???s book test of Weston et al.	dummyTurk
2960	0.0	(2015).	dummyTurk
2961	0.0	We are in the process of getting permission to release the section of the dataset from the proprietary source and will release that if possible.	dummyTurk
2962	0.0	Yes, the dataset is small.	dummyTurk
2963	0.0	Please note that the bottleneck for this work was the human annotations (there???s no shortage of stories).	dummyTurk
2964	0.0	Given the proof of concept in this paper, we hope future work will invest in larger datasets for training the ranking models.	dummyTurk
2965	0.0	We agree that going beyond single-sentence questions (an assumption we borrowed from Heilman and Smith) would be a great direction that could lead to more interesting and useful questions, especially if we advanced our models to explicitly locate answers when formulating the question (which Heilman did for non-Why questions).	dummyTurk
2966	0.0	This is a great direction for future work.	dummyTurk
2967	0.0	The Authors propose to improve the efficiency of text categorization algorithms by appropriate clustering/embedding of words.	dummyTurk
2968	0.0	In this way, new unseen words from test data could be processed.	dummyTurk
2969	0.0	This is an interesting work that goes in the direction as the original word embedding approach.	dummyTurk
2970	0.0	The proposed approach is interesting and shows improvement of the results.	dummyTurk
2971	0.0	Yet, given what is said by the Authors in the introduction, in the evaluation, the comparison should be done between word embedding clusters and Authors' clusters, while what is done is the comparison between no clusters (no substitutions) and Authors' clusters.	dummyTurk
2972	0.0	Response:  The novelty of our method lies in combining semantic (word embedding model based) and distributional (word frequency based) clustering.	dummyTurk
2973	0.0	As we discuss this in Section 2 both approach has been done before separately, but never combined.	dummyTurk
2974	0.0	We see you point that it may have been valuable to compare our results to the word embedding clusters as well.	dummyTurk
2975	0.0	We actually did that for ourselves (this is where the distributional idea came from), however due to the small page limit of the submission, we were not able to include it in the paper.	dummyTurk
2976	0.0	This is the main weakness of the work.	dummyTurk
2977	0.0	It is difficult to assess it even if the argumentation and demonstration are impressive.	dummyTurk
2978	0.0	I have other questions and comments:	dummyTurk
2979	0.0	"???	what is the role of the Google embeddings?"	dummyTurk
2980	0.0	Response:  We used the Google embeddings to generate the semantic clusters.	dummyTurk
2981	0.0	Generally, any word embedding model trained on any comprehensive data set is possible.	dummyTurk
2982	0.0	We selected the most well-known combination.	dummyTurk
2983	0.0	"???	in addition to the citation (Ma and Zhang, 2015), the Authors should also cite other existing works that indicate that there is an improvement in text categorization when word embeddings are exploited."	dummyTurk
2984	0.0	Response:  In Section 2.2 there is another paper which exploit word embedding models.	dummyTurk
2985	0.0	We argue that  novelty of the problem and topic (user-specific signals) explains the scarcity of related work.	dummyTurk
2986	0.0	"???	the Authors should indicate that ""words"" and ""terms"" are interchangeable, or (whoch is better) prefer using ""words""."	dummyTurk
2987	0.0	"Similarly, ""term extraction"" refers to another goals and applications."	dummyTurk
2988	0.0	Response:  We deemed these two words interchangeable in the paper, however it is a valid point improving clarity.	dummyTurk
2989	0.0	"???	concerning the second reason in section 3.1.1 on task-specific synonyms, does it mean that Authors would like to use the same clusters for different tasks?"	dummyTurk
2990	0.0	In that way, how does it combine with the proposed methods in which positive and negative examples are processed differently?	dummyTurk
2991	0.0	These examples are specific to a given task.	dummyTurk
2992	0.0	Response: We used the same semantic (based on the Google embeddings), but task-specific distributional information to create the clusters.	dummyTurk
2993	0.0	This is actually the main idea in our method, that when the semantic clustering is not enough, than distributional information may improve the clustering.	dummyTurk
2994	0.0	 Prerequisite relations essentially can be considered as the learning dependency among concepts, which means a concept is necessary to learn before one can proceed to understand more advanced knowledge.	dummyTurk
2995	0.0	"For example, we should learn ""binary tree"" before ""AVL tree"" when learning data structure."	dummyTurk
2996	0.0	 MOOC proposed semantic, contextual and structural features for concept pairs in Massive Open Online Courses, and then trained binary classifiers such as Na??ve Bayes, Logistic Regression, SVM with linear kernel and Random Forest for the relation learning.	dummyTurk
2997	0.0	CPR-Recover proposed an unsupervised optimization-based method based on causality and sparsity assumptions to learn concept-level prerequisite relations from course dependencies.	dummyTurk
2998	0.0	 We will proofread the paper carefully.	dummyTurk
2999	0.0	 Some notations were used in a previous work.	dummyTurk
3000	0.0	We will clearly state and highlight the novel contribution of our work.	dummyTurk
3001	0.0	 Our approach can find more prerequisite relations among concepts with the aid of the recovery-based model, but it would also introduce a few noisy concept pairs.	dummyTurk
3002	0.0	 In future, we would like to directly learn the prerequisite relations among concepts with an end-to-end learning system.	dummyTurk
3003	0.0	 Term definitions can something like	dummyTurk
3004	0.0	 We would say	dummyTurk
3005	0.0	 Indeed our related work needs to be improved.	dummyTurk
3006	0.0	Thanks	dummyTurk
3007	0.0	 Good suggestion.	dummyTurk
3008	0.0	"Actually we are doing a follow-up work in textual entailment, by incorporating lexicon definitions, so that ""shoot"" may entail ""kill"" in sentence context."	dummyTurk
3009	0.0	 Most prior (unsupervised) work learn word representations on specific domains, so, limited generalization, supervised work mostly train the system merely on the training set of a specific benchmark, so this trained classifier can not generalize to new benchmark.	dummyTurk
3010	0.0	We use definitions which are agnostic to domains and benchmarks, so it is supposed to generalize well	dummyTurk
3011	0.0	 The definitions in our experiments come from WordNet and Wikipedia.	dummyTurk
3012	0.0	 Trying on more advanced SNLI entailment systems on hypernym detection problem could be interesting.	dummyTurk
3013	0.0	We included the motivation of using AttentiveConv in the first paragraph of Section 3.2.	dummyTurk
3014	0.0	We will include the results of some top SNLI systems next	dummyTurk
3015	0.0	As for the  reviewer's remaining concerns, we really appreciate them, very insightful and constructive.	dummyTurk
3016	0.0	We will definitely work hard to handle them.	dummyTurk
3017	0.0	We are aware that currently some potential readers are not familiar with Reinforcement Learning (RL), thus we provided a mapping between RL terminology and NLP/Information Extraction terminology in Table 1 and attempted to bridge the gap.	dummyTurk
3018	0.0	We will further replace RL terms with expressions more accessible to NLP readers.	dummyTurk
3019	0.0	Along with writing, we will further elaborate the explanations on figures and polish these figures to improve readability.	dummyTurk
3020	0.0	Clarification on line:60:90: In the ???campaign??? examples, supervised approaches tend to classify both as triggers as Attack type because there are more Attack-???campaign??? instances than Demonstrate-???campaign??? instances in the corpus.	dummyTurk
3021	0.0	Human readers??? decisions will not be biased by such distribution and they can correctly determine a Demonstrate-???campaign??? regardless of its less frequent occurrence.	dummyTurk
3022	0.0	We assume that human readers put ???weights??? on ambiguous cases and we model such a weight as a ???reward???.	dummyTurk
3023	0.0	Motivation: Our motivation of adopting IRL/GAIL is to tackle ambiguous cases.	dummyTurk
3024	0.0	Ambiguous cases are impacted by biased label/feature distribution (e.g., in ACE2005 training split: Attack-???campaign???:Demonstrate-???campaign???=2:1), so some supervised models introduce contextual or external features/data and attempt to indirectly influence ambiguous cases with changes in distribution.	dummyTurk
3025	0.0	In our work, instead of focusing on distribution, we directly assess ambiguous cases with a dynamic mechanism -- in Demonstrate-???campaign??? example, we penalize wrong Attack label with strong negative reward and promote correct Demonstrate label with strong positive reward.	dummyTurk
3026	0.0	"GAN: In [Goodfellow2014], a well-optimized discriminator maximizes the margins between estimated probabilities with regard to real and fake images and stimulates the generator to generate images as ""real""."	dummyTurk
3027	0.0	In our work, the discriminator assesses labels from ground-truth (and correct labels from the extractor) with high rewards and wrong ones with low rewards; and the discriminator is iteratively optimized.	dummyTurk
3028	0.0	For ambiguous cases, if the extractor repeatedly attempts with wrong labels, the discriminator outputs lower and lower rewards and eventually stimulates the extractor to output correct labels.	dummyTurk
3029	0.0	We also elaborated the discriminator???s output with the Demonstrate-???campaign??? example in Line:674:686	dummyTurk
3030	0.0	We appreciate the recommendation on other competing approaches and we recognize them as top approaches.	dummyTurk
3031	0.0	Some of them have been cited in our submission.	dummyTurk
3032	0.0	Our goal is to develop an end-to-end event extraction framework.	dummyTurk
3033	0.0	For fair comparison, we require a competitive approach which labels both triggers and argument roles and is trained upon the same data split.	dummyTurk
3034	0.0	[Yang2017,Feng2016,Nguyen2018] only detected event triggers.	dummyTurk
3035	0.0	Therefore, we choose [Li2013], which is a non-Neural Networks (NN) approach that achieves the best performance, and [Nguyen2016], which is a state-of-the-art NN approach, for the setting of perfect entity annotation.	dummyTurk
3036	0.0	And [Li2014] is the best choice for settings with system generated entities.	dummyTurk
3037	0.0	Moreover, as mentioned in line:628:630, we tune the parameters according to F1 scores of argument role labeling instead of trigger labeling to align with [Li] papers and [Nguyen2016].	dummyTurk
3038	0.0	We acknowledge the trade-off in the scores among different concentrations and that we need to further elaborate and clarify these differences.	dummyTurk
3039	0.0	Given more space we will provide more comprehensive comparison and analysis on other datasets as suggested.	dummyTurk
3040	0.0	We will release source code and make resources publicly available.	dummyTurk
3041	0.0	See reply to weakness argument 2.	dummyTurk
3042	0.0	See reply to weakness argument 2.	dummyTurk
3043	0.0	See reply to weakness argument 2.	dummyTurk
3044	0.0	RL resembles seq2seq in terms of considering historical outputs.	dummyTurk
3045	0.0	At least in our opinion, a core step to apply RL on other tasks is to re-define loss functions into reward functions.	dummyTurk
3046	0.0	Thanks for your meticulous comments and valuable time in making our work thorough.	dummyTurk
3047	0.0	We address reviewer-specific clarifications below.	dummyTurk
3048	0.0	W1: This technique is actually required in our real QA systems since a user-friendly human-computer interface not only needs to precisely retrieve the answers but also interprets the process to users in human language.	dummyTurk
3049	0.0	In this way, the interpretation can help users better understand the correctness of returned answers.	dummyTurk
3050	0.0	In fact, we have found many potential applications of our model such as generating short news report from structured sports game data and Table-to-Text generation (Sha et al.	dummyTurk
3051	0.0	AAAI 2018, Liu et al.	dummyTurk
3052	0.0	AAAI 2018).	dummyTurk
3053	0.0	Moreover, in existing NLP community, many approaches represent a text as either a sequence (seq-LSTM) or a tree (tree-LSTM).	dummyTurk
3054	0.0	With our graph encoder,	dummyTurk
3055	0.0	a text can be represented as a more complex graph, e.g., the combination of word sequence and dependency parse tree and constituent tree.	dummyTurk
3056	0.0	We think our approach provides a solution to simultaneously learn these different structured features in a neural network model.	dummyTurk
3057	0.0	W2: We tune hyper-parameters on the validation set.	dummyTurk
3058	0.0	W3: In fact, we consider these logical connectors in our approach.	dummyTurk
3059	0.0	Specifically, we add nodes in the graph to represent these connectors.	dummyTurk
3060	0.0	W4: We use Seq2Seq models as baselines since they have been recently proved to be able to capture some structured information, such as Tree or Table.	dummyTurk
3061	0.0	We compare our model with these models to demonstrate that these seq2seq methods may suffer information loss when imposing an order on graph structured data.	dummyTurk
3062	0.0	W5: We did manually evaluate the correctness of generated questions.	dummyTurk
3063	0.0	Since it is impractical to manually evaluate the correctness of all generated questions, we carefully analyzed 300 generated questions as mentioned in Error Analysis (session 5.6).	dummyTurk
3064	0.0	Among them, 224 questions (74.7%) correctly described the meanings of SQL queries.	dummyTurk
3065	0.0	Q1: Thanks for pointing out this interesting direction.	dummyTurk
3066	0.0	In fact, we are wording on a seq2graph project.	dummyTurk
3067	0.0	We would like to share some observations with you.	dummyTurk
3068	0.0	From current results we have, it seems more difficult to decode a graph than a sequence, because there is only one way to generate a sequence but may have numerous ways to generate a graph.	dummyTurk
3069	0.0	This makes it difficult to train the seq2graph model.	dummyTurk
3070	0.0	We are still working on this and hopefully, we could find some ways to address this issue.	dummyTurk
3071	0.0	Additional Comment: Thanks very much for your kind suggestion about the writing.	dummyTurk
3072	0.0	We will polish the paper accordingly.	dummyTurk
3073	0.0	Thanks for your meticulous comments and valuable time in making our work thorough.	dummyTurk
3074	0.0	We appreciate your detail comments and address reviewer-specific clarifications below.	dummyTurk
3075	0.0	W1: This time complexity of node embedding generation algorithm is |K|*|V|_{avg} where |K| is the hop size and |V|_{avg} is the average neighbor size of nodes in the graph.	dummyTurk
3076	0.0	"For large input graphs that may have large |V|_{avg}, we could use the sampling method introduced in ""FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling"" (Chen et al."	dummyTurk
3077	0.0	ICLR 2018) to guarantee the efficiency of this algorithm on large graphs.	dummyTurk
3078	0.0	W2: Experimental results on the SQL-to-Text could demonstrate our graph encoder is better than the sequence encoder.	dummyTurk
3079	0.0	We conduct this toy experiment to verify whether our pooling-based method that generates the graph embedding still works in the seq2seq models.	dummyTurk
3080	0.0	Q1: We explain graph encoding algorithm using the following simple example.	dummyTurk
3081	0.0	Suppose we have three nodes in a graph whose structure is A -> B -> C. In the first iteration, the information of A is propagated to B (line 5 in Algorithm 1).	dummyTurk
3082	0.0	The embedding of B is then updated (line 7 in Algorithm 1), resulting B_new.	dummyTurk
3083	0.0	In the second iteration, the information of B_new is propagated to C (line 5).	dummyTurk
3084	0.0	Since the B_new already conveys the information of A, the C thus could aggregate A's information into its embedding.	dummyTurk
3085	0.0	We hope this explanation could help you better understand how our algorithm works.	dummyTurk
3086	0.0	If some edges have valuable information, we could also add one node for each edge and still uses the proposed algorithm to generate the node embeddings.	dummyTurk
3087	0.0	Q2: Here, each node has a text attribute that equals to an integer.	dummyTurk
3088	0.0	The graph is in the chain form where each node has at most one parent and one child.	dummyTurk
3089	0.0	Similar to the experiment in Sec.	dummyTurk
3090	0.0	5.5, in the input graph each node has a text attribute that equals to an integer (representing its ID).	dummyTurk
3091	0.0	The expected output is a sequence of integers representing the nodes in the shortest path between START and END node.	dummyTurk
3092	0.0	Additional Comments: Thanks very much for your advice, we may attempt it in the future.	dummyTurk
3093	0.0	Thanks for your comments and suggestions.	dummyTurk
3094	0.0	???stable words??? are those words chosen to make alignment.	dummyTurk
3095	0.0	Their meanings keep stable in a period of time, according to the Historical Thesaurus.	dummyTurk
3096	0.0	We first pick stable words by the frequency in time line and then find stable times according to the Historical Thesaurus to build stable word-time intervals for alignment.	dummyTurk
3097	0.0	We make a bridge between different times by stable words.	dummyTurk
3098	0.0	???road??? is a stable word from 1810s to 1820s.	dummyTurk
3099	0.0	Since it has the same meaning, we can use ???road??? in 1820s to predict the context of ???road??? in 1810s like ???lane???.	dummyTurk
3100	0.0	The aim is to make ???road??? in 1810s and 1820s become closer.	dummyTurk
3101	0.0	We take polysemy of words into consideration by using the multiple time of stable meanings of stable words to build multiple cross-time constraints.	dummyTurk
3102	0.0	The stable words and their time of meanings come from the Historical Thesaurus.	dummyTurk
3103	0.0	Negative sample is widely used in traditional word2vec learning.	dummyTurk
3104	0.0	We use the target words to maximize the co-occurrence of its context, and we should also minimize the relation of those words not appearing in the context.	dummyTurk
3105	0.0	Negative sample reduces the number of negative words to accelerate on constraints 1 and 3.	dummyTurk
3106	0.0	We train corpus from 1710s to 1920s together, each decade holds its own W and b to reflect the relation between every pairs of words in that decade and update by gradient descent.	dummyTurk
3107	0.0	We use the mini-batch to train the embedding.	dummyTurk
3108	0.0	We train embedding from 1710s to 1920s, while the time range of DTE task is from 1700s to 2010s.	dummyTurk
3109	0.0	Due to the lack of some word-time information after 1920s, if our models make mistakes, it may be more confusing to make worse scores.	dummyTurk
3110	0.0	Under this unfair situation, we achieve best precision and second scores and much higher precision than the one with best scores.	dummyTurk
3111	0.0	And compare with baseline in same condition, we achieve higher precision and score.	dummyTurk
3112	0.0	Thanks for your comments.	dummyTurk
3113	0.0	In figure 2, the derogatory sense such as ???cowards??? takes the major position and there are no commendatory words.	dummyTurk
3114	0.0	With the time went by, the situation became more complex.	dummyTurk
3115	0.0	After 1870s, it shows the tendency of decreasing while the commendatory words show the tendency of rising.	dummyTurk
3116	0.0	We will build stronger qualitative evaluation latter.	dummyTurk
3117	0.0	"(1)	Comparing with IXA, AMBRA, SCAN as well as SVM SCAN."	dummyTurk
3118	0.0	We train embedding from 1710s to 1920s, while the time range of DTE task is from 1700s to 2010s.	dummyTurk
3119	0.0	Due to the lack of some change information after 1920s, if our models make mistakes, it may be more confusing to make worse scores.	dummyTurk
3120	0.0	Under the unfair situation, we achieve best precision and second scores and much higher precision than the one with best scores.	dummyTurk
3121	0.0	"(2)	Comparing with pre-train vectors of Google and diachronic vectors without alignment."	dummyTurk
3122	0.0	In the same condition, our model achieves the best scores and best precision.	dummyTurk
3123	0.0	The weights on the constraints are based on the importance of the constraints, which means that the within-time constraints are more important than the cross-time constraints.	dummyTurk
3124	0.0	In the Historical Thesaurus, the meaning of words has time section, which is not just the appearance of the first usage.	dummyTurk
3125	0.0	We combine similar meanings and use the major meanings to prevent from using some rare meanings.	dummyTurk
3126	0.0	We just use the dictionary to make alignment, the meaning of words is determined in the corpus at that time.	dummyTurk
3127	0.0	If the mismatch happens, it just influences those stable words in wrong stable times.	dummyTurk
3128	0.0	The predict constraint happens when the central word is a stable word.	dummyTurk
3129	0.0	We will correct Equation 3 in the new version.	dummyTurk
3130	0.0	First, we would like to note that some methods in the comparison are transductive, e.g.	dummyTurk
3131	0.0	the SFA approach of (Pan et al., 2010).	dummyTurk
3132	0.0	As suggest by the reviewer, we tested the Tranductive SVM (Joachims et al, 2001) approach in the multi-source sentiment classification setting.	dummyTurk
3133	0.0	We obtained the following results using K_0/1:	dummyTurk
3134	0.0	DEK???B: 82.4%;	dummyTurk
3135	0.0	BEK???D: 83.0%;	dummyTurk
3136	0.0	BDK???E: 83.9%;	dummyTurk
3137	0.0	BDE???K: 86.0%.	dummyTurk
3138	0.0	Although SVM (see comment to Weakness3 of Reviewer3) yields performance similar to KRR, the results of TSVM are about 1-2% below our top results presented in Table 1.	dummyTurk
3139	0.0	We will include these results in the final paper.	dummyTurk
3140	0.0	Our results are supported by statistical significance tests and we do mention that we make use of the unlabeled test samples.	dummyTurk
3141	0.0	However, we can rephrase our presentation to make it more modest.	dummyTurk
3142	0.0	Indeed, the first approach (transductive string kernels) does not use the training labels.	dummyTurk
3143	0.0	It can be seen as an unsupervised kind of transduction and it helps the second approach (the transductive classifier) to obtain better results.	dummyTurk
3144	0.0	If we have a normalized kernel matrix K (with values between [0, 1]) we can obtain a distance matrix D as follows:	dummyTurk
3145	0.0	D = 1 - K. We use this transformation in Eq.	dummyTurk
3146	0.0	(4) and (5).	dummyTurk
3147	0.0	The reason for using the RBF kernel as in Eq.	dummyTurk
3148	0.0	(5) is to project the similarity values from the interval [0, 1] to the interval [0.36, 1], and to remove the values equal to 0 (two strings can have a similarity of 0 if they have NO n-grams in common).	dummyTurk
3149	0.0	Since we use the scalar product of kernel matrices in Eq.	dummyTurk
3150	0.0	(6), having many values equal to zero would result in a more sparse matrix.	dummyTurk
3151	0.0	Since train and test documents are from different domains, it is likely that many similarities are zero in the initial kernel matrix.	dummyTurk
3152	0.0	This was due to lack of space.	dummyTurk
3153	0.0	We considered other sections to be more important.	dummyTurk
3154	0.0	Since the final paper has an extra page, we will make sure to include a conclusion.	dummyTurk
3155	0.0	Yes indeed the results suggest that using LR would be most reasonable, we will state this explicitly.	dummyTurk
3156	0.0	With Figure 1 we were attempting to use alternative (not measured as classification accuracy) evaluation for embeddings.	dummyTurk
3157	0.0	It is true that the results do not seem to translate to downstream tasks; we will adjust discussion parts accordingly.	dummyTurk
3158	0.0	We have already prepared detailed illustration for ``bound'' and ``unbound'' context representations.	dummyTurk
3159	0.0	If the paper is accepted  we will use additional page to place this illustration.	dummyTurk
3160	0.0	"Thank you for your detailed comments, we have fixed typos and covered more existing works concerning intrinsic and extrinsic tasks, particularly the following papers: ""Evaluation methods for unsupervised word embeddings"", ""Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance"", and ""Evaluating Word Embeddings Using a Representative Suite of Practical Tasks""."	dummyTurk
3161	0.0	Adding more analysis is a good suggestion.	dummyTurk
3162	0.0	If accepted, we can provide more examples in the additional content page.	dummyTurk
3163	0.0	We use h_t to show relation to multiplicative attention.	dummyTurk
3164	0.0	We tried many self-attention methods, but they increase computational costs without boosting performances, and sometimes lead to value explosions.	dummyTurk
3165	0.0	FiLM works better(97.7%) with pre-trained feature.	dummyTurk
3166	0.0	CMM didn???t use raw-pixels because most works on CLEVR used pre-trained features.	dummyTurk
3167	0.0	Adam converges fast but sometimes hurts performance.	dummyTurk
3168	0.0	Using SGD+Momentum after Adam isn???t abnormal and proved effective in some cases.	dummyTurk
3169	0.0	Example-papers:	dummyTurk
3170	0.0	"""Improving-Generalization-Performance-by-Switching-from-Adam-to-SGD"""	dummyTurk
3171	0.0	"""Natural-language-inference-over-interaction-space"""	dummyTurk
3172	0.0	70,000 samples isn???t enough to learn complex logics; it's more biased than CLEVR, limiting the generalization ability.	dummyTurk
3173	0.0	Well-designed batch-norms/dropout on questions would help.	dummyTurk
3174	0.0	Hand-picked without much effort.	dummyTurk
3175	0.0	Additional	dummyTurk
3176	0.0	-Question scanning: one pass of RNN encoding.	dummyTurk
3177	0.0	-Not intuitive: previous models generate programs from questions, and execute on images.	dummyTurk
3178	0.0	Why don???t they generate question-processing programs from images?	dummyTurk
3179	0.0	-This is the only difference.	dummyTurk
3180	0.0	But visual reasoning has become an individual field: few referred papers report results on VQA.	dummyTurk
3181	0.0	-Good advices, but finding best DAN-parameters requires effort.	dummyTurk
3182	0.0	We'll clarify Hadamard and figure-2.	dummyTurk
3183	0.0	-We didn???t modify our model to fit either CLEVR or NLVR-public test set, and just listed all results.	dummyTurk
3184	0.0	So it???s enough to show the models??? abilities, also easy-to-reproduce.	dummyTurk
3185	0.0	Many ablations can be done but are less important to the topic.	dummyTurk
3186	0.0	The NLVR-unreleased-test-policy is slightly abnormal requiring 2-months between runs.	dummyTurk
3187	0.0	We???ll contact for a run afterwards.	dummyTurk
3188	0.0	-We use N-1 textual-attention-steps in CMMs with N>1 blocks(Figure-1).	dummyTurk
3189	0.0	Thanks for insightful opinions.	dummyTurk
3190	0.0	Main concerns will be solved if there is a camera-ready submission.	dummyTurk
3191	0.0	Still, with significant improvements w.r.t FiLM on NLVR and number-related CLEVR-questions, our extension is critical.	dummyTurk
3192	0.0	Hoping for slightly higher overall score.	dummyTurk
3193	0.0	weakness-1: The conversations in Task 3 contains system responses such as greetings, thank you, stallers and recommendations (what about restaurant X).	dummyTurk
3194	0.0	Roughly 25% of the system responses are recommendations.	dummyTurk
3195	0.0	The Table 1 shows 75% accuracy as it predicts all except recommendations.	dummyTurk
3196	0.0	Table 2, we evaluate only recommendations, and hence MN and GMN get 0, as they fail all recommendation tasks.	dummyTurk
3197	0.0	weakness-2: We agree with you completely.	dummyTurk
3198	0.0	As our system outperforms even with such strict metrics, we refrained from using other sophisticated metrics.	dummyTurk
3199	0.0	weakness-3: Hierarchical attention improves the scores by 2-2.5 percentage points and also improves restaurant recommendation by more than 10%.	dummyTurk
3200	0.0	There is no added complexity as we re-use the sentence-level attention computed for sequence decoder with attention.	dummyTurk
3201	0.0	Thank you so much for your kind and helpful comments.	dummyTurk
3202	0.0	We appreciate it a lot.	dummyTurk
3203	0.0	 Yes, the 300+ sentences may behave differently.	dummyTurk
3204	0.0	We regarded these 300+ sentences as the alignment errors, thus we did not analyze them.	dummyTurk
3205	0.0	We will check them manually and add the explanations.	dummyTurk
3206	0.0	 Yes, the alignment was trained on the test set.	dummyTurk
3207	0.0	We agree with you that we could get a better alignment if we train the fast-align model on both the training set and the test set.	dummyTurk
3208	0.0	1) The attention mechanism has different patterns on different word types (Ghader and Monz, (2017)), we can only validate our hypothesis on the ambiguous nouns.	dummyTurk
3209	0.0	That is to say, our alignment works well for ambiguous nouns, but we cannot guarantee that our alignment works well on the other word types.	dummyTurk
3210	0.0	"2) The ""alignment"" using argmax attention weight is only used for the evaluation."	dummyTurk
3211	0.0	In Section 3, we need the real alignment information to get the attention weight on each word.	dummyTurk
3212	0.0	Koehn and Knowls, (2017) and Ghader and Monz, (2017) have confirmed that the attention mechanism is different from the traditional word alignment.	dummyTurk
3213	0.0	Thus, we used fastalign to get the alignment information.	dummyTurk
3214	0.0	The conclusion will be moderated.	dummyTurk
3215	0.0	Given the diversity of ideas for mechanisms underlying word learning, only by running this analysis on many such algorithms will it be possible to have the final word on the IDS vs ADS debate.	dummyTurk
3216	0.0	Such a large scale exploration is more advanced in symbolic-based systems, and our work suggests that this also needs to be done with speech-based systems.	dummyTurk
3217	0.0	The effect of sampling rate is probably less than what it seems: the boundaries are evaluated within a temporal window of +/-30 ms.	dummyTurk
3218	0.0	Therefore the sampling rate would only affect MODIS for segments longer than 60ms (for segments long enough to enable an error corresponding to positing a boundary in the middle of phoneme) and such errors would not exist for PUDDLE.	dummyTurk
3219	0.0	We will acknowledge it in the rewritten discussion.	dummyTurk
3220	0.0	Thiessen et al showed that in a highly controlled artificial grammar setting with no prosodic boundaries, infants performed better with more exaggerated prosody than with a flat prosody.	dummyTurk
3221	0.0	In our view, it is reasonable to study the information processing properties in isolation from the attentional properties -- but clearly, it is not the whole picture.	dummyTurk
3222	0.0	Most of the models we and others have been studying focus on the information processing aspect of learning, neglecting the importance of attention and arousal for learning (but see output of ACORNS project).	dummyTurk
3223	0.0	We have now added a sentence pointing this out.	dummyTurk
3224	0.0	Thanks for your review!	dummyTurk
3225	0.0	We believe our generation model will be improved with a more advanced retrieval model, which is not the focus of this work, though.	dummyTurk
3226	0.0	We also hope the current model would be able to identify coherent talking points through keyphrase extraction.	dummyTurk
3227	0.0	Both advanced retrieval techniques and topic selection models will be explored in our future work.	dummyTurk
3228	0.0	Our DEC-SEPARATE and DEC-SHARED setups are significantly better than seq2seq baselines based on METEOR and BLEU in almost all setups.	dummyTurk
3229	0.0	"Based on our observation, the ""SEQ2SEQ+endec&evd"" model often generates generic arguments (e.g."	dummyTurk
3230	0.0	"""I disagree with you"") and produces repetitive content."	dummyTurk
3231	0.0	Alternate training is applied mainly when different subtasks have unpaired training data, but in our case we have gold-standard keyphrases and arguments for the same input, and these two tasks are very related.	dummyTurk
3232	0.0	Therefore we believe it is natural to train the losses jointly.	dummyTurk
3233	0.0	First of all, we will make changes based on your suggestion in our revision.	dummyTurk
3234	0.0	Many details are omitted from submission due to space limitation.	dummyTurk
3235	0.0	We also did not dump everything into the supplementary material to avoid overburdening our reviewers.	dummyTurk
3236	0.0	We will make sure all information is available upon publication.	dummyTurk
3237	0.0	> As far as I understood, each input OP could have different arguments as the example in Figure 1, are training pairs constructed for each argument?	dummyTurk
3238	0.0	Yes, there are more than one gold-standard argument per OP in most cases.	dummyTurk
3239	0.0	The diversity of the topics in the arguments often depends on the OP.	dummyTurk
3240	0.0	On average, there are about 10 arguments for each OP.	dummyTurk
3241	0.0	In most cases, the arguments provide diverse angles and leverage different evidence.	dummyTurk
3242	0.0	We will measure the diversity in a more quantitative way in the future.	dummyTurk
3243	0.0	> Table 2 reports BLEU-2 (bigrams) ?	dummyTurk
3244	0.0	As in the end, the task is close to a summarisation task, probably ROUGE should be reported too.	dummyTurk
3245	0.0	Here we also use METEOR, which considers both precision and recall, and accounts for paraphrase, synonyms, and stemming.	dummyTurk
3246	0.0	ROUGE only considers recall based on strict ngram matching.	dummyTurk
3247	0.0	> Table 2, caption.	dummyTurk
3248	0.0	The phrase 'our separate decoder models' refers to both DEC-SHARED and DEC-SEPARATE or only to the last one?	dummyTurk
3249	0.0	It refers to the DEC-SEPARATE setup only.	dummyTurk
3250	0.0	We will make it clearer in our revision.	dummyTurk
3251	0.0	> In human evaluation (Table 4) 'OUR MODEL' refers to DEC-SHARED or DEC-SEPARATE ?	dummyTurk
3252	0.0	It seems that authors refer to DEC-SEPARATE (which according to examples in supplementary material seem be have better outputs).	dummyTurk
3253	0.0	"In Table 4, ""our model"" refers to DEC-SEPARATE setup."	dummyTurk
3254	0.0	We chose it over DEC-SHARED because it has better METEOR and BLEU in most cases.	dummyTurk
3255	0.0	In addition, we find that DEC-SHARED tends to generate shorter and less informative argument.	dummyTurk
3256	0.0	The overall quality of DEC-SEPARATE output is considered better.	dummyTurk
3257	0.0	> Will the dataset (plus collected evidence) and code be made available ?	dummyTurk
3258	0.0	We have planned and will release the collected and cleaned argument data from Reddit, along with the articles and sentences retrieved from Wikipedia.	dummyTurk
3259	0.0	We will also release source code for our system upon publication.	dummyTurk
3260	0.0	This is omitted in submission because we believe releasing datasets is a commonly-acknowledged standard practice in the NLP community.	dummyTurk
3261	0.0	Thank you for the encouraging and insightful review!	dummyTurk
3262	0.0	W1.	dummyTurk
3263	0.0	We agree that comparisons to state-of-the-arts on individual tasks send a stronger message.	dummyTurk
3264	0.0	As you pointed out, our focus is the general architecture, we therefore compared mTreeLSTM to LSTM and TreeLSTM on various tasks instead of focusing on specific tasks (which typically emphasizes task-dependent setup/components).	dummyTurk
3265	0.0	We carried out such comparisons for the NLI task on a bigger data set, partly because NLI is arguably more semantically complex among the tasks we considered and hence better showcases the usefulness of mTreeLSTM.	dummyTurk
3266	0.0	W2.	dummyTurk
3267	0.0	i) Table 5 shows the effects of the relation embedding size.	dummyTurk
3268	0.0	TreeLSTM doesn't have this parameter; its results served as a baseline.	dummyTurk
3269	0.0	The size of LSTM hidden state is fixed as in Tai et al.	dummyTurk
3270	0.0	2015, and is kept the same for all models.	dummyTurk
3271	0.0	ii) In our experiments, training converges after 10 iterations for all models and we didn't observe better results with more iterations.	dummyTurk
3272	0.0	Thus we trained each model for 10 iterations, and took the model that obtained best results on the development set.	dummyTurk
3273	0.0	W3.	dummyTurk
3274	0.0	In dependency and AMR trees, we can obtain explicit relation information between words, thus mTreeLSTM can be directly applied to these trees.	dummyTurk
3275	0.0	In contrast, no such relation information exists in constituency trees and, consequently, mTreeLSTM cannot be directly applied.	dummyTurk
3276	0.0	W4.	dummyTurk
3277	0.0	Thanks for pointing it out: Line 163 should be revised.	dummyTurk
3278	0.0	What we tried to say is that AMR can be helpful in certain complex tasks (such as for longer sentences in NLI).	dummyTurk
3279	0.0	We will comment more on these subsets of data in the final version.	dummyTurk
3280	0.0	Others.	dummyTurk
3281	0.0	Thanks for the detailed suggestions!	dummyTurk
3282	0.0	We will correct the style and grammar issues.	dummyTurk
3283	0.0	Thank you very much for your thoughtful comments and suggestions.	dummyTurk
3284	0.0	Limited by space, we have to condense some content in this paper, so the comparison of different parsers is indeed kind of insufficient.	dummyTurk
3285	0.0	We believe that your suggestion is helpful and feasible, and we will provide detailed discussion in our new edition.	dummyTurk
3286	0.0	We really appreciate your constructive suggestions and we will make adjustments to improve the paper.	dummyTurk
3287	0.0	Thank you very much for providing the thoughtful comments and suggestions.	dummyTurk
3288	0.0	Your suggestion to compare to the state-of-the-art models is reasonable but there seems to be some misunderstanding about the STS task.	dummyTurk
3289	0.0	We conduct experiments on STS Benchmark, which is a careful selection of English data used in SemEval between 2012 and 2017, rather than the STS2017 dataset.	dummyTurk
3290	0.0	The results on this dataset can be found at http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark, where results of single models and ensemble models are respectively ranked.	dummyTurk
3291	0.0	As you can see, the results of ECNU on STS Benchmark is 0.81 rather than 0.8518 and our ensemble model is state-of-the-art.	dummyTurk
3292	0.0	(However, we indeed make a clerical error in Table 5 by mixing up the results of ECNU and CNN-HCTI, and we will correct it in our new edition).	dummyTurk
3293	0.0	For the MSRP task, simply referring to Pang et al.	dummyTurk
3294	0.0	(2016), we carelessly missed the earlier but more superior results (Ji and Eisenstein 2013).	dummyTurk
3295	0.0	We will follow your helpful suggestion and provide sufficient comparison to the state-of-the-art models in our new edition.	dummyTurk
3296	0.0	Adding a data analysis section is feasible and we will do it in the new edition.	dummyTurk
3297	0.0	Thanks for your constructive advice!	dummyTurk
3298	0.0	This is about the misunderstanding mentioned above.	dummyTurk
3299	0.0	We conduct experiments on STS Benchmark, which only consists of English data, rather than STS2017.	dummyTurk
3300	0.0	In our paper, we mentioned the explicitness of the relation label in Stanford Parser.	dummyTurk
3301	0.0	Your question about collapsed dependencies further provides a more exact insight, and we will conduct experiments to investigate it.	dummyTurk
3302	0.0	We really appreciate your constructive suggestions.	dummyTurk
3303	0.0	We will correct the mistakes as well as adding more details in the unclear parts to improve our paper.	dummyTurk
3304	0.0	FUSS (special case) is different from WMD in that WMD can disregard the interactions (distances) between the non-closest pairs, while FUSS is capable of taking such interactions into account (including the self-interactions), depending on the choice of word similarity and the coefficient.	dummyTurk
3305	0.0	This makes two methods quite different on a technical level, in addition to one being a distance metric and the other being a similarity measure.	dummyTurk
3306	0.0	We use exactly the same approach to calculate the word weights as in Arora et al., 2017, i.e.	dummyTurk
3307	0.0	alpha = a / (a + p(w)), where p(w) is the estimated word frequency and a = 10^???3.	dummyTurk
3308	0.0	We will of course include this important information in the updated manuscript.	dummyTurk
3309	0.0	FUSS does not offer any dramatic gains for the mapping of lexical resources datasets (OnWN and FNWN), which were 2 out of 3 datasets in STS13.	dummyTurk
3310	0.0	Since we were unable to include SMT in STS13 due to licensing restrictions, our approach yields to some competing baselines on STS13.	dummyTurk
3311	0.0	Unfortunately, we do not have any deep insights on why OnWN and FNWN are problematic for FUSS at this time.	dummyTurk
3312	0.0	The biggest difference between our fuzzification procedure and the one proposed by Zhao and Mao (2017) is that our sentence fuzzifier in Eq.	dummyTurk
3313	0.0	4 uses max, which is a proper T-conorm.	dummyTurk
3314	0.0	Zhao and Mao (2017) use the sum (Eq.	dummyTurk
3315	0.0	5 in their paper), which is not a T-conorm.	dummyTurk
3316	0.0	Therefore we feel our approach is more in line with fuzzy set theory.	dummyTurk
3317	0.0	We will restructure the background section to address your point.	dummyTurk
3318	0.0	It is true that the pun does not quite work; it is definitely something we can change.	dummyTurk
3319	0.0	N/A	dummyTurk
3320	0.0	N/A	dummyTurk
3321	0.0	"""Future"" refers to ""years following a YEAR"", where YEAR appears in the biography."	dummyTurk
3322	0.0	The actual YEAR is (almost) always in the past (before 2018) in the biographies we work with, but spatial knowledge can be extracted involving years after YEAR.	dummyTurk
3323	0.0	Thank you very much for the suggestion, we will add it to our future work.	dummyTurk
3324	0.0	We agree, that our definition of trend is somewhat simplistic.	dummyTurk
3325	0.0	However, to our knowledge, there is no easy way to come up with an alternative definition that addresses the potential problems the reviewer enumerates.	dummyTurk
3326	0.0	Our hope would be that a large collection (we provide it as one of our contributions) contains enough clear signal that will outweight any noise due to the pointed problems.	dummyTurk
3327	0.0	Note also that scientific publications don`t suffer much from redundancy: most scientific publication must contain an original core, so they aren`t redundant.	dummyTurk
3328	0.0	We agree, that trend analysis is corpus-dependent and that we didn`t address the important questions of how to create a corpus for a particular study.	dummyTurk
3329	0.0	There is a separate field in corpus linguistics on corpus creation - on the one hand a complex problem, on the other a problem that much successful research has been done on, so	dummyTurk
3330	0.0	we don't feel that this should be the focus of our paper.	dummyTurk
3331	0.0	In any case, we used all documents that semantic scholar made available, except for initial years with few publications and except for non-English publications.	dummyTurk
3332	0.0	So in our case there was a clear and simple corpus creation rationale.	dummyTurk
3333	0.0	This is an excellent point.	dummyTurk
3334	0.0	If the paper is accepted, we`ll add more detailed description, analysis and discussion of the Gold Trends we identified in the meta-literature.	dummyTurk
3335	0.0	Thanks for your review.	dummyTurk
3336	0.0	We will clarify our paper about achieving state-of-the-art performance for GEC (among classification approaches), and modify our conclusions accordingly.	dummyTurk
3337	0.0	We will also compare our models with other latest approaches (e.g., Yannakoudakis et al.	dummyTurk
3338	0.0	2017; Chollampatt and Ng 2017, 2018; and others).	dummyTurk
3339	0.0	In fact we are going to make our codes public after we clean them up.	dummyTurk
3340	0.0	Finally, as for Ji et al.	dummyTurk
3341	0.0	2017, we are able to only find the F0.5 score of that method, and the performance numbers are not listed in that paper.	dummyTurk
3342	0.0	There are some examples that our neural models can correct but CUUI cannot.	dummyTurk
3343	0.0	We can add an example table in Appendix in the final version.	dummyTurk
3344	0.0	We will go through our paper carefully and fix all the problems.	dummyTurk
3345	0.0	We have improved perofrmance for five common error types in this paper.	dummyTurk
3346	0.0	The same methodology can be employed to other less frequent types, such as incorrect adjective/ adverb order (e.g., personally I ??? I personally).	dummyTurk
3347	0.0	Of course, we need to first locate the target phrase correctly.	dummyTurk
3348	0.0	We expect this is feasible, again using the Stanford Corenlp toolkit.	dummyTurk
3349	0.0	We regard it as future work, and we believe our method is promising for other types.	dummyTurk
3350	0.0	We will release our evaluation source code and annotated dataset as soon as possible.	dummyTurk
3351	0.0	Firstly, thanks for your constructive suggestions.	dummyTurk
3352	0.0	Showing the metric results under different n-grams is very appealing and could show readers more information about the metric.	dummyTurk
3353	0.0	We will add it when additional pages are available.	dummyTurk
3354	0.0	Secondly, for the correlation between OTEM and UTEM, please notice that over-translating some source phrases does not necessarily encourage or disable the model to under-translate other source phrases, and vice versa.	dummyTurk
3355	0.0	In theory, both metric do not necessarily follow any specific correlated pattern.	dummyTurk
3356	0.0	Finally, the range of our metric is 0~100, which we will state clear in our new version.	dummyTurk
3357	0.0	For the identification of over-translated and under-translated phrases, we simply use the over-matched and under-matched target n-grams as an alternative.	dummyTurk
3358	0.0	Our algorithm only relies on counting matched and mismatched n-grams.	dummyTurk
3359	0.0	It is as efficient as the BLEU metric.	dummyTurk
3360	0.0	Thanks very much.	dummyTurk
3361	0.0	This is a typo, and we will correct it!	dummyTurk
3362	0.0	Thanks for your great suggestion in the human annotation and evaluation.	dummyTurk
3363	0.0	We will improve our annotation and provide more details about the evaluation following your suggestions.	dummyTurk
3364	0.0	We will also release our annotated dataset to help other researchers to reproduce our results.	dummyTurk
3365	0.0	Our algorithm is completely automatic.	dummyTurk
3366	0.0	It only requires the MT translation and gold references.	dummyTurk
3367	0.0	It can be used the same as the BLEU.	dummyTurk
3368	0.0	Yes, it is 4 just as in BLEU.	dummyTurk
3369	0.0	We computed the coefficient kappa for the two annotators, and the value is 0.845 and 0.860 for over- and under-translation respectively.	dummyTurk
3370	0.0	We believe that our annotators are very consistent.	dummyTurk
3371	0.0	 In principle, using other centrality measures in the prediction task in Section 6 could be interesting.	dummyTurk
3372	0.0	The reason why we did not do so for this paper is that we wanted to focus on tie strength in order to directly connect to the sociolinguistic theories motivating our study.	dummyTurk
3373	0.0	Our aim was not to optimise for predictive power.	dummyTurk
3374	0.0	 Thanks for the references.	dummyTurk
3375	0.0	We were aware of Danescu-Niculescu-Mizil et al.	dummyTurk
3376	0.0	(2013), but we decided not to include it in our review of related work because it focuses on the social dynamics of users rather than on linguistic innovations.	dummyTurk
3377	0.0	 Thanks, we will compare Kullback-Leibler divergence to the Kolmogorov-Smirnov test and mention the results in the paper in case this leads to additional insights.	dummyTurk
3378	0.0	 As we explain in Section 3.1, we selected these 20 subredditis because they are diverse in terms of subject matter and size (hence we can check whether our analysis generalises), while at the same time displaying features that favour innovation diffusion.	dummyTurk
3379	0.0	About your comment regarding Equation 1: Please note that your example does not apply here, as this equation is only intended to be used for nodes that are connected to each other.	dummyTurk
3380	0.0	When we get the chance to edit the paper we will add more examples and add a manual evaluation of a small subset.	dummyTurk
3381	0.0	Vroniplag contains different categories of plagiarisms.	dummyTurk
3382	0.0	"We used the ""Verschleierung"" (obfuscation) type only."	dummyTurk
3383	0.0	A Verschleierung is a paraphrase of a sentence without proper citation.	dummyTurk
3384	0.0	Mostly these are not almost identical.	dummyTurk
3385	0.0	We calculated statistics about the similarity of the plagiarisms using bag-of-words distances and deleted pairs that are too similar (bow distance < 6).	dummyTurk
3386	0.0	The classification results are different in Vroniplag and MSRPar.	dummyTurk
3387	0.0	But it is a very hard (maybe impossible) task to look inside of the classifier and find out the reason.	dummyTurk
3388	0.0	We had a theory that the classification is easier on Vroniplag and the results of the classification indicated that the theory was right.	dummyTurk
3389	0.0	We cannot prove this theory and we cannot say for sure that there are no other reasons.	dummyTurk
3390	0.0	We had the theory that paraphrase detection is easier on the Vroniplag dataset then on the MSR Paraphrase Corpus because the training example of sentence pairs not being paraphrases are semantically very different in Vroniplag and semantically more similar in MSR Paraphrase Corpus.	dummyTurk
3391	0.0	Therefore we trained a classifier to support our theory.	dummyTurk
3392	0.0	The higher evaluation scores on the Vroniplag dataset does not proof our theory but it gives the results we expected so it is at least an indication that our theory was right.	dummyTurk
3393	0.0	When we edit the paper we will state the purpose of the evaluation more clearly.	dummyTurk
3394	0.0	"Maybe ""Exploration"" is a better heading then ""Evaluation"" for this part of the paper."	dummyTurk
3395	0.0	We considered tf-idf common knowledge but in our last paper we got critizised because reviewers did not know it.	dummyTurk
3396	0.0	When we edit the paper we will add some details about the parameters of the classification algorithm.	dummyTurk
3397	0.0	We adopted the style of the MSRPar dataset to divide the dataset in a test and a train set.	dummyTurk
3398	0.0	This makes it easier to compare results between experiments because they are trained and tested on exactly the same data.	dummyTurk
3399	0.0	But I see your point.	dummyTurk
3400	0.0	It was a very tight decision between cross validation and the train / test split.	dummyTurk
3401	0.0	Thank you for your detailed review.	dummyTurk
3402	0.0	Good point, we do not disagree.	dummyTurk
3403	0.0	We only emphasise that Elman seems to refer to different types of structure, with experiments about word boundaries but also inducing (semantic) word-type clusterings from (distributional) verb-argument statistics.	dummyTurk
3404	0.0	We will change 5.1 to make this more clear.	dummyTurk
3405	0.0	See below.	dummyTurk
3406	0.0	Apologies for this.	dummyTurk
3407	0.0	Please see our response to R1 for clarification - we will resolve this in the main text of the paper.	dummyTurk
3408	0.0	The agent does not process language character-by-character.The instructions are segmented into words, and each word type corresponds to a different input unit in the agent.	dummyTurk
3409	0.0	As experiments in this paper involve early word learning, the language input is always one word.	dummyTurk
3410	0.0	"There was therefore no substantive difference between LSTM language encoding and a simple word embedding layer (""BOW"" in appendix, since it does not encode word order)."	dummyTurk
3411	0.0	We described the LSTM language encoder in the main text to show the agent could in theory process ordered phrases and sentences, but this was misleading wrt.	dummyTurk
3412	0.0	the present experiments.	dummyTurk
3413	0.0	We will adjust this.	dummyTurk
3414	0.0	See above.	dummyTurk
3415	0.0	There are six fine-grained motor actions available to the agent.	dummyTurk
3416	0.0	Please see our response to R1 for more information.	dummyTurk
3417	0.0	Weakness1	dummyTurk
3418	0.0	The discussion of semantic vectors is relevant to this paper.	dummyTurk
3419	0.0	Previous work cited in the paper used WTMF to score summaries, but not to build pyramids.	dummyTurk
3420	0.0	We were inspired by this method, but did not want to adopt WTMF without comparison with other methods.	dummyTurk
3421	0.0	We assume that more accurate similarity is preferable, because that is the task that human annotators are asked to judge when they construct SCUs and score new summaries: the different phrases within an SCU must express the same meaning.	dummyTurk
3422	0.0	We agree that the problem of semantic similarity is a very large topic in its own right, due to the page limit we cannot provide detailed discussion.	dummyTurk
3423	0.0	Weakness2	dummyTurk
3424	0.0	Thanks for pointing out the unclarity.	dummyTurk
3425	0.0	Each segment is essentially a tensed VP and its subject found by phrase parser and dependency parser.	dummyTurk
3426	0.0	Q1	dummyTurk
3427	0.0	Yes.	dummyTurk
3428	0.0	We have tried all methods listed in table 1.	dummyTurk
3429	0.0	WTMF shows the best performance in PyrEval because it reaches the highest Pearson correlation when correlating the pyramid scores with human scores.	dummyTurk
3430	0.0	Q2	dummyTurk
3431	0.0	As in (Arora et.al 2016): STS'12: MSRpar, MSRvid, SMT-eur, OnWN, SMT-news; STS'13: headline, OnWN, FNWN, SMT; STS'14:deftforum, deftnews, headline, images, OnWN, tweet news.	dummyTurk
3432	0.0	 Q3	dummyTurk
3433	0.0	We have done lots of error analysis in general but not specifically between two EDUAs.	dummyTurk
3434	0.0	This is a good suggestion and we will work on it.	dummyTurk
3435	0.0	We use EDUAG in section 7.	dummyTurk
3436	0.0	Thanks for noticing the typos in algorithm and we will fix them	dummyTurk
3437	0.0	Weakness1	dummyTurk
3438	0.0	Say that named entities are not handled separately.	dummyTurk
3439	0.0	Because we use a fairly simple method to segment each sentence into clause-like units, each segment is essentially a tensed VP and its subject.	dummyTurk
3440	0.0	Due to page limits we cannot provide details.	dummyTurk
3441	0.0	Thanks for pointing out the uses of latent vectors.	dummyTurk
3442	0.0	Indeed our future work is to find a richer semantic representation method for PyrEval.	dummyTurk
3443	0.0	"This step will enable the evaluation to focus more on ""semantics"", particularly when we apply it to abstractive summarizers."	dummyTurk
3444	0.0	The difference between Rouge and PyrEval could be found in response to Reviewer 1 Question 2.	dummyTurk
3445	0.0	Q1	dummyTurk
3446	0.0	We have two EDUAs: we first present the problem they are both addressing, then we present the different approaches EDUAs are taking.	dummyTurk
