\section{Experiments}\label{sec:expts}

In the first set of experiments we evaluate the baselines and the different methods for modelling correlations between workers' preferences. In the second set of experiments, we assess the value of different language features. Finally, the third experiment evaluates approaches that integrate both argument features and models of preference correlations.

Prior work on convincingness:
\begin{itemize}
 \item \cite{habernal2016argument} shows how to predict convincingness of arguments by training a NN 
 from crowdsourced annotations. 
 \item \cite{lukin2017argument} shows that persuasion is correlated with personality traits.
\end{itemize}

We build on this to show...
\begin{itemize}
 \item How we can predict convincingness for a specific user given only previous preferences and 
 preferences of others (collaborative filtering)
 \item How a combination of text and personality features improves predictions of convincingness
 \item That we can extract human-interpretable latent features in people and items,
 which improve performance over just using the input features.
\end{itemize}
This is all useful because we can use the approach to determine which features are worth 
obtaining, make predictions when data is sparse, and obtain data from users efficiently.

The steps to show this are:
\begin{enumerate}
  \item Show a table comparing the baselines, alternative collaborative filtering methods, 
  results from \cite{habernal2016argument}, and unsupervised method
  \item Add in results when using the input information with out method
  \item Show a table comparing the baselines, alternative collaborative filtering methods, 
  results from \cite{lukin2017argument}, and unsupervised method
  \item Add in results using item information, person information and both
  \item Visualise latent features?
  \item Table showing importance of input features
  \item Add results with lower confidence items excluded to the tables in 1-4. We can also plot the
  effect of confidence threshold on our results and on the rival methods.
  \item Add in Bier/cross entropy -- may need to rerun the original code from the previous papers?
  \item Run \cite{habernal2016argument} and my complete method with reduced data -- check accuracy as it increases. Use confidence cut-off from previous results.
  \item Simple active learning approach selecting the most uncertain data point (this will be due to 
  uncertainty about a person, an item with too little data, or disagreement/stochasticity in the likelihood). The plot can be added to the previous results and should be run with rival methods.
\end{enumerate}

\subsection{Note on difficult tasks for crowdworkers}

Would it be possible to include some results from argumentext here that show how workers were unable to be consistent with each other when marking argument spans?
