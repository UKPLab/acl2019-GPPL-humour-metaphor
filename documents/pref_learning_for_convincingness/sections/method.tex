\section{Scalable Bayesian Preference Learning}\label{sec:model}

First, we introduce a probabilistic model for preference learning~\cite{chu2005preference}.
We observe preference pairs, each consisting of a pair of feature vectors $\mathbf x_i$ and $\mathbf x_j$, for arguments $i$ and $j$,
%the $k$th pair refers to arguments $i$ and $j$ $x_i$ and $x_j$ and its label may 
and a label $y \in \{i \succ j, j \succ i\}$.
%, where $i \succ j$ indicates that $i$ is more convincing than
%$j$, and $j \succ i$ means the opposite.
We assume that the likelihood of $y$ depends on the latent convincingness, $f(\mathbf{x_i})$ and 
$f(\mathbf x_j)$, of the arguments in the pair. 
Our goal is to predict $y$ for pairs that have not been observed, 
%as well as to predict the convincingness, 
and predict $f(\mathbf x_i)$, which may be used to rank arguments.
The relationship between convincingness and pairwise labels is described by the following:% \emph{preference likelihood}:
\begin{flalign}
& p( i \succ j | f(\mathbf x_i), f(\mathbf x_j), \delta_{i}, \delta_{j} ) & \nonumber\\
& \hspace{0.9cm} = \begin{cases}
 1 & \text{if }f(\mathbf x_i) + \delta_{i} \geq f(j) + \delta_{j} \\
 0 & \text{otherwise,}
 \end{cases} &
 \label{eq:pl}
\end{flalign}
where $\delta \sim \mathcal{N}(0, 1)$ is Gaussian-distributed noise. 
If the convincingness $f(\mathbf x_i)$ is higher than the convincingness $f(\mathbf x_j)$, 
the preference label $i \succ j$ is more likely to be true.
However, the label also depends on the noise terms, $\delta_{i}$ and $\delta_{j}$,
to allow for errors caused by, for example, disagreement between human annotators.
%, or interpreting clicks in a user interface as preferences between the items displayed.
%Errors may also occur if pairwise labels are derived from implicit data such as clicks streams. For example,  
%a user selecting one argument from a list rather than another may be treated as a pairwise preference label that gives a 
%weak indication of that user's preferences between the two arguments.
%To obtain the likelihood of a pairwise label given only $f(x_i)$ and $f(x_j)$, 
We simplify Equation \ref{eq:pl} by integrating out $\delta_{i}$ and $\delta_{j}$ to obtain the \emph{preference likelihood}:
\begin{flalign}
& p( i \succ j | f(\mathbf x_i), f(\mathbf x_j) ) & \nonumber\\
& = \int\int p( i \succ j | f(\mathbf x_i), f(\mathbf x_j), \delta_{i}, \delta_{j} ) &\nonumber\\
& \hspace{3cm}\mathcal{N}(\delta_{i}; 0, 1)\mathcal{N}(\delta_{j}; 0, 1) d\delta_{i} d\delta_{j} &\nonumber\\
& = \Phi\left( z \right), 
\label{eq:plphi}
\end{flalign}
where $z = (f(\mathbf x_i) - f(\mathbf x_j)) / \sqrt{2}$,
and $\Phi$ is the cumulative distribution function of the standard normal distribution. 

We assume that convincingness is a function, $f$, of argument features, 
drawn from a Gaussian process prior: $f \sim \mathcal{GP}(0, k_{\theta}s)$, where 
$k_{\theta}$ is a kernel function with hyper-parameters $\theta$, 
and $s$ is a scale parameter. 
The kernel function controls the smoothness of $f$ over the feature space,
while $s$ controls the variance of $f$. 
Increasing $s$ means that, on average, the magnitude of $f(\mathbf x_i)-f(\mathbf x_j)$ increases  
so that $\Phi(z)$ is closer to $0$ or $1$, and erroneous pairwise labels are less likely.
Therefore, larger values of $s$ correspond to less observation noise
and there is no need for a separate term for the variance of $\delta$, as in Chu and Ghaharamani~\shortcite{chu2005preference}.
%, because $s$ scales $f$ relative to $\delta$.
We assume a Gamma distribution $1/s \sim \mathcal{G}(a_0, b_0)$ with shape $a_0$ and scale $b_0$,
as this is a conjugate prior.

Given $N$ arguments and $P$ labeled preference pairs, $\mathbf y=\{y_1,...,y_P\}$,
we can make predictions by finding the posterior distribution over the convincingness values, 
$\mathbf f = \{f(\mathbf {x}_1),...,f(\mathbf {x}_N)\}$, given by:
\begin{flalign}
& p\left(\mathbf f | \mathbf{y}, k_{\theta}, a_0, b_0 \right) 
\propto p(\mathbf y | \mathbf f) p(\mathbf f | k_{\theta}, a_0, b_0) & \nonumber \\
& \! =  \frac{1}{Z} \! \int  \prod_{k=1}^P \Phi\!\left( z_k \right) 
\mathcal{N}(\mathbf f; \mathbf 0, \mathbf K_{\theta}s) \mathcal{G}(s; a_0, b_0) \mathrm{d}s, \!\!\!\! &
\label{eq:post}
\end{flalign}
where $Z = p\left(\mathbf{y} | k_{\theta}, a_0, b_0 \right)$.
Unfortunately, neither $Z$ nor the integral over $s$ 
can be computed analytically, so we must turn to approximations.

Chu and Ghahramani~\shortcite{chu2005preference}
used a Laplace approximation for GPPL, which finds a maximum a-posteriori (MAP) solution
that has been shown to perform poorly in many cases
~\cite{nickisch2008approximations}. 
More accurate estimates of the posterior could be obtained using Markov chain Monte Carlo sampling (MCMC),
but this is very computationally expensive ~\cite{nickisch2008approximations}. 
Instead, we use a faster \emph{variational} method that maintains the benefits of the Bayesian approach
~\cite{reece2011determining,steinberg2014extended} and adapt this method 
to the preference likelihood given by Equation \ref{eq:plphi}.

To apply the variational approach, we define an approximation $q(\mathbf f)$ to Equation \ref{eq:post}. 
First, we approximate the preference likelihood with a Gaussian, $\prod_{k=1}^P \Phi\left( z_k \right) \approx \mathcal{N}(\mathbf y; \mathbf G\hat{\mathbf f}, \mathbf Q)$. This allows us to avoid the intractable integral in $Z$ and obtain another Gaussian, $q(\mathbf f) = \mathcal{N}(\mathbf f; \hat{\mathbf f}, \mathbf C)$. 
The parameters $\hat{\mathbf f}$ and $\mathbf C$ 
depend on the approximate preference likelihood 
and an approximate distribution over $s$: $q(s) = \mathcal{G}(s; a, b)$. 
The variational inference algorithm begins by initializing the parameters $\mathbf G$, $ \hat{\mathbf f}$, $\mathbf C$, $a$ and $b$ at random. Then, the  algorithm proceeds iteratively updating each parameter in turn, given the current values for the other parameters. 
This optimization procedure minimizes the Kullback-Leibler (KL) divergence of $p(\mathbf f |\mathbf y, k_{\theta}, a_0, b_0)$ from $q(\mathbf f)$, causing $q(\mathbf f)$ to converge to an approximate posterior. 

The update equations for the mean $\hat{\mathbf f}$ and covariance $\mathbf C$ require inverting the covariance matrix, $K_{\theta}$, at a computational cost of $\mathcal{O}(N^3)$, which is impractical with more than a few hundred data points. 
Furthermore, the updates also require $\mathcal{O}(NP)$ computations and
have $\mathcal{O}(N^2 + NP + P^2)$ memory complexity.
To resolve this, 
%Starting from random initial values, we update $q(f,s)$ iteratively to maximize a lower bound on the log marginal likelihood, 
%$\mathcal{L} \leq \log p(\mathbf y | \theta, a_0, b_0)$.
%This optimization procedure minimizes the Kullback-Leibler divergence of $p(f,s|\mathbf y, \theta, a_0, b_0)$ from $q(f,s)$,
%meaning that $q(f,s)$ converges to an approximate posterior. 
we apply a recently introduced technique, stochastic variational inference (SVI) 
\cite{hoffman2013stochastic,hensman_scalable_2015},
%to enable variational inference 
to scale to datasets containing at least tens of thousands of arguments and pairwise labels.

SVI makes two approximations: it assumes $M$ \emph{inducing points},
which act as a substitute for the observed arguments;
it uses only a random subset of the data containing $P_n$ pairs at each iteration. 
At each iteration, $t$, rather than update $\hat{\mathbf{f}}$ and $\bs C$ directly, 
we update the mean $\hat{\mathbf{f}}_m$ and covariance $\bs C_m$ for the inducing
points. The update for each parameter $\lambda \in \{\hat{\mathbf{f}}_m, \bs C_m\}$ takes the form of a weighted mean of the previous estimate and a new estimate computed from only a subset of observations:
\begin{flalign}
\lambda^{(t)} = (1 - \rho_t) \lambda ^ {(t-1)} + \rho_t \hat{\lambda}_t P/P_n,
\end{flalign}
where $\rho=t^{-u}$ is the step size, $u$ is a forgetting rate, % for previous values,
and $\hat{\lambda}_t$ is the new estimate computed from $P_n$ out of $P$ observations.
The values of $\hat{\mathbf{f}}$ and $\bs C$ can be estimated from 
the inducing point distribution.
% according to:
%\begin{flalign}
%& \hat{\mathbf{f}} \approx \bs K_{nm} \bs K_{mm}^{-1} \hat{\mathbf{f}_m} & \nonumber\\
%& \bs C \approx \bs K_{nm} \bs K_{mm}^{-1} (\bs C_m - \bs K_{mm}s) \bs K_{mm}^{-1} \bs K_{nm}^{T}. & 
%\end{flalign}
By choosing $M <\!\!< N$ and $P_n <\!\!< P$, we limit the computational
complexity of each SVI iteration to $\mathcal{O}(M^3 + MP_n)$ and the 
memory complexity $\mathcal{O}(M^2 + MP_n + P_n^2)$.
To choose representative inducing points, 
we use K-means++~\cite{arthur2007k} with $K=M$ to rapidly cluster the feature vectors, 
then take the cluster centers as inducing points.
Compared to standard K-means, K-means++ introduces a new method for choosing the initial cluster seeds that
%provides theoretical bounds on the error function. In practice, this 
reduces the number of poor-quality clusterings.% that result from random initialization.

A further benefit of GPs is that they enable \emph{automatic relevance determination (ARD)}
to identify informative features, which works as follows.
The prior covariance of $f$ is defined by a kernel function of the form 
$k_{\theta}(\mathbf x, \mathbf x') = \prod_{d=1}^D k_d(|x_d - x_d'| / l_d)$, 
where $k_d$ is a function of the distance between the values of feature $d$ 
for items $x$ and $x'$, and a length-scale hyper-parameter, $l_d$.
The length-scale controls the smoothness of the function across the feature space,
and can be optimized by choosing the value of $l_d$ that maximizes the approximate log marginal likelihood, $\mathcal{L} \approx \log p(\bs y)$. 
This process is known as \emph{maximum likelihood II (MLII)}~\cite{rasmussen_gaussian_2006}.
Features with larger length-scales after optimization are less relevant because their values
have less effect on $k_{\theta}(\mathbf x, \mathbf x') $.
To avoid the cost of optimizing the length-scales, we can alternatively set them using a median heuristic,
which has been shown to perform well in practice~\cite{gretton2012optimal}: 
$ \tilde{l}_{d} = \frac{1}{D} \mathrm{median}\left( \left\{ |x_{i,d} - x_{j,d}|, \right.\right.$
$ \left.\left. \forall i=1,...,N, \forall j=1,...,N\right\} \right) $.
