\section{Scalable Inference}\label{sec:inf}

Given a set of pairwise labels, $\bs y$, 
the goal is to infer the posterior distribution over the preference values $\bs f$, in the single
user case, or $\bs F=\bs V^T \bs W$ in the multi-user case. 
Previous approaches include a Laplace approximation for the single user case~\citep{chu2005preference}
and a combination of expectation propagation (EP) with variational Bayes (VB) for a 
multi-user model~\citep{houlsby2012collaborative}.
The Laplace approximation is a maximum a-posteriori (MAP) solution that
takes the most probable values of parameters rather than integrating over their distributions
and has been shown to perform poorly for tasks such as classification~\citep{nickisch2008approximations}. 
EP approximates the true posterior with a simpler, factorized distribution
that can be learned using an iterative algorithm.
The true posterior is multi-modal, since the latent factors can be re-ordered arbitrarily without
affecting $\bs F$: this is the non-identifiability problem.
A standard EP approximation would average these modes before predicting $\bs F$,
producing uninformative predictions over $\bs F$.
~\citet{houlsby2012collaborative} resolve this by incorporating a VB step, which approximates a single mode.
A drawback of EP is that unlike VB, convergence is not guaranteed~\citep{minka2001expectation}.
%do they also linearise in the same way? -- both linearise. But EP uses a joint over y and f as its approximation to p(y|f), then optimises the parameters iteratively. It's not guaranteed to converge. Variational EGP instead approximates
% p(y|f) directly with the best fit Gaussian. It's not clear whether this could be updated iteratively but it doesn't
% seem to work if done simultaneously with the other variables we need to learn (the linearisation), 
% perhaps because the algorithm for learning the weights breaks if the variance of q(y|f), Q, keeps changing. 
% Possibly because Q does not change incrementally. So it's
% possible that an outer loop could be used.

The cost of inference can be reduced using a \emph{sparse} approximation based on a set of 
\emph{inducing points}, which act as substitutes for the set of points in the training dataset.
By choosing a fixed number of inducing points, $M \ll N$, the computational cost is fixed at $\mathcal{O}(M^3)$.
These points must be selected so as to give a good approximation, 
using either heuristics or optimizing their positions to maximize the approximate
marginal likelihood. 
\citet{houlsby2012collaborative} use a FITC approximation~\citep{snelson2006sparse} 
with their EP method to limit the costs of inference. However, in practice, FITC 
is unsuitable for datasets with more than a few thousands points
as it is not amenable to distributed computation, does it address other expensive operations
with computational complexity $\mathcal{O}(NP)$ and memory complexity $\mathcal{O}(P^2 + NP + N^2)$, which may 
become limiting when the number of data points is large\citep{hensman2015scalable}.  
We turn to stochastic variational inference (SVI) ~\citep{hoffman2013stochastic} 
to derive a more scalable approach
for Gaussian process preference learning, including
a multi-user model founded on Bayesian matrix factorization.
% how does hensman 2015 optimize inducing points? This is one selling point of SVI. 
% Advantages of VB:  Nickisch 2008 got poorer results for VB methods they tried than EP. But our method may be different?, Seeger 2000
First, we define an approximate posterior that can be estimated using SVI, 
then provide the update equations for an iterative algorithm to optimize
this approximation. We begin with the model for a single user, then extend this to the multi-user case using matrix factorization. 

\subsection{An Approximate Preference Likelihood}

Due to the non-Gaussian likelihood, Equation \ref{eq:plphi},
the posterior distribution over $\bs f$ contains intractable integrals:
\begin{flalign}
p(\bs f | \bs y, k_{\theta}, \alpha_0, \beta_0) = 
\frac{\int \prod_{p=1}^P \Phi(z_p) \mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s) 
\mathcal{G}(s; \alpha_0, \beta_0) d_s}{\int \int \prod_{p=1}^P \Phi(z_p) \mathcal{N}(\bs f'; \bs 0, \bs K_{\theta}/s) 
\mathcal{G}(s; \alpha_0, \beta_0) d s d f' }.
\label{eq:post_single}
\end{flalign}
To simplify the integral in the denominator, we approximate the preference likelihood with a Gaussian:
\begin{flalign}
\prod_{p=1}^P \Phi(z_p) \approx \mathcal{N}(\bs y; \Phi(\bs z), \bs Q),
\label{eq:likelihood_approx}
\end{flalign}
where $\bs z=\{z_1, ..., z_P\}$
and $\bs Q$ is a diagonal noise covariance matrix.
We estimate the diagonal entries of $\bs Q$ by moment matching
the approximate likelihood with a beta-binomial with variance given by:
\begin{flalign}
Q_{p,p}=\mathbb{E}_{\bs f}[\Phi(z_p)(1 - \Phi(z_p))] 
%= \mathbb{E}_{\bs f}[\Phi(z_p)] - \mathbb{E}_{\bs f}[\Phi(z_p)]^2 - \mathbb{V}_{\bs f}[\Phi(z_p)], 
= \frac{ (y_p + \gamma_0)(1-y_p + \lambda_0) }{ (2 + \gamma_0 + \lambda_0},
\end{flalign}
where $\gamma_0$ and $\lambda_0$ are parameters of a Bernoulli distribution that has the same variance as the prior $p(\Phi(z_p) | \bs K_{\theta}, \alpha_0, \beta_0)$ using numerical integration.
Setting $\bs Q$ in this way matches the moments of the true likelihood, $\Phi(z_p)$,
to those of the Gaussian approximation.

Unfortunately, the nonlinear term, $\Phi(\bs z)$ means that the posterior is still intractable, 
so we linearize $\Phi(\bs z)$ by taking its first-order Taylor series expansion
about the expected value of $\bs f$:
\begin{flalign}
\Phi(\bs z) &\approx \tilde{\Phi}(\bs z) = \bs G (\bs f-\mathbb{E}[\bs f]) + \Phi(\mathbb{E}[\bs z]), \\
G_{p,i} &= \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p])) (2y_p - 1)( [i = a_p] - [i = b_p]) 
\end{flalign}
where $\bs G$ is the Jacobian matrix of the pairwise likelihood with elements $G_{p,i}$. 
This creates a dependency between the posterior mean of $\bs f$ and the linearization terms in the likelihood,
which can be estimated iteratively using variational inference~\citep{steinberg2014extended},
as we will describe below.
The linearization makes the approximate likelihood conjugate to $\mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/s)$,
so that the approximate posterior over $\bs f$ is also Gaussian. 

Given our likelihood approximation, we can now use variational inference to estimate the marginal over $\bs f$,
by optimizing an approximate posterior over all latent variables:
\begin{flalign}
p(\bs f, s | \bs y, \bs x, k_{\theta}, \alpha_0, \beta_0) & \approx q(s)q(\bs f), & \nonumber \\
\textrm{where } \log q(s) & = \log \mathcal{N}(\mathbb{E}[\bs f]; \bs 0, \bs K_{\theta}/s) + \log \mathcal{G}(s; \alpha_0, \beta_0) + \textrm{const}, & \nonumber \\
\log q(\bs f) & = \log \mathcal{N}(\bs y; \tilde{\Phi}(\bs z), \bs Q) + \log \mathcal{N}(\bs f; \bs 0, \bs K_{\theta}/\mathbb{E}[s]) + \textrm{const}. &
\label{eq:vb_approx}
\end{flalign}
%don't some of the expectations over s simplify out?
The Gaussian likelihood approximation and linearization
also appear in methods based on expectation propagation~\citep{rasmussen_gaussian_2006} 
and the extended Kalman filter~\citep{reece2011determining,steinberg2014extended}.
However, neither these methods nor our approximation in Equation \ref{eq:vb_approx}
make inference sufficiently scalable, as they all require
inversion of an $N \times N$ matrix and further computations involving $N \times P$ and $P \times P$ matrices.
We therefore modify Equation \ref{eq:vb_approx} to enable stochastic variational inference (SVI).

\subsection{Sparse Approximation for the Single-User Model}

We introduce a sparse approximation to the Gaussian process that allows
us to limit the size of the covariance matrix that needs to be inverted,
and permit stochastic inference methods that consider only a subset of the $P$ observations at each iteration
~\citep{hensman2013gaussian,hensman2015scalable}. 
To do this, we introduce a set of $M$ \emph{inducing points}, with inputs $\bs x_m$,
 function values $\bs f_m$, and covariance $\bs K_{mm}$.
The covariance between the observations and the inducing points is $\bs K_{nm}$.
We then modify the variational approximation in Equation \ref{eq:vb_approx} to introduce the inducing points 
(for clarity, we omit $\theta$ from this point on):
\begin{flalign}
p(\bs f, \bs f_m, s | \bs y, \bs x, \bs x_m, k_{\theta}, \alpha_0, \beta_0) &\approx q(\bs f, \bs f_m, s) = q(s)q(\bs f)q(\bs f_m), \label{eq:svi_approx}
\end{flalign}
\begin{flalign}
\log q(\bs f_m) &= \log \mathcal{N}\left(\bs y; \tilde{\Phi}(\bs z), \bs Q\right)]
+ \log\mathcal{N}\left(\bs f_m; \bs 0, \bs K_{mm}/\mathbb{E}\left[s\right]\right)  + \textrm{const}, \nonumber \\
%&= \log \int \mathcal{N}(\bs y - 0.5; \bs G \bs f, \bs Q) 
%\mathcal{N}(\bs f; \bs A \bs f_m, \bs K - \bs A \bs K_{nm}^T) & \nonumber\\
%& \hspace{3.2cm} \mathcal{N}(\bs f_m; \bs 0, \bs K_{mm}\mathbb{E}[1/s]) \textrm{d} \bs f + \textrm{const} & \nonumber\\
 & = \log \mathcal{N}(\bs f_m; \hat{\bs f}_m, \bs S ), \\
\bs S^{-1} &= \bs K^{-1}_{mm}/\mathbb{E}[s] + \bs A^T \bs G^T \bs Q^{-1} \bs G \bs A, \label{eq:S}\\
\hat{\bs f}_m &= \bs S \bs A^T \bs G^T \bs Q^{-1} (\bs y - \Phi(\mathbb{E}[\bs z]) + \bs G \mathbb{E}[\bs f] ), \label{eq:fhat_m}
\end{flalign}
where $\bs A = \bs K_{nm} \bs K^{-1}_{mm}$.
The factor $q(s)$ remains unchanged from Equation \ref{eq:vb_approx}, while
$q(\bs f)$ is now assumed to be independent of the observations:
 \begin{flalign}
\log q(\bs f) & = \log \mathcal{N}(\bs f; \bs A \hat{\bs f}_m, 
\bs K + \bs A (\bs S - \bs K_{mm}/\mathbb{E}[s]) \bs A^T ).
\end{flalign}
The use of inducing points therefore avoids the need to invert an $N \times N$ covariance matrix to compute the posterior.

To choose inducing points that can represent the spread of data in our observations
across feature space, 
we use K-means++~\cite{arthur2007k} with $K=M$ to  
cluster the feature vectors, 
then take the cluster centers as inducing points.
%Compared to standard K-means, K-means++ introduces a new method 
%for choosing the initial cluster seeds that
%provides theoretical bounds on the error function. In practice, this 
%reduces the number of poor-quality clusterings.
An alternative approach would be to learn the placement of inducing points
as part of the variational inference procedure ~\citep{???},
or by maximizing the variational lower bound on the log marginal likelihood 
(see next section).
However, the former breaks the convergence guarantees, and both approaches
may add substantial computational cost. 
Therefore, in this paper, we show that it is often sufficient to place inducing points up-front, and leaving their optimization for future work. 

\subsection{SVI for Single-User Preference Learning}

%The approximate posterior can now be optimized using stochastic variational inference (SVI),
%which uses a series of stochastic updates involving different subsets of the observations.
%Variational inference 
To estimate the approximate posterior, we can apply variational inference, which
iteratively reduces the KL-divergence between our approximate posterior, $q(s)q(\bs f)q(\bs f_m)$
and the true posterior, $p(s, \bs f, \bs f_m | \bs K, \alpha_0, \beta_0, \bs y)$,
by maximizing a lower bound, $\mathcal{L}$, on the marginal likelihood, $\log p(\bs y | \bs K, \alpha_0, \beta_0)$ :
\begin{flalign}
\log p(\bs y | \bs K, \alpha_0, \beta_0) & = \textrm{KL}(q(\bs f, \bs f_m, s)  || p(\bs f, \bs f_m, s | \bs y, \bs K, \alpha_0, \beta_0)) - \mathcal{L}. &
\end{flalign}
By taking expectations with respect to the variational $q$ distributions, the lower bound is:
\begin{flalign}
\mathcal{L} =\; & \mathbb{E}_{q(\bs f, \bs f_m, s)}[\log p(\bs y | \bs f) + \log p(\bs f_m, s | \bs K, 
\alpha_0, \beta_0) -\log q(\bs f_m) - \log q(s) ] & \nonumber \\ \label{eq:lowerbound}
=\; & \sum_{p=1}^P \mathbb{E}_{q(\bs f)}[\log p(y_p | f_{a_p}, f_{b_p})] - \frac{1}{2} \bigg\{ \log|\bs S| - M + \log|\bs K_{mm}| - \mathbb{E}[\log s] 
\nonumber &\\
& + \hat{\bs f}_m\mathbb{E}[s] \bs K_{mm}^{-1}\hat{\bs f}_m + 
\textrm{Tr}(\mathbb{E}[s] \bs K_{mm}^{-1} \bs S) \bigg\}  + \log\Gamma(\alpha) - \log\Gamma(\alpha_0)  + \alpha_0(\log \beta_0) \nonumber\\
& + (\alpha_0-\alpha)\mathbb{E}[\log s]+ (\beta-\beta_0) \mathbb{E}[s] - \alpha \log \beta, &
\end{flalign}
%         invK_mm_expecFF = self.invK_mm.dot(self.uS + self.um_minus_mu0.dot(self.um_minus_mu0.T))
%         self.rate_s = self.rate_s0 + 0.5 * np.trace(invK_mm_expecFF)
where $\alpha= \alpha_0 + \frac{M}{2}$ and $\beta = \beta_0 + \frac{
\textrm{Tr}(\bs K^{-1}_{mm}(S + \hat{\bs f}_m \hat{\bs f}_m^T))}{2}$,
and the terms relating to $\mathbb{E}[p(\bs f | \bs f_m) - q(\bs f)]$ cancel.
The iterative algorithm proceeds by updating each of the $q$ factors in turn,
taking expectations with respect to the other factors. 

The only term in $\mathcal{L}$ that refers to the observations, $\bs y$, 
is a sum of $P$ terms, each of which refers to one observation only.
This means that $\mathcal{L}$ can be maximized iteratively by considering a random subset of 
observations at each iteration~\citep{hensman2013gaussian}.
Hence, we replace Equations \ref{eq:fhat_m} and \ref{eq:S} for computing
$\hat{\bs f}_m$ and $\bs S$ over all observations with a sequence of stochastic updates.

For the $i$th update, we randomly select observations $\bs y_i = \{ y_p \forall p \in D_i \}$.
Rather than using the complete matrices, we perform updates using subsets:
$\bs Q_i$ contains rows and columns for observations in $\bs D_i$,
$\bs K_{im}$ and $\bs A_i$ contain only rows referred to by $y_p \forall p \in \bs D_i$,
$\bs G_i$ contains rows in $\bs D_i$ and columns referred to by $y_p \forall p \in \bs D_i$,
and $\hat{\bs z}_i = \{ \mathbb{E}[\bs z_p] \forall p \in D_i \}$.
%All matrices with subscript $_i$ contain only the subset of elements relating to 
%observations in $\bs D_i$.
% The linearization matrix $\bs G_i$ is the subset of elements in $\bs G$ relating to observations in $\bs D_i$, 
%  is the corresponding subset of elements in $\bs Q$,
%  is the covariance between the items referred to by pairs in $\bs D_i$ 
% and the inducing points,
% and  contains the corresponding rows of $\bs A$.
The update equations optimize the natural parameters of the Gaussian distribution by following the
natural gradient~\citep{hensman2015scalable}:
\begin{flalign}
\bs S^{-1}_i  & = (1 - \rho_i) \bs S^{-1}_{i-1} + \rho_i\left( \mathbb{E}[s]\bs K_{mm}^{-1} + w_i\bs K_{mm}^{-1}\bs K_{im}^T \bs G^T_{i} \bs Q^{-1}_i \bs G_{i} \bs K_{im} \bs K_{mm}^{-T} \right)& 
\label{eq:S_stochastic} \\
\hat{\bs f}_{m,i}  & = \bs S_i \left( (1 - \rho_i) \bs S^{-1}_{i-1} \hat{\bs f}_{m,i-1}  + 
\right. \nonumber \\
& \hspace{1.5cm} \rho_i w_i 
\bs K_{mm}^{-1} 
\left.\bs K_{im}^T \bs G_{i}^T \bs Q_i^{-1} \left( \bs y_i  - \Phi(\mathbb{E}[\bs z_i]) + \bs G_{i} \bs A_i \hat{\bs f}_{m,i} \right) \right) & 
\label{eq:fhat_stochastic}
\end{flalign}
where
$\rho_i=(i + delay)^{-forgettingRate}$ is a mixing coefficient that controls the update rate,
$w_i = \frac{P}{|D_i|}$ weights each update according so sample size,
and $delay$ and $forgettingRate$ are hyperparameters of the algorithm~\citep{hoffman2013stochastic}, .


The scale parameter, $s$, can also be learned as part of the SVI procedure. Its variational factor,
$q(s)$, has the following update equations:
\begin{flalign}
\mathbb{E}[s] & = \frac{2a_0 + M}{2b} \label{eq:Es}\\
\mathbb{E}[\log s] & = \Psi(2a_0 + M) - \log(2b), \label{eq:Elogs}
\end{flalign}
where $\Psi$ is the digamma function.

The complete SVI algorithm is summarized in Algorithm \ref{al:singleuser}.
\begin{algorithm}
 \KwIn{ Pairwise labels, $\bs y$, training item features, $\bs x$, 
 test item features $\bs x^*$}
 \nl Compute kernel matrices $\bs K$, $\bs K_{mm}$ and $\bs K_{nm}$ given $\bs x$
 \nl Initialise $\mathbb{E}[s]$, $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to prior means
 and $\bs S$ to prior covariance $\bs K_mm$\;
 \While{$\mathcal{L}$ not converged}
 {
 \nl Select random sample, $\bs D_i$, of $P$ observations
 \While{$\bs G_i$ not converged}
  {
  \nl Compute $\bs G_i$ given $\mathbb{E}[\bs f_i]$ \;
  \nl Compute $\hat{\bs f}_{m,i}$ and $\bs S_{i}$ \;
  \nl Compute $\mathbb{E}[\bs f_i]$ \;
  }
 \nl Update $q(s)$ and compute $\mathbb{E}[s]$ and $\mathbb{E}[\log s]$\;
 }
\nl Compute kernel matrices for test items, $\bs K_{**}$ and $\bs K_{*m}$, given $\bs x^*$ \;
\nl Use converged values of $\mathbb{E}[\bs f]$and $\hat{\bs f}_m$ to estimate
posterior over $\bs f^*$ at test points \;
\KwOut{ Posterior mean of the test values, $\mathbb{E}[\bs f^*]$ and covariance, $\bs C^*$ }
\caption{The SVI algorithm for preference learning with a single user.}
\label{al:singleuser}
\end{algorithm}
The use of an inner loop to learn $\bs G_i$ avoids the need to store the complete matrix, 
$\bs G$.
The inferred distribution over the inducing points can be used 
for predicting the values of test items, $f(\bs x^*)$:
\begin{flalign}
\bs f^* &= \bs K_{*m} \bs K_{mm}^{-1} \hat{\bs f}_m, \\
\bs C^* &= \bs K_{**} + \bs K_{*m} \bs K_{mm}^{-1} (\bs S - \bs K_{mm} / \mathbb{E}[s] ) \bs K_{*m}^T \bs K_{mm}^{-1},
\end{flalign}
where $\bs C^*$ is the posterior covariance of the test items, $\bs K_{**}$ is their prior covariance, and
$\bs K_{*m}$ is the covariance between test and inducing points.
It is possible to recover the lower bound proposed by 
\citet{hensman2015scalable} for classification by generalizing the
likelihood to arbitrary nonlinear functions, and omitting terms relating to $p(s|\alpha_0,\beta_0$ and $q(s)$.
However, our approach avoids expensive quadrature methods by linearizing the likelihood to enable analytical updates. We also infer $s$ in a Bayesian manner, 
rather than treating as a hyper-parameter, which is important for preference learning where $s$ controls the noise level of the observations relative to  $f$. 

\subsection{SVI for Crowd Preference Learning}

We now extend the SVI method to the crowd preference learning model proposed in
Section \ref{sec:crowd_model}.
To begin with, we extend the variational posterior in Equation \ref{eq:svi_approx}
to approximate the crowd model defined in Equation \ref{eq:joint_crowd}.
\begin{flalign}
& p( \bs V, \bs V_m, \bs W, \bs W_m, \bs t, \bs t_m, s_1, ..., s_D, \sigma | \bs y, \bs x, \bs x_m, \bs u, \bs u_m, k, \alpha_0, \beta_0 ) \approx & \nonumber \\
& \hspace{3.2cm} q(\bs V)q(\bs W)q(\bs t)q(\bs V_m)q(\bs W_m)q(\bs t_m)\prod_{d=1}^{D}q(s_d)q(\sigma), &
\end{flalign}
where $\bs u_m$ are the feature vectors of inducing points for the users.
This approximation factorizes the joint distribution between the latent item factors, $\bs V$, the latent user factors, $\bs W$, and the common means, $\bs t$, 
but does not requiring factorization across the latent dimensions $\bs w_1,...,\bs w_D$ and $\bs v_1,...,\bs v_D$.
The variational factors for the inducing points can be obtained by deriving expectations as follows, beginning with the latent item factors:
%TODO rename D_i
%TODO make the vectors all lower case for V_d and W_d
%TODO do something to make sure all the block diags are properly indiciated including A_v... but actually I think they are useless because the off-diagonal blocks only affect the collapsed posterior covariance and are never used in computing any other variables.
%TODO intialization of the factors
%TODO put definition of f in somewhere in the model section 
%TODO big Sigma is variance of W because it's different...
%TODO why is the scaling by W nice and simple without off-diagonals, but scaling by V is not? I think that when Q is diagonal, off-diagonals in any scaling factors are irrelevant.
\begin{flalign}
\log q(\bs V_m) = \;\;& \mathbb{E}_{q(\bs W),q(\bs t)}[\log \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right)] & \nonumber \\
& + \sum_{d=1}^D \log\mathcal{N}(\bs v_{m,d}; \bs 0, \bs K_{v,mm}/\mathbb{E}[s_d]) 
+ \textrm{const} & \nonumber \\
% are the dimensions collapsed to a single MVN?
= \;& \sum_{d=1}^D \log \mathcal{N}(\bs v_{m,d}; \hat{\bs v}_{m,d}, \bs S_{v,d}) & \\
\bs S_{v,d}^{-1} = \;\;& \bs K^{-1}_{v,mm}/\mathbb{E}[s_d] 
+ \bs A_v^T \bs G^T \textrm{diag}(\hat{\bs w}_{d,\bs j}^2 + \bs\Sigma_{d,\bs j,\bs j})\bs Q^{-1} \bs G \bs A_v & \\
\hat{\bs v}_{m,d} = \;\;& \bs S_{v,d} \bs A_v^T \bs G^T \textrm{diag}(\hat{\bs w}_{d,\bs j}) \bs Q^{-1}(\bs y - \Phi\left(\mathbb{E}[\bs z]) + \bs G(\hat{\bs v}_d^T \hat{\bs w}_d)\right), &
\end{flalign}
where $\bs A_v = \bs K_{v,nm} \bs K_{v,mm}^{-1}$
and $\hat{\bs w}_{d}$ and $\bs\Sigma_{d}$ are the posterior mean and covariance of the $d$th latent user factor
and the subscript $._{\bs j} = \{ ._{j_p} \forall p \in 1,...,P \}$ contains rows that correspond to the 
users indicated in the observations, $\bs y$. 
The matrix $\textrm{diag}(\hat{\bs W}_{d,\bs j}^2 + \bs\Sigma_{d,\bs j})$ therefore
scales the diagonal observation precision, $\bs Q^{-1}$, by the latent user factors.
The variational factor for the inducing points of the common item mean follows a similar pattern:
\begin{flalign}
\log q(\bs t_m) = \;\;& \mathbb{E}_{q(\bs V)q(\bs W)}[\log \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right)] 
+ \mathbb{E}[\log\mathcal{N}(\bs t_m; \bs 0, \bs K_{t,mm}/s)] 
+ \textrm{const} & \nonumber \\
= \;\;& \log \mathcal{N}\left( \bs t; \hat{\bs t}, \bs S_t \right) & \\
\bs S_t^{-1} = \;\;& \bs K^{-1}_{t,mm}/\mathbb{E}[\sigma] 
+ \bs A_t^T \bs G^T \bs Q^{-1} \bs G \bs A_t & \\
\hat{\bs t}_{m} = \;\;& \bs S_{t} \bs A_t^T \bs G^T \bs Q^{-1}
\left(\bs y - \Phi(\mathbb{E}[\bs z]) + \bs G(\hat{\bs t})\right), &
\end{flalign}
where $\bs A_t = \bs K_{t,nm} \bs K_{t,mm}^{-1}$.
Finally, the variational factor for the inducing points of the latent user factors
requires a different linearization term, $\bs H \in P \times U$, to sum over items rather than users:
\begin{flalign}
H_{p,k} = \Phi(\mathbb{E}[z_p])(1 - \Phi(\mathbb{E}[z_p]) (2y_p - 1) [u_p = k] % needs to be added or subtracted depending on a or b
\end{flalign} 
%now multiply by V. What about covariances between v?
The variational factor is then as follows:
\begin{flalign}
\log q(\bs W_m) = \;\;& \mathbb{E}_{q(\bs V)q(\bs t)}[\log \mathcal{N}\left( \bs y; \tilde{\Phi}(\bs z), Q \right)] 
+ \sum_{d=1}^D \mathbb{E}[\log\mathcal{N}(\bs w_d; \bs 0, \bs K_{w,mm})]
+ \textrm{const} & \nonumber \\
= \;\;& \sum_{d=1}^D \log \mathcal{N}\left( \bs w_d; \hat{\bs w}_d, \bs \Sigma \right) & \\
\bs \Sigma^{-1}_{d} = \;\;& \bs K^{-1}_{w,mm}
+ \bs A_w^T \left( \bs H^T \textrm{diag}(\hat{\bs v}_{d,\bs a}^2 + \bs S_{d,\bs a, \bs a}) \bs Q^{-1} \bs H^T \right. & \nonumber \\
& \left. + \bs H^T \textrm{diag}(\hat{\bs v}_{d,\bs b}^2 + \bs S_{d,\bs b, \bs b}) 
   \bs Q^{-1} \bs H^T \right) \bs A_w & \\
\hat{\bs w}_{m,d} = \;\;& \bs \Sigma_{d} \bs A_w^T 
\left( \bs H^T\textrm{diag}(\hat{\bs v}_{d,\bs a}) - \bs H^T\textrm{diag}(\hat{\bs v}_{d,\bs b}) \right) \bs Q^{-1} & \nonumber \\
& \left(\bs y - \Phi(\mathbb{E}[\bs z]) + \bs G(\hat{\bs v}_d^T \hat{\bs w}_d)\right), &
\end{flalign}
where the subscripts $._{\bs a} = \{ ._{a_p} \forall p \in 1,...,P \}$
and  $._{\bs b} = \{ ._{b_p} \forall p \in 1,...,P \}$ are lists of indices to the first and 
second items in the pairs, respectively, and $\bs A_w = \bs K_{w,um} \bs K_{w,mm}^{-1}$.

The equations for the means and covariances 
can be adapted for stochastic updating by applying weighted sums over
the stochastic update and the previous values in the 
same way as  Equation \ref{eq:S_stochastic} and \ref{eq:fhat_stochastic}.
The stochastic updates for the inducing points of the latent factors depend 
on expectations with respect to the observed points. 
As with the single user case, the variational factors at the observed items are independent of the observations given the variational factors of the inducing points
(likewise for the observed users):
\begin{flalign}
\log q(\bs V) & = \sum_{d=1}^D \log \mathcal{N}\left( \bs v_d; \bs A_v\hat{\bs v}_{m,d}, 
\frac{\bs K_{v}}{\mathbb{E}[s_d]} + \bs A_v (\bs S_{m,d} - \frac{\bs K_{v,mm}}{\mathbb{E}[s_d]})\bs A_v \right) & \label{eq:qv} \\
\log q(\bs t) & = \log \mathcal{N}\left( \bs t; \bs A_t \hat{\bs t}_m, 
\frac{\bs K_{t}}{\mathbb{E}[\sigma]} + \bs A_t (\bs S_t - \frac{\bs K_{t,mm}}{\mathbb{E}[\sigma]})\bs A_t \right)  & \label{eq:qt}\\
\log q(\bs W) & = \sum_{d=1}^D \log \mathcal{N}\left( \bs w_d; \bs A_w \hat{\bs w}_{m,d}, \bs K_{w} + \bs A_w (\bs\Sigma - \bs K_{w,mm}) \bs A_w \right). &
\label{eq:qw}
\end{flalign}
The expectations for the inverse scales, $s_1,...,s_d$ and $\sigma$, can be 
computed using the formulas in Equations \ref{eq:Es} and \ref{eq:Elogs} by
substituting in the corresponding terms for each $\bs v_d$ or $\bs t$ instead of $\bs f$. 
Predictions in the latent factor model can be made using Equations \ref{eq:qv}, \ref{eq:qt} and \ref{eq:qw} by substituting the covariance
terms relating to observation items, $\bs x$, and users, $\bs u$, with corresponding
covariance terms for the prediction items and users.

As with the single user model, the lower bound on the marginal likelihood 
contains sums over the observations, hence is suitable for stochastic variational
updates:
% APPENDIXIFY
\begin{flalign}
\mathcal{L} & = \label{eq:lowerbound_crowd}
\sum_{p=1}^P \mathbb{E}_{q(\bs f)}[\log p(y_p | \bs v_{a_p}^T \bs w_{a_p} + t_{a_p}, \bs v_{b_p}^T \bs w_{b_p} + t_{b_p})] 
- \frac{1}{2} 
\Bigg\{  \sum_{d=1}^D \bigg\{  - M_n - M_u & \nonumber \\
&  
 + \log|\bs K_{v,mm}| + \log|\bs K_{w,mm}|
\log|\bs S_{v,d}|  - \mathbb{E}[\log s_d] 
+ \hat{\bs v}_{m,d}^T \mathbb{E}[s_d]\bs K_{v,mm}^{-1}\hat{\bs v}_{m,d} & \nonumber \\
& 
+ \textrm{Tr}(\mathbb{E}[s_d] \bs K_{v,mm}^{-1} \bs S_{v,d}) 
+ \log|\bs \Sigma_{d}|  + \hat{\bs w}_{m,d}^T \bs K_{w,mm}^{-1}\hat{\bs w}_{m,d} 
+ \textrm{Tr}(\bs K_{w,mm}^{-1} \bs \Sigma_{d})
\bigg\}
& \nonumber \\
&  
- M_n + \log|\bs K_{t,mm}|
+ \log|\bs S_{t}|  - \mathbb{E}[\log \sigma] 
+ \hat{\bs t}^T \mathbb{E}[\sigma] \bs K_{t,mm}^{-1} \hat{\bs t}  &
\nonumber \\
&
+ \textrm{Tr}(\mathbb{E}[\sigma] \bs K_{t,mm}^{-1} \bs S_{t})
\Bigg\} 
- (D+1)(\log\Gamma(\alpha_0)  + \alpha_0(\log \beta_0))
& \nonumber \\
& + \sum_{d=1}^D \bigg\{ 
\log\Gamma(\alpha_d) + (\alpha_0 - \alpha_d)\mathbb{E}[\log s_d]
+ (\beta_d - \beta_0) \mathbb{E}[s_d] - \alpha_d \log \beta_d \bigg\}
 & 
\nonumber \\ 
& + \log\Gamma(\alpha_{\sigma}) + (\alpha_0 - \alpha_{\sigma})\mathbb{E}[\log \sigma]
+ (\beta_{\sigma} - \beta_0) \mathbb{E}[s_d] - \alpha_{\sigma} \log \beta_{\sigma}
, &
\end{flalign}

In this section, we proposed an SVI scheme for Bayesian matrix factorization given pairwise observations. The inference scheme can readily be adapted to regression or classification tasks by swapping out the preference likelihood, resulting in 
different values for $\bs G$ and $\bs H$. We now show how to learn the  
length-scale parameter required to compute covariances using typical kernel functions, then demonstrate how our method can be applied to learning user preferences or
consensus opinion when faced with disagreement.
 
