\section{Introduction}\label{sec:intro}

Many tasks are more suited to pairwise comparisons than classification etc. 
Crowds of non-expert annotators may label more accurately if presented with pairs. 
Implicit feedback may be taken from user actions in an application that can be represented as a preference, such as choosing
an option over other options.

There are several works for learning from noisy pairwise comparisons so far (Horvitz et al. 2013 or something like that?). 
However, these do not provide a way to take account of item features or to model different but valid subjective viewpoints. 
They assume there is a single ground truth and can therefore model only one task and one user's (or a consensus of all users) preferences at once. 

Work by Felt et al. 2015, Simpson et al. 2015 etc. shows that item features are particularly useful when combining crowdsourced data. A Gaussian process has not been tested for this purpose before?

GP preference learning presents a way to learn from noisy preferences but assumes constant noise and a single underlying preference function. 
The collaborative Gaussian process (Houlsby et al. 2012) learns multiple users' preferences. 
However existing implementations do not scale and do not identify ground truth. 

We show how to scale it using SVI and how to use the model to identify ground truth from subjective preferences. 

%Things we assume are discussed in the intro:
% -- recommendation -- item-item similarity, user-user similarity (CF), user-item matching
% -- learning from implicit preferences/user actions
% -- need to handle sparsity i.e. most pairs of users/items not labelled
% -- noisiness of pairwise labels? (btw can we use our model to combine implicit labels for a single user --> ground truth for this user --> not all sources of data agree with true orderings). 
% -- benefits of BMF
% -- use of GPs for BMF?
% -- Older learning-to-rank algorithms: 
%Learning to rank from pairwise comparison data has been studied for document retrieval systems
% 2. R. Herbrich, T. Graepel, K. Obermayer, "Large margin rank boundaries for ordinal regression" in , MIT Press, 2000.
% Show Context

% a learning algorithm was developed by extending AdaBoost in [3]
% 3. Y. Freund, R. D. Iyer, R. E. Schapire, Y. Singer, "An efficient boosting algorithm for combining preferences", Journal of Machine Learning Research, vol. 4, pp. 933-969, 2003.
% Show Context

% A simple probabilistic model based on a neural network, called RankNet, was introduced in [4] 
% 4. C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. N. Hullender, "Learning to rank using gradient descent", ICML, pp. 89-96, 2005.
% T. Joachims, "Optimizing search engines using clickthrough data", KDD, 2002.

%an SVM algorithm was generalized in [6] to learn a linear function for ranking
% 6. T. Joachims, "Optimizing search engines using clickthrough data", KDD, 2002.


%uchida2017entity -- learning preferences from user reviews
%APRIL paper from Alex? As a motivating example of why we need to learn preference function.

% A crowd may not necessarily consist of individual human annotators working towards a common goal of annotating a dataset. 
% It may be separate users of a system, whose individiual preferences we wish to predict,
% or we may treat the same user carrying out different tasks as a separate individual.  
% Each user may also generate labels for different types of user interaction.
% Hence, we can go beyond simple user-item matrices to treat each type of annotation  
% as a different 'user', and thereby combine different sources of information.
% Like the workers in a crowd, each source of annotations has a particular correlation with a ground truth
% that we wish to predict, i.e. the preferences for a particular user in a specific context.
% We must learn this correlation by identifying the ground truth from either explicit user ratings
% or consensus between relevant sources of information, defined a priori.

In this paper, we develop methodology to solve the following questions:
\begin{enumerate}
  \item How can we learn a rating function over large sets of items given a large number of pairwise comparisons?
  \item How do we account for the different personal preferences of annotators when inferring the ground truth?
\end{enumerate}
To answer these questions we make the following technical contributions:
\begin{enumerate}
 \item We propose a method for predicting either gold-standard or personalized ratings by aggregating crowdsourced preference labels using a model of the noise and biases of individual annotators. % say why this is not possible with Dawid and Skene
  \item %To enable collaborative preference learning % collaborative preference learning needs to be defined above
   To enable this method to scale to large, real-world datasets, we develop
   stochastic variational inference for Bayesian matrix factorization and Gaussian process preference learning.
  \item To expedite hyper-parameter tuning, we introduce a technique for gradient-based length-scale optimization of Gaussian processes.
\end{enumerate}

The next section of the paper discusses related work.
We then we develop our model for preference learning from crowds in Section \ref{sec:model},
followed by our proposed inference method in Section \ref{sec:inf} and
hyper-parameter optimisation technique in Section \ref{sec:ls}.
The, we evaluate our approach empirically, showing first its behaviour on synthetic data in Section
\ref{sec:synth}, then its scalability and predictive performance on several real-world datasets 
in Section \ref{sec:real}.

